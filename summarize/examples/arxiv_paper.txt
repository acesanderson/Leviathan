Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework
Yuping Wu, Hao Li, Hongbo Zhu, Goran Nenadic, Xiao-Jun Zeng
Department of Computer Science University of Manchester {yuping.wu, hao.li-2, hongbo.zhu, gnenadic, x.zeng}@manchester.ac.uk
Correpsonding Author.
Abstract
Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive summarization with the help of salient information identified by the extractive model. Previous works that adopt this paradigm train the extractor and abstractor separately and introduce extra parameters to highlight the extracted salients to the abstractor, which results in error accumulation and additional training costs. In this paper, we first introduce a parameter-free highlight method into the encoder-decoder framework: replacing the encoder attention mask with a saliency mask in the cross-attention module to force the decoder to focus only on salient parts of the input. A preliminary analysis compares different highlight methods, demonstrating the effectiveness of our saliency mask. We further propose the novel extract-and-abstract paradigm, ExtAbs1, which jointly and seamlessly performs Extractive and Abstractive summarization tasks within single encoder-decoder model to reduce error accumulation. In ExtAbs, the vanilla encoder is augmented to extract salients, and the vanilla decoder is modified with the proposed saliency mask to generate summaries. Built upon BART and PEGASUS, experiments on three datasets show that ExtAbs can achieve superior performance than baselines on the extractive task and performs comparable, or even better than the vanilla models on the abstractive task.

Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework


Yuping Wu, Hao Li, Hongbo Zhu, Goran Nenadic, Xiao-Jun Zengâ€ 
Department of Computer Science
University of Manchester
{yuping.wu, hao.li-2, hongbo.zhu, gnenadic, x.zeng}@manchester.ac.uk


1Introduction
The automatic text summarization task aims to condense the important information in a given text and form a summary. The extractive and abstractive are the two most common approaches to this task by extracting the most salient textual segments in the text or generating a sequence of words with salient information. With the great success achieved by Transformer, most of recent extractive and abstractive summarization models (Cheng et al., 2023; Li et al., 2023) are established from pre-trained Transformer-based models, among which, the encoder-decoder architecture dominates.

Refer to caption
Figure 1:Simplified visualization of the cross-attention values of the first head at the penultimate decoder layer in BertAbsSum. The attention value between an input sentence and output sentence is calculated by summing token attention in an input sentence for each output token and averaging over the output sentence. Sentences â‘¡, â‘¢ and â‘¦ constitute the reference summary and each colour represents a piece of salient information.
The extract-then-abstract paradigm takes advantage of the inherent connection between the extractive and abstractive summarization by generating the abstractive summary with the utilization of extracted salient information. Existing works that explore this paradigm generate the summary with either the extractions as input only (Ernst et al., 2022; Lebanoff et al., 2020) or the original input document with extractions highlighted (Bao and Zhang, 2021; Xiong et al., 2022; Dou et al., 2021; Adams et al., 2023) as input. However, on the one hand, these works treat the extractor and abstractor as two functionally independent models which result in duplicate encoding, and most works train them individually, exposing the abstractor to errors accumulated from the extractor. On the other hand, the methods of highlighting extractions in these works inevitably introduce extra learning parameters and result in extra training costs, e.g., highlight embedding layer or extra encoder for extractions.

We first propose a parameter-free highlight method by augmenting attention, i.e., the saliency mask, a mask for salient tokens in the input sequence. By replacing the vanilla encoder attention mask (i.e., the non-padded token mask) with the proposed saliency mask in the cross-attention module, the decoder is forced to only aggregate information from those salient tokens and ignore non-salient ones. As illustrated in Figure 1, the vanilla model tends to be overconfident (sharper attention distribution) when determining the cross-attention values and thus fails to capture salient information with the wrong attention. Whereas with the proposed saliency mask, the model generates a more balanced attention distribution over the explicitly narrowed-down attentive scope. To quantify the effectiveness of the saliency mask, we conduct preliminary analysis on the CNN/DM dataset to compare it with other highlight methods, and the results validate its effectiveness.

Then, we propose the novel extract-and-abstract paradigm ExtAbs (depicted in Figure 2), adapting any given encoder-decoder model to perform extractive and abstractive summarization jointly and seamlessly with encoder shared between the extractor and abstractor. In ExtAbs, the encoder is augmented by integrating the span extractor to learn text-span representations and a classification layer to perform the extractive classification task. The augmented encoder serves as the extractor to extract salient text spans. Together with the encoder, the decoder serves as the abstractor and is modified by alternating the encoder attention mask with the proposed saliency mask in the cross-attention module. The saliency mask is determined by the extractorâ€™s predicted top-
z
 salient text spans. Jointly training the augmented encoder and decoder enables the extract-and-abstract paradigm within a single encoder-decoder framework, removing the functional independence between the extractor and abstractor, along with duplicate encoding and error accumulation. Experiments are conducted on the CNN/DM, Reddit and PubMed datasets with ROUGE scores and BARTScore as automatic metrics. On CNN/DM, ExtAbs generates better abstractive summaries than the vanilla model. On Reddit and PubMed, it achieves the SOTA extractive performance while maintaining a comparable performance on the abstractive task compared with the vanilla model. Human evaluation also validates the quality of summaries generated by ExtAbs.

The contributions of this paper are threefold:

1) We propose a parameter-free highlight method for the extract-then-abstract paradigm, i.e., the saliency mask, with preliminary analysis validating its effectiveness.
2) We propose ExtAbs, a novel extract-and-abstract paradigm, which enhances the vanilla encoder-decoder model to jointly and seamlessly perform extractive and abstractive summarization. In ExtAbs, the jointly trained encoder not only mitigates errors that arise from disjoint processing for the abstractor but also improves extractive outputs (saliency masks). By learning from both extractive and abstractive instances and being optimized in a multi-task setting, the encoder achieves a higher standard of encoding performance, leading to better summarization performance.
3) The experimental results show that ExtAbs achieves superior abstractive performance than the vanilla model on CNN/DM and SOTA extractive performance on both Reddit and PubMed.
Refer to caption
Figure 2:The architecture of the proposed ExtAbs. The left part serves as the extractor to perform saliency classification for each textual segment in the document. The right part is the abstractor, which generates a summary based on the encoder output and saliency mask determined by the predicted saliency scores.
2Related Work
Text Summarization
Most previous works about automatic text summarization focus on doing it in only either an extractive or abstractive way. Along with the development of Transformer-based models, the paradigm of fine-tuning a pre-trained language model (PLM) dominates the methods in both ways. PLMs like BERT have been widely adopted as the input encoder for extractive summarization models (Cheng et al., 2023; Ruan et al., 2022; Kwon et al., 2021; Zhong et al., 2020). For abstractive summarization, PLMs such as BART Lewis et al. (2020), PEGASUS Zhang et al. (2020a), ProphetNet Qi et al. (2020) and Longformer Beltagy et al. (2020) were introduced for the better generation result. Most of the recent abstractive models (Pu et al., 2023; Goyal et al., 2022; Liu et al., 2022; Dou et al., 2021) were built on one of these PLMs with the encoder-decoder architecture.

Extract-then-Abstract
Extracting salient information first and then performing abstractive summarization is naturally coherent. Some work (Bao and Zhang, 2021; Xiong et al., 2022; Dou et al., 2021; Ernst et al., 2022; Lebanoff et al., 2020; Adams et al., 2023; Pilault et al., 2020) explored this extract-then-abstract paradigm in a two-step manner, training the extractor and abstractor independently and using extra learning parameters to highlight extractions. Some work (Li et al., 2020; Song et al., 2022) adapted the reinforcement learning to train the extractor by maximizing the reward derived from the abstractor. Only a few work (Hsu et al., 2018; Mendes et al., 2019) jointly trained the extractor and abstractor. Existing works essentially treat the extractor and abstractor as two functionally independent models.

Exploring the gaps of functional independence and additional highlight parameters in the existing extract-then-abstract paradigm, we develop a unified extract-and-abstract approach that trains the extractor and abstractor within a single encoder-decoder model. Building on the Transformer-based PLM, we introduce a saliency mask to highlight extracted salient information for the abstractor, resulting in an effective parameter-free method.

3Saliency Mask
Motivation
The extract-then-abstract paradigm enhances the abstractive model by highlighting salient information identified by the extractive model, making it valuable to explore the effective highlight method. Some previous works (You et al., 2019; Xu et al., 2020b; Wang et al., 2022) have explored different augmentation methods for the attention mechanism to emphasize tokens of interest such as salient tokens in the input sequence or control tokens. Their experimental results demonstrate the effectiveness of attention augmentation, inspiring us to propose a parameter-free highlight method by augmenting attention, i.e., saliency mask, the mask for salient tokens in the input sequence. Replacing the vanilla non-padded token mask with the saliency mask in the cross-attention module intuitively highlights salient information without introducing any extra parameters and explicitly forces the decoder to only attend to salient tokens.

Saliency Mask
Given an input sequence with 
n
 number of tokens, let 
l
 denote the list of indices of tokens in the salient parts of the input sequence, i.e., 
l
=
[
i
,
â€¦
,
j
,
â€¦
,
k
]
 where 
1
â‰¤
i
<
j
<
k
â‰¤
n
. The saliency mask 
m
â¢
a
â¢
s
â¢
k
âˆˆ
â„
n
 is derived as follows:

m
â¢
a
â¢
s
â¢
k
i
=
{
1
,
if 
i
âˆˆ
l
0
,
otherwise
(1)
For a given encoder-decoder model, the cross-attention value between the transformed encoder output 
K
âˆˆ
â„
n
Ã—
d
k
 and decoder intermediate output 
Q
âˆˆ
â„
m
Ã—
d
k
 is modified to conduct the element-wise product with the saliency mask, i.e.,

a
â¢
t
â¢
t
â¢
n
~
â¢
(
Q
,
K
)
=
s
â¢
o
â¢
f
â¢
t
â¢
m
â¢
a
â¢
x
â¢
(
Q
â¢
K
T
d
k
)
âŠ™
m
â¢
a
â¢
s
â¢
k
(2)
where 
d
k
 is the dimension of 
Q
 and 
K
 and 
m
 is the number of tokens in the decoded sequence. For the calculative consistency, the size of 
m
â¢
a
â¢
s
â¢
k
 is repeatedly expanded to be 
â„
m
Ã—
n
. As a result, the cross-attention module outputs

A
â¢
t
â¢
t
â¢
e
â¢
n
â¢
i
â¢
o
â¢
n
~
â¢
(
Q
,
K
,
V
)
=
a
â¢
t
â¢
t
â¢
n
~
â¢
(
Q
,
K
)
â¢
V
(3)
Sentence-level and EDU-level Saliency Mask
Depending on the text granularity, the salient parts are not necessarily the same for a document. The two most common text granularities used in the summarization task are the sentence and sub-sentence, such as the Elementary Discourse Unit (EDU). EDU is defined as the terminal node in the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and has been widely applied in summarization models (Pu et al., 2023; Adams et al., 2023; Wu et al., 2023; Xu et al., 2020a). It is evidenced by previous works that EDUs provide more fine-grained information than sentences for a summarizer. Therefore, we propose EDU-level and sentence-level saliency masks derived from salient EDUs and sentences, respectively.

Preliminary Analysis
To verify the effectiveness of the proposed saliency mask, we conduct a preliminary analysis of the CNN/DM dataset to compare it with other highlight methods. The baseline highlight methods include ContextRewriter Bao and Zhang (2021) and EDURewriter Xiong et al. (2022) which introduce additional group tag embedding layer for salient tokens, and GSum Dou et al. (2021) which incorporates an additional encoder for the salients. Following these previous works, a greedy selection algorithm is applied to determine the salients in a document by greedily maximizing the ROUGE scores between the selected salients and the reference summary (pseudo-code in Appendix A). For a fair comparison, all methods are evaluated on BertSumAbs Liu and Lapata (2019) with the pre-trained BERT-base as the encoder and 6 Transformer decoder layers as the decoder. Notably, EDURewriter highlights EDU-level salients while the other two highlight at sentence level.

As shown in Table 1, our proposed EDU-level saliency mask outperforms all three other highlight methods and the sentence-level saliency mask on ROUGE scores. Moreover, the saliency mask significantly outperforms the vanilla mask, where the whole input sequence is attended. The results demonstrate that our proposed highlight method better boosts abstractive model performance without introducing extra learning parameters.

Method	ROUGE-1	ROUGE-2	ROUGE-L
BertSumAbs	41.24	18.80	38.24
\cdashline1-4 
GSum
â€ 
 	55.18	32.54	52.06
ContextRewriter
â€ 
52.57	29.71	49.69
EDURewriter
â€ 
54.49	31.76	51.79
Saliency Mask (sentence)	52.63	29.93	49.66
Saliency Mask (EDU)	57.27	34.22	54.23
Table 1:Results of different salient information highlight methods for model BertSumAbs. F1 scores are reported. â€  indicates that the results are copied from the corresponding original paper.
4ExtAbs
4.1Problem Formulation
Given a document 
D
 consisting of 
m
 textual segments and the 
i
-th textual segment contains 
n
i
 words, i.e., 
D
=
[
t
â¢
s
1
,
â€¦
,
t
â¢
s
m
]
 and 
t
â¢
s
i
=
[
w
i
â¢
1
,
â€¦
,
w
i
â¢
n
i
]
, and a base model 
â„³
 with encoder-decoder architecture, the aim is to modify 
â„³
 to derive an extractive summary and an abstractive summary, i.e., 
S
e
â¢
x
â¢
t
âˆ—
=
[
t
s
i
,
â€¦
,
t
s
j
,
â€¦
.
,
t
s
k
]
 where 
1
â‰¤
i
<
j
<
k
â‰¤
m
 and 
S
a
â¢
b
â¢
s
âˆ—
=
[
w
1
s
âˆ—
,
â€¦
.
,
w
l
s
âˆ—
]
.

Additionally, let 
ð’®
 denote the human-written reference summary for 
D
 with 
r
 words, i.e., 
ð’®
=
[
w
1
s
,
â€¦
,
w
r
s
]
. The set of ground truth labels for each textual segment could be derived from 
ð’®
, i.e., 
G
â¢
T
=
[
g
â¢
t
1
,
â€¦
,
g
â¢
t
m
]
, via the same greedy algorithm as applied in Section 3. If 
t
â¢
s
i
 is selected by the algorithm, 
g
â¢
t
i
=
1
; otherwise, 
g
â¢
t
i
=
0
.

4.2Model
As illustrated in Figure 2, the proposed model ExtAbs (pseudo-code in Appendix B) has two modules. In the extractor, the entire input sequence goes through the encoder in 
â„³
 to get contextual representations for input tokens, and the self-attentive span extractor aggregates token representations to derive representations for each textual segment. Then, the classification layer predicts saliency scores for each textual segment. In the abstractor, a saliency mask is first generated for those textual segments with top-
z
 highest saliency scores. Then, the decoder generates summary tokens auto-regressively but attends to the input sequence based on the saliency mask. The extractive summary is formed with the textual segments with top-
k
 saliency scores, and the abstractive summary is the sequence generated by the decoder.

Extractor
Given a document, the encoder in 
â„³
 takes 
D
 as input and outputs hidden states as contextual representations for each token in 
D
:

{
w
11
e
,
â€¦
,
w
1
â¢
n
1
e
,
â€¦
,
w
m
â¢
1
e
,
â€¦
,
w
m
â¢
n
m
e
}
=
â„³
e
â¢
n
â¢
c
â¢
(
D
)

(4)
Following previous works that formulate the extractive task as sequence labelling, we introduce the span aggregation and classification layers. A self-attentive span extractor is applied to aggregate token representations for each textual segment based on the predicted token attentions, where a feed-forward network (FFN) calculates an attention score for each token and the softmax layer normalizes the scores, i.e.,

Î±
i
â¢
j
=
e
â¢
x
â¢
p
â¢
(
F
â¢
F
â¢
N
â¢
(
w
i
â¢
j
e
)
)
âˆ‘
k
=
1
n
i
e
â¢
x
â¢
p
â¢
(
F
â¢
F
â¢
N
â¢
(
w
i
â¢
k
e
)
)
(5)
t
â¢
s
i
e
=
âˆ‘
j
=
1
n
i
Î±
i
â¢
j
â¢
w
i
â¢
j
e
(6)
Lastly, the linear classification layer with the sigmoid function 
Ïƒ
 and trainable parameters 
W
c
 predicts a saliency score for each textual segment, i.e.,

s
â¢
c
â¢
o
â¢
r
â¢
e
i
=
Ïƒ
â¢
(
W
c
â¢
t
â¢
s
i
e
)
(7)
After ranking the predicted saliency scores, those textual segments with top-
k
 highest scores are concatenated in the order they appear in the document to form the extractive summary, i.e.,

S
e
â¢
x
â¢
t
âˆ—
=
[
t
â¢
s
i
1
,
â€¦
,
t
â¢
s
i
j
,
â€¦
,
t
â¢
s
i
k
]
(8)
where 
i
j
â‰¤
m
 and 
s
â¢
c
â¢
o
â¢
r
â¢
e
i
j
âˆˆ
 top-
k
â¢
(
ð¬ðœð¨ï¿½ï¿½ðž
)
,
j
=
1
,
2
,
â€¦
,
k
.

Abstractor
Following the convention of auto-regressive generation, the decoder in 
â„³
 generates a probability distribution 
ð
â¢
(
w
t
)
 over the pre-defined vocabulary at the 
t
-th decoding step based on the encoderâ€™s hidden states of the input sequence and the previously decoded tokens. A token 
w
t
s
âˆ—
 is sampled from the dictionary based on 
ð
â¢
(
w
t
)
 according to a specific decoding strategy. Differently, the vanilla encoder mask in the cross-attention module is replaced with a saliency mask, i.e.,

ð
â¢
(
w
t
|
ð°
<
t
s
âˆ—
,
m
â¢
a
â¢
s
â¢
k
)
=
â„³
d
â¢
e
â¢
c
â¢
(
[
w
11
e
,
â€¦
,
w
m
â¢
n
m
e
]
,
m
â¢
a
â¢
s
â¢
k
,
ð°
<
t
s
âˆ—
)

(9)
During training, 
m
â¢
a
â¢
s
â¢
k
 is determined like the preliminary analysis described in Section 3, i.e., the greedily selected salients based on reference summary. When conducting inference, 
m
â¢
a
â¢
s
â¢
k
 is determined by the predicted saliency scores, i.e.,

m
â¢
a
â¢
s
â¢
k
i
â¢
j
=
{
1
,
if 
s
â¢
c
â¢
o
â¢
r
â¢
e
i
âˆˆ
 top-
z
â¢
(
ð¬ðœð¨ð«ðž
)
if 
and 
j
=
1
,
2
,
â€¦
,
n
i
0
,
otherwise
(10)
4.3Loss Function
A multi-task objective is adapted as the loss function 
â„’
 to train the model on both tasks jointly. The binary cross entropy between the predicted scores and ground truth labels is minimized for the extractive task, and the negative log-likelihood of the reference summary is minimized for the abstractive task. To stabilize the training, we additionally introduce the Kullbackâ€“Leibler (KL) divergence loss as a regularizer to prevent the large divergence caused by the saliency mask. The loss function is formulated as below.

â„’
e
â¢
x
â¢
t
=
âˆ’
âˆ‘
i
=
1
m
(
g
â¢
t
i
â¢
l
â¢
o
â¢
g
â¢
(
s
â¢
c
â¢
o
â¢
r
â¢
e
i
)
+
(
1
âˆ’
g
â¢
t
i
)
â¢
l
â¢
o
â¢
g
â¢
(
1
âˆ’
s
â¢
c
â¢
o
â¢
r
â¢
e
i
)
)

(11)
â„’
a
â¢
b
â¢
s
=
âˆ’
âˆ‘
i
=
1
r
l
â¢
o
â¢
g
â¢
P
â¢
(
w
i
s
|
ð°
<
i
s
,
m
â¢
a
â¢
s
â¢
k
)
(12)
â„’
K
â¢
L
=
âˆ‘
i
=
1
r
K
L
(
ð
(
w
i
|
ð°
<
i
,
m
a
s
k
)
âˆ¥
ð
(
w
i
|
ð°
<
i
)
)

(13)
â„’
=
Î±
â¢
â„’
e
â¢
x
â¢
t
+
Î²
â¢
â„’
a
â¢
b
â¢
s
+
Î³
â¢
â„’
K
â¢
L
(14)
where 
Î±
,
Î²
 and 
Î³
 are hyperparameters to balance the three terms.

Dataset	Model	Top	ROUGE-1	ROUGE-2	ROUGE-L	BERTScore	BARTScore
CNN/DM	Extractive SOTAâ€ 	-	44.80	21.66	42.56	-	-
MatchSum
â€ 
-	44.41	20.86	40.55	-	-
Extractor
â¢
(
BART
)
k
=7	43.90	21.49	41.71	0.86	-4.40
Extractor
â¢
(
PEGASUS
)
k
=7	43.75	21.40	41.58	0.86	-4.39
ExtAbs
â¢
(
BART
)
-ext	
k
=7	43.96	21.59	41.78	0.86	-4.37
ExtAbs
â¢
(
PEGASUS
)
-ext	
k
=7	44.04	21.55	41.88	0.86	-4.41
\cdashline2-8	BART (ours)	-	44.31	21.34	41.36	0.88	-4.44
PEGASUS (ours)	-	43.67	20.96	40.74	0.88	-4.42
GSum
â€ 
-	45.94	22.32	42.48	-	-
GSum (ours)	-	45.69	22.28	42.38	0.89	-4.11
EDURewriter
â€ 
-	43.09	20.24	40.52	-	-
ContextRewriter
â€ 
-	43.52	20.57	40.56	-	-
ExtAbs
â¢
(
BART
)
-abs	
z
=8	45.31	21.84	42.28	0.88	-4.25
ExtAbs
â¢
(
PEGASUS
)
-abs	
z
=8	45.06	22.02	42.09	0.88	-4.35
Reddit	Extractive SOTAâ€ 	-	27.01	7.06	22.70	-	-
MatchSum
â€ 
-	25.09	6.17	20.13	-	-
Extractor
â¢
(
BART
)
k
=5	27.94	7.63	23.59	0.84	-4.96
Extractor
â¢
(
PEGASUS
)
k
=5	28.05	7.50	23.56	0.84	-4.98
ExtAbs
â¢
(
BART
)
-ext	
k
=5	28.51	8.10	23.99	0.84	-4.95
ExtAbs
â¢
(
PEGASUS
)
-ext	
k
=5	28.00	7.73	23.52	0.84	-4.98
\cdashline2-8	BART (ours)	-	33.01	11.63	26.93	0.88	-4.70
PEGASUS (ours)	-	31.52	11.13	25.83	0.88	-4.77
GSum
â€ 
-	34.52	12.71	27.58	-	-
ExtAbs
â¢
(
BART
)
-abs	
z
=8	33.42	11.41	26.68	0.88	-4.49
ExtAbs
â¢
(
PEGASUS
)
-abs	
z
=8	31.84	10.16	25.41	0.88	-4.73
PubMed	Extractive SOTAâ€ 	-	43.08	16.71	38.30	-	-
MatchSum
â€ 
-	41.21	14.91	36.75	-	-
Extractor
â¢
(
BART
)
k
=22	43.48	17.27	40.72	0.85	-4.56
Extractor
â¢
(
PEGASUS
)
k
=22	43.54	17.22	40.78	0.85	-4.56
ExtAbs
â¢
(
BART
)
-ext	
k
=22	43.48	17.29	40.73	0.85	-4.56
ExtAbs
â¢
(
PEGASUS
)
-ext	
k
=22	43.71	17.42	40.93	0.85	-4.55
\cdashline2-8	BART (ours)	-	43.57	16.47	40.17	0.86	-4.63
PEGASUS (ours)	-	43.41	17.13	39.93	0.86	-4.64
GSum
â€ 
-	45.09	16.72	41.32	-	-
ExtAbs
â¢
(
BART
)
-abs	
z
=25	43.90	16.12	40.49	0.86	-4.41
ExtAbs
â¢
(
PEGASUS
)
-abs	
z
=25	43.61	17.26	40.16	0.86	-4.39
Table 2:Experimental results on test sets of three datasets. ExtAbs(*) refers to our adapted version of the corresponding vanilla encoder-decoder model *. ext and abs refer to the extractive and abstractive results, respectively. â€  indicates that the results are copied from the corresponding original paper. 
k
 and 
z
 refer to the number of extracted textual segments for extractive summary and saliency mask, respectively, determined by validation sets. The best and second scores within each block are bold and underlined, respectively.
5Experiments
5.1Experimental Setup
Datasets
CNN/DailyMail (CNN/DM) Hermann et al. (2015) is a widely used dataset for summarization tasks. Each news article comes with several highlight sentences written by humans as the reference summary in the dataset. Reddit Kim et al. (2019) is crawled from the social media forum Reddit where the content in the crawled post and TL;DR are treated as the document and reference summary, respectively. PubMed Cohan et al. (2018) is collected from the scientific paper repository PubMed.com, where the abstract is taken as the reference summary. Following Zhong et al. (2020), we use the truncated version of PubMed with the introduction section in the paper as the document. The detailed statistics about the three datasets are listed in Appendix C.

Baselines
BART Lewis et al. (2020) and PEGASUS Zhang et al. (2020a) are the two most widely adopted pre-trained encoder-decoder models for abstractive summarization, and our proposed ExtAbs is built upon them. We also include the Extractor in ExtAbs built upon their encoders only as extractive baselines. ContextRewriter Bao and Zhang (2021) and EDURewriter Xiong et al. (2022) are two extract-then-abstract models with the same highlight method but the former one highlights at sentence level, while the latter one highlights EDUs. GSum Dou et al. (2021) extends BART with an extra encoder for highlighting purposes and achieves superior performance on multiple datasets. Notably, GSum serves as the abstractor in the extract-then-abstract paradigm only when it takes the extracted sentences as guidance. MatchSum Zhong et al. (2020) is the extractor of salient sentences used in GSum. EDU-VL Wu et al. (2023) is an extractive model that extracts EDUs from the input document and achieves SOTA results on CNN/DM and Reddit. MemSum Gu et al. (2022) is a reinforcement learning-based extractive model, achieving SOTA performance on PubMed. We also include GPT-4 Achiam et al. (2023) as a strong LLM baseline by following Zhang et al. (2023)â€™s work to prompt it to perform extractive, abstractive and extract-then-abstract summarization tasks on a randomly selected subset for each dataset.

Evaluation Metrics
Automatic and human evaluations are conducted to evaluate the model performance comprehensively. The automatic evaluation metrics include ROUGE-1/2/L Lin (2004), BERTScore 2Zhang et al. (2020b) and BARTScore 3Yuan et al. (2021) to measure from lexical and semantic perspectives. Human evaluation metrics include factuality, informativeness and ranking.

Implementation Details
All models are trained using Pytorch on up to four A100 80G GPUs. The checkpoint with the best ROUGE-L score on the validation set is taken as the final model for each experiment. More implementation details about hyperparameters are provided in Appendix D.

5.2Results
Main experimental results are presented in Table 2, and the corresponding statistical significance test to determine if an improvement is significant is presented in Appendix E. The BERTScore for all models are very close to each other; therefore, we will ignore them from the discussion.

Main Results
On CNN/DM, while the proposed 
ExtAbs
â¢
(
âˆ—
)
 underperforms the SOTA extractive model on all ROUGE scores, it significantly outperforms MatchSum on ROUGE-2/L. In the abstractive task, 
ExtAbs
â¢
(
BART
)
 and 
ExtAbs
â¢
(
PEGASUS
)
 significantly outperform their vanilla counterparts on all ROUGE scores. 
ExtAbs
â¢
(
âˆ—
)
 also surpasses other extract-then-abstract models like EDURewriter and ContextRewriter. Though GSum achieves the best results, it is noteworthy that GSum primarily focuses on abstractive performance, whereas ExtAbs seamlessly unifies extractive and abstractive summarization within a single model. On Reddit and PubMed, 
ExtAbs
â¢
(
âˆ—
)
 outperforms the SOTA extractive model and significantly outperforms MatchSum on all ROUGE scores, while achieving comparable scores to the vanilla abstractive baseline, i.e., the scores on some metrics are higher while some are lower. Besides, varying degrees of improvement are observed when comparing 
ExtAbs
â¢
(
âˆ—
)
 to the corresponding 
Extractor
â¢
(
âˆ—
)
 across three datasets, indicating that joint training enhances extractive performance.

Model	Task	R-1	R-2	R-L	BS
CNN/DailyMail
ExtAbs	ext	42.99	19.70	40.67	-4.43
abs	44.26	21.31	41.82	-4.31
\cdashline1-6 GPT-4 	ext	38.60	14.65	32.07	-4.89
abs	36.17	12.68	28.45	-4.80
ext-abs	35.13	12.16	27.73	-4.92
Reddit
ExtAbs	ext	27.80	9.08	23.87	-4.86
abs	34.40	12.31	26.86	-4.64
\cdashline1-6 GPT-4 	ext	26.94	6.83	19.40	-4.95
abs	25.97	6.97	18.63	-4.95
ext-abs	24.01	5.18	16.99	-5.15
PubMed
ExtAbs	ext	44.79	18.47	42.09	-4.58
abs	44.72	17.33	40.98	-4.67
\cdashline1-6 GPT-4 	ext	40.86	14.62	32.19	-5.11
abs	37.30	11.47	29.28	-5.25
ext-abs	40.16	12.62	31.06	-5.14
Table 3:Results on 50 randomly sampled instances from test sets. Here BS refers to BARTScore.
GPT-4 Results
As shown in Table 3, GPT-4 underperforms 
ExtAbs
â¢
(
BART
)
 on all extractive-only, abstractive-only and extract-then-abstract settings on all metrics.

Discussion
We attribute the inconsistent abstractive performance of ExtAbs across the three datasets to the quality of extractive summaries from the extractor and the potential boost from the proposed saliency mask. The average ROUGE score gain is significantly higher on CNN/DM (13.35) than on Reddit (6.22) and PubMed (2.24) when inferring with saliency mask derived from the reference summary, suggesting a larger potential boost on CNN/DM. The comparable abstractive performance of ExtAbs on Reddit is likely due to lower extractive summary quality, while PubMed shows less potential boost from the saliency mask. More details are provided in Appendix F.

5.3Ablation Analysis
Results of ablation analysis are shown in Table 4.

Hyperparameter	Task	R-1	R-2	R-L	BS
ExtAbs
â¢
(
BART
)
ext	43.96	21.59	41.78	-4.37
abs	45.31	21.84	42.28	-4.25
\cdashline2-6 input = sentences 	ext	43.47	20.82	39.95	-4.44
abs	45.14	21.80	42.08	-4.26
\cdashline2-6 
Î±
=
50
 	ext	44.07	21.62	41.86	-4.38
abs	45.01	21.60	41.98	-4.28
\cdashline2-6 w/o 
m
â¢
a
â¢
s
â¢
k
 	ext	42.82	20.51	40.70	-4.41
abs	44.78	21.59	41.72	-4.26
Table 4:Results of the ablation analysis on the proposed 
ExtAbs
â¢
(
BART
)
 with different input granularities and 
Î±
 values, with and without saliency mask.
Granularity of Highlighted Information
The sentence-level 
ExtAbs
â¢
(
BART
)
, using sentences as textual segments, is trained to compare with the EDU-level one. Abstractive summaries generated by the sentence-level model achieve scores comparable to those of the EDU-level model. However, there is a significant decrease in all ROUGE scores and BARTScore for extractive summaries derived from the sentence-level model. This observation echoes the conclusion drawn by Li et al. (2016) and Wu et al. (2023), i.e., EDU is a better text unit for extractive summarization.

Extractive vs. Abstractive
We further tune the hyperparameter 
Î±
 to validate the influence made by the loss function. The increase of 
Î±
 (weight of extractive loss) results in better scores for the extractive summary while lower scores for the abstractive summary. The result suggests a tradeoff between extractive and abstractive summarization performances within one single model.

Saliency Mask
To validate the effectiveness of the proposed saliency mask in ExtAbs, we compare model performance with and without the proposed saliency mask. It is observed that there is a decrease in all four metrics for both extractive and abstractive summaries, demonstrating the necessity of the saliency mask.

5.4Human Evaluation
Model	Fact.	Info.	Ranking
BART	0.80	0.20	2.20
ExtAbs
â¢
(
BART
)
-ext	1.00	0.37	2.07
ExtAbs
â¢
(
BART
)
-abs	0.70	0.37	1.73
Table 5:Human evaluation results on sampled instances.
We randomly sample 30 CNN/DM test instances to compare summaries generated by the baseline BART and our proposed 
ExtAbs
â¢
(
BART
)
. Annotators are asked to rate either 0 or 1 to indicate whether the generated summary is faithful to the source document (factuality) and contains all salient information from the reference summary (informativeness), and rank among the three summaries for the overall quality of a summary. Table 5 presents the averaged human evaluation results. Firstly, the extractive summaries are entirely factual, while varying degrees of hallucination are observed in abstractive summaries, aligning with expectations for extractive summaries. Secondly, informativeness scores are relatively low across all summary types, indicating the challenge of comprehensively capturing all salient information. Overall, abstractive summaries generated by our proposed 
ExtAbs
â¢
(
BART
)
 achieve the lowest ranking value, suggesting annotatorsâ€™ preference over the baseline. Additional evaluation results from GPT-4 of these samples on more aspects are in Appendix G.

6Conclusion
In this paper, we discover and highlight the importance of highlighting salient information in the extract-then-abstract paradigm by applying the saliency mask in the decoder of the abstractor. Our proposed saliency mask is parameter-free and achieves higher ROUGE scores than other highlight methods. Then, we propose ExtAbs, an extract-and-abstract framework to unify any encoder-decoder model to jointly and seamlessly perform extractive and abstractive summarization tasks. In ExtAbs, the encoder is augmented and serves as the extractor, and the decoder along with the encoder serves as the abstractor. Our experiments on Reddit and PubMed demonstrate that the proposed method generates better extractive summaries and performs comparable, or even better than the vanilla model on the abstractive task.

7Limitations
Firstly, the proposed ExtAbs has only been tested on BART and PEGASUS, but we acknowledge that there are other widely used pre-trained encoder-decoder models, such as the T5 family. It could be worthwhile to conduct experiments with more baseline models given sufficient time and resources. Secondly, the proposed highlight method, i.e., saliency mask, can only be applied to the encoder-decoder models and cannot be extended to the decoder-only models directly, e.g., the GPT family. Considering the recent popularity of the decoder-only model, it is worth exploring a compatible way for the decoder-only models, such as integrating the saliency mask with the original self-attention mask. We leave such exploration for future work.
