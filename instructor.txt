======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/docs/overrides/main.html
=======
{% extends "base.html" %} {% block announce %}
<script>
    !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.async=!0,p.src=s.api_host+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys onSessionId".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
    posthog.init('phc_bAUjZfg1PI0Ca2IOQCM053Y5873PRZhJ0DvTDbGsN9A',{api_host:'https://p.useinstructor.com'})
</script>

For updates follow
<strong>@jxnlco</strong> on
<a href="https://twitter.com/jxnlco">
  <span class="twemoji twitter">
    {% include ".icons/fontawesome/brands/twitter.svg" %}
  </span>
  <strong>Twitter</strong>
</a>
and
<span class="twemoji star">
  {% include ".icons/fontawesome/solid/star.svg" %}
</span>
us on
<a href="https://www.github.com/jxnl/instructor">
  <span class="twemoji github">
    {% include ".icons/fontawesome/brands/github.svg" %}
  </span>
  <strong>GitHub</strong> </a
>. If you don't like python, check out the
<a href="https://instructor-ai.github.io/instructor-js/"
  ><strong>TS/JS</strong></a
>
and
<a href="https://hexdocs.pm/instructor/Instructor.html">
  <strong>Elixir</strong>
</a>
ports. {% endblock %}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/anthropic/run.py
=======
from pydantic import BaseModel
import anthropic
import instructor

# Patching the Anthropics client with the instructor for enhanced capabilities
client = instructor.from_anthropic(anthropic.Anthropic())


class Properties(BaseModel):
    key: str
    value: str


class User(BaseModel):
    name: str
    age: int
    properties: list[Properties]


user = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1024,
    max_retries=0,
    messages=[
        {
            "role": "user",
            "content": "Create a user for a model with a name, age, and properties.",
        }
    ],
    response_model=User,
)

print(user.model_dump_json(indent=2))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/auto-ticketer/run.py
=======
import instructor
from openai import OpenAI

from typing import Optional
from pydantic import BaseModel, Field
from enum import Enum

client = instructor.from_openai(OpenAI())


class PriorityEnum(str, Enum):
    high = "High"
    medium = "Medium"
    low = "Low"


class Subtask(BaseModel):
    """
    Correctly resolved subtask from the given transcript
    """

    id: int = Field(..., description="Unique identifier for the subtask")
    name: str = Field(..., description="Informative title of the subtask")


class Ticket(BaseModel):
    """
    Correctly resolved ticket from the given transcript
    """

    id: int = Field(..., description="Unique identifier for the ticket")
    name: str = Field(..., description="Title of the task")
    description: str = Field(..., description="Detailed description of the task")
    priority: PriorityEnum = Field(..., description="Priority level")
    assignees: list[str] = Field(..., description="List of users assigned to the task")
    subtasks: Optional[list[Subtask]] = Field(
        None, description="List of subtasks associated with the main task"
    )
    dependencies: Optional[list[int]] = Field(
        None, description="List of ticket IDs that this ticket depends on"
    )


class ActionItems(BaseModel):
    """
    Correctly resolved set of action items from the given transcript
    """

    items: list[Ticket]


def generate(data: str):
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=ActionItems,
        messages=[
            {
                "role": "system",
                "content": "The following is a transcript of a meeting between a manager and their team. The manager is assigning tasks to their team members and creating action items for them to complete.",
            },
            {
                "role": "user",
                "content": f"Create the action items for the following transcript: {data}",
            },
        ],
    )


prediction = generate(
    """
Alice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.

Bob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?

Alice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.

Carol: I can help with the front-end part of the authentication system.

Bob: Great, Carol. I'll handle the back-end optimization then.

Alice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.

Carol: Is the new billing system already in place?

Alice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?

Bob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.

Alice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.

Carol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.

Alice: Sounds like a plan. Let's get these tasks modeled out and get started."""
)

print(prediction.model_dump_json(indent=2))
"""
{
  "items": [
    {
      "id": 1,
      "name": "Improve Authentication System",
      "description": "Revamp the front-end and optimize the back-end of the authentication system",
      "priority": "High",
      "assignees": [
        "Bob",
        "Carol"
      ],
      "subtasks": [
        {
          "id": 2,
          "name": "Front-end Revamp"
        },
        {
          "id": 3,
          "name": "Back-end Optimization"
        }
      ],
      "dependencies": []
    },
    {
      "id": 4,
      "name": "Integrate Authentication System with Billing System",
      "description": "Integrate the improved authentication system with the new billing system",
      "priority": "Medium",
      "assignees": [
        "Bob"
      ],
      "subtasks": [],
      "dependencies": [
        1
      ]
    },
    {
      "id": 5,
      "name": "Update User Documentation",
      "description": "Update the user documentation to reflect the changes in the authentication system",
      "priority": "Low",
      "assignees": [
        "Carol"
      ],
      "subtasks": [],
      "dependencies": [
        2
      ]
    }
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/avail/run.py
=======
from pydantic import BaseModel, Field
from typing import Literal
from collections.abc import Iterable
from datetime import datetime, timedelta

from openai import OpenAI
import instructor

client = instructor.from_openai(OpenAI())


class DateRange(BaseModel):
    explain: str = Field(
        ...,
        description="Explain the date range in the context of the text before generating the date range and the repeat pattern.",
    )
    repeats: Literal["daily", "weekly", "monthly", None] = Field(
        default=None,
        description="If the date range repeats, and how often, this way we can generalize the date range to the future., if its special, then we can assume it is a one time event.",
    )
    days_of_week: list[
        Literal[
            "monday",
            "tuesday",
            "wednesday",
            "thursday",
            "friday",
            "saturday",
            "sunday",
            None,
        ]
    ] = Field(
        ...,
        description="If the date range repeats, which days of the week does it repeat on.",
    )
    time_start: datetime = Field(
        description="The start of the first time range in the day."
    )
    time_end: datetime = Field(
        description="The end of the first time range in the day."
    )


class AvailabilityResponse(BaseModel):
    availability: list[DateRange]


def prepare_dates(n=7) -> str:
    # Current date and time
    now = datetime.now()

    acc = ""
    # Loop for the next 7 days
    for i in range(n):
        # Calculate the date for each day
        day = now + timedelta(days=i)
        # Print the day of the week, date, and time
        acc += "\n" + day.strftime("%A, %Y-%m-%d %H:%M:%S")

    return acc.strip()


def parse_availability(text: str) -> Iterable[AvailabilityResponse]:
    return client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[
            {
                "role": "system",
                "content": "You are a state of the art date range parse designed to correctly extract availabilities.",
            },
            {
                "role": "user",
                "content": text,
            },
            {
                "role": "user",
                "content": f"To help you understand the dates, here are the next 7 days: {prepare_dates()}",
            },
        ],
        response_model=Iterable[AvailabilityResponse],
    )


if __name__ == "__main__":
    text = """
    #1
    
    12/8-12/24
    9am - 5pm Monday - Saturday
    10am - 5pm Sunday

    #2
    We are open Friday, after Thanksgiving, and then Saturdays and Sundays 9 a.m. till dusk.``
    """
    schedules = parse_availability(text)
    for schedule in schedules:
        print(schedule.model_dump_json(indent=2))
        {
            "availability": [
                {
                    "explain": "For the first date range, the availability is from December 8 to December 24, from 9 am to 5 pm on Mondays through Saturdays",
                    "repeats": "weekly",
                    "days_of_week": [
                        "monday",
                        "tuesday",
                        "wednesday",
                        "thursday",
                        "friday",
                        "saturday",
                    ],
                    "time_start": "2023-12-08T09:00:00",
                    "time_end": "2023-12-08T17:00:00",
                },
                {
                    "explain": "For the same date range, the availability on Sundays is from 10 am to 5 pm",
                    "repeats": "weekly",
                    "days_of_week": ["sunday"],
                    "time_start": "2023-12-10T10:00:00",
                    "time_end": "2023-12-10T17:00:00",
                },
            ]
        }
    {
        "availability": [
            {
                "explain": "The second date range starting from the Friday after Thanksgiving, which is November 24, 2023, and then on Saturdays and Sundays from 9 am until dusk. Assuming 'dusk' means approximately 5 pm, similar to the previous timings.",
                "repeats": "weekly",
                "days_of_week": ["friday", "saturday", "sunday"],
                "time_start": "2023-11-24T09:00:00",
                "time_end": "2023-11-24T17:00:00",
            }
        ]
    }


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/avail/run_mixtral.py
=======
import os
from pydantic import BaseModel, Field
from typing import Literal
from datetime import datetime, timedelta

from openai import OpenAI
import instructor

client = instructor.from_openai(
    OpenAI(
        base_url="https://api.endpoints.anyscale.com/v1",
        api_key=os.environ["ANYSCALE_API_KEY"],
    ),
    mode=instructor.Mode.JSON_SCHEMA,
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
)


class DateRange(BaseModel):
    explain: str = Field(
        ...,
        description="Explain the date range in the context of the text before generating the date range and the repeat pattern.",
    )
    repeats: Literal["daily", "weekly", "monthly", None] = Field(
        default=None,
        description="If the date range repeats, and how often, this way we can generalize the date range to the future., if its special, then we can assume it is a one time event.",
    )
    days_of_week: list[
        Literal[
            "monday",
            "tuesday",
            "wednesday",
            "thursday",
            "friday",
            "saturday",
            "sunday",
            None,
        ]
    ] = Field(
        ...,
        description="If the date range repeats, which days of the week does it repeat on.",
    )
    time_start: datetime = Field(
        description="The start of the first time range in the day."
    )
    time_end: datetime = Field(
        description="The end of the first time range in the day."
    )


class AvailabilityResponse(BaseModel):
    availability: list[DateRange]


def prepare_dates(n=7) -> str:
    # Current date and time
    now = datetime.now()

    acc = ""
    # Loop for the next 7 days
    for i in range(n):
        # Calculate the date for each day
        day = now + timedelta(days=i)
        # Print the day of the week, date, and time
        acc += "\n" + day.strftime("%A, %Y-%m-%d %H:%M:%S")

    return acc.strip()


def parse_availability(text: str):
    return client.chat.completions.create_iterable(
        max_tokens=10000,
        messages=[
            {
                "role": "system",
                "content": "You are a state of the art date range parse designed to correctly extract availabilities.",
            },
            {
                "role": "user",
                "content": text,
            },
            {
                "role": "user",
                "content": f"To help you understand the dates, here are the next 7 days: {prepare_dates()}",
            },
        ],
        response_model=AvailabilityResponse,
        max_retries=3,
    )


if __name__ == "__main__":
    text = """
    #1
    
    12/8-12/24
    9am - 5pm Monday - Saturday
    10am - 5pm Sunday

    #2
    We are open Friday, after Thanksgiving, and then Saturdays and Sundays 9 a.m. till dusk.``
    """
    schedules = parse_availability(text)
    for schedule in schedules:
        print(schedule.model_dump_json(indent=2))
        {
            "availability": [
                {
                    "explain": "For the first date range, the availability is from December 8 to December 24, from 9 am to 5 pm on Mondays through Saturdays",
                    "repeats": "weekly",
                    "days_of_week": [
                        "monday",
                        "tuesday",
                        "wednesday",
                        "thursday",
                        "friday",
                        "saturday",
                    ],
                    "time_start": "2023-12-08T09:00:00",
                    "time_end": "2023-12-08T17:00:00",
                },
                {
                    "explain": "For the same date range, the availability on Sundays is from 10 am to 5 pm",
                    "repeats": "weekly",
                    "days_of_week": ["sunday"],
                    "time_start": "2023-12-10T10:00:00",
                    "time_end": "2023-12-10T17:00:00",
                },
            ]
        }
    {
        "availability": [
            {
                "explain": "The second date range starting from the Friday after Thanksgiving, which is November 24, 2023, and then on Saturdays and Sundays from 9 am until dusk. Assuming 'dusk' means approximately 5 pm, similar to the previous timings.",
                "repeats": "weekly",
                "days_of_week": ["friday", "saturday", "sunday"],
                "time_start": "2023-11-24T09:00:00",
                "time_end": "2023-11-24T17:00:00",
            }
        ]
    }


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/batch-classification/run-cache.py
=======
import instructor
import asyncio

from openai import AsyncOpenAI
from pydantic import BaseModel, Field, field_validator
from enum import Enum

client = instructor.from_openai(AsyncOpenAI(), mode=instructor.Mode.TOOLS)
sem = asyncio.Semaphore(5)


class QuestionType(Enum):
    CONTACT = "CONTACT"
    TIMELINE_QUERY = "TIMELINE_QUERY"
    DOCUMENT_SEARCH = "DOCUMENT_SEARCH"
    COMPARE_CONTRAST = "COMPARE_CONTRAST"
    EMAIL = "EMAIL"
    PHOTOS = "PHOTOS"
    SUMMARY = "SUMMARY"


# You can add more instructions and examples in the description
# or you can put it in the prompt in `messages=[...]`
class QuestionClassification(BaseModel):
    """
    Predict the type of question that is being asked.
    Here are some tips on how to predict the question type:
    CONTACT: Searches for some contact information.
    TIMELINE_QUERY: "When did something happen?
    DOCUMENT_SEARCH: "Find me a document"
    COMPARE_CONTRAST: "Compare and contrast two things"
    EMAIL: "Find me an email, search for an email"
    PHOTOS: "Find me a photo, search for a photo"
    SUMMARY: "Summarize a large amount of data"
    """

    # If you want only one classification, just change it to
    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``
    chain_of_thought: str = Field(
        ..., description="The chain of thought that led to the classification"
    )
    classification: list[QuestionType] = Field(
        description=f"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used",
    )

    @field_validator("classification", mode="before")
    def validate_classification(cls, v):
        # sometimes the API returns a single value, just make sure it's a list
        if not isinstance(v, list):
            v = [v]
        return v


# Modify the classify function
async def classify(data: str):
    async with sem:  # some simple rate limiting
        return data, await client.chat.completions.create(
            model="gpt-4",
            response_model=QuestionClassification,
            max_retries=2,
            messages=[
                {
                    "role": "user",
                    "content": f"Classify the following question: {data}",
                },
            ],
        )


async def main(questions: list[str]):
    tasks = [classify(question) for question in questions]
    resps = []
    for task in asyncio.as_completed(tasks):
        question, label = await task
        resp = {
            "question": question,
            "classification": [c.value for c in label.classification],
            "chain_of_thought": label.chain_of_thought,
        }
        resps.append(resp)
    return resps


if __name__ == "__main__":
    import asyncio

    questions = [
        "What was that ai app that i saw on the news the other day?",
        "Can you find the trainline booking email?",
        "What was the book I saw on amazon yesturday?",
        "Can you speak german?",
        "Do you have access to the meeting transcripts?",
        "what are the recent sites I visited?",
        "what did I do on Monday?",
        "Tell me about todays meeting and how it relates to the email on Monday",
    ]

    asyncio.run(main(questions))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/batch-classification/run.py
=======
import json
import instructor
import asyncio

from openai import AsyncOpenAI
from pydantic import BaseModel, Field, field_validator
from enum import Enum

client = AsyncOpenAI()
client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)
sem = asyncio.Semaphore(5)


class QuestionType(Enum):
    CONTACT = "CONTACT"
    TIMELINE_QUERY = "TIMELINE_QUERY"
    DOCUMENT_SEARCH = "DOCUMENT_SEARCH"
    COMPARE_CONTRAST = "COMPARE_CONTRAST"
    EMAIL = "EMAIL"
    PHOTOS = "PHOTOS"
    SUMMARY = "SUMMARY"


# You can add more instructions and examples in the description
# or you can put it in the prompt in `messages=[...]`
class QuestionClassification(BaseModel):
    """
    Predict the type of question that is being asked.
    Here are some tips on how to predict the question type:
    CONTACT: Searches for some contact information.
    TIMELINE_QUERY: "When did something happen?
    DOCUMENT_SEARCH: "Find me a document"
    COMPARE_CONTRAST: "Compare and contrast two things"
    EMAIL: "Find me an email, search for an email"
    PHOTOS: "Find me a photo, search for a photo"
    SUMMARY: "Summarize a large amount of data"
    """

    # If you want only one classification, just change it to
    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``
    chain_of_thought: str = Field(
        ..., description="The chain of thought that led to the classification"
    )
    classification: list[QuestionType] = Field(
        description=f"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used",
    )

    @field_validator("classification", mode="before")
    def validate_classification(cls, v):
        # sometimes the API returns a single value, just make sure it's a list
        if not isinstance(v, list):
            v = [v]
        return v


async def classify(data: str):
    async with sem:  # some simple rate limiting
        return data, await client.chat.completions.create(
            model="gpt-4",
            response_model=QuestionClassification,
            max_retries=2,
            messages=[
                {
                    "role": "user",
                    "content": f"Classify the following question: {data}",
                },
            ],
        )


async def main(questions: list[str], *, path_to_jsonl: str = None):
    tasks = [classify(question) for question in questions]
    for task in asyncio.as_completed(tasks):
        question, label = await task
        resp = {
            "question": question,
            "classification": [c.value for c in label.classification],
        }
        print(resp)
        if path_to_jsonl:
            with open(path_to_jsonl, "a") as f:
                json_dump = json.dumps(resp)
                f.write(json_dump + "\n")


if __name__ == "__main__":
    import asyncio

    questions = [
        "What was that ai app that i saw on the news the other day?",
        "Can you find the trainline booking email?",
        "What was the book I saw on amazon yesturday?",
        "Can you speak german?",
        "Do you have access to the meeting transcripts?",
        "what are the recent sites I visited?",
        "what did I do on Monday?",
        "Tell me about todays meeting and how it relates to the email on Monday",
    ]

    asyncio.run(main(questions))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/batch-classification/run_langsmith.py
=======
import instructor
import asyncio

from langsmith import traceable
from langsmith.wrappers import wrap_openai

from openai import AsyncOpenAI
from pydantic import BaseModel, Field, field_validator
from enum import Enum

client = wrap_openai(AsyncOpenAI())
client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)
sem = asyncio.Semaphore(5)


class QuestionType(Enum):
    CONTACT = "CONTACT"
    TIMELINE_QUERY = "TIMELINE_QUERY"
    DOCUMENT_SEARCH = "DOCUMENT_SEARCH"
    COMPARE_CONTRAST = "COMPARE_CONTRAST"
    EMAIL = "EMAIL"
    PHOTOS = "PHOTOS"
    SUMMARY = "SUMMARY"


# You can add more instructions and examples in the description
# or you can put it in the prompt in `messages=[...]`
class QuestionClassification(BaseModel):
    """
    Predict the type of question that is being asked.
    Here are some tips on how to predict the question type:
    CONTACT: Searches for some contact information.
    TIMELINE_QUERY: "When did something happen?
    DOCUMENT_SEARCH: "Find me a document"
    COMPARE_CONTRAST: "Compare and contrast two things"
    EMAIL: "Find me an email, search for an email"
    PHOTOS: "Find me a photo, search for a photo"
    SUMMARY: "Summarize a large amount of data"
    """

    # If you want only one classification, just change it to
    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``
    chain_of_thought: str = Field(
        ..., description="The chain of thought that led to the classification"
    )
    classification: list[QuestionType] = Field(
        description=f"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used",
    )

    @field_validator("classification", mode="before")
    def validate_classification(cls, v):
        # sometimes the API returns a single value, just make sure it's a list
        if not isinstance(v, list):
            v = [v]
        return v


# Modify the classify function
@traceable(name="classify-question")
async def classify(data: str):
    async with sem:  # some simple rate limiting
        return data, await client.chat.completions.create(
            model="gpt-4",
            response_model=QuestionClassification,
            max_retries=2,
            messages=[
                {
                    "role": "user",
                    "content": f"Classify the following question: {data}",
                },
            ],
        )


async def main(questions: list[str]):
    tasks = [classify(question) for question in questions]
    resps = []
    for task in asyncio.as_completed(tasks):
        question, label = await task
        resp = {
            "question": question,
            "classification": [c.value for c in label.classification],
            "chain_of_thought": label.chain_of_thought,
        }
        resps.append(resp)
    return resps


if __name__ == "__main__":
    import asyncio

    questions = [
        "What was that ai app that i saw on the news the other day?",
        "Can you find the trainline booking email?",
        "What was the book I saw on amazon yesturday?",
        "Can you speak german?",
        "Do you have access to the meeting transcripts?",
        "what are the recent sites I visited?",
        "what did I do on Monday?",
        "Tell me about todays meeting and how it relates to the email on Monday",
    ]

    asyncio.run(main(questions))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/caching/example_diskcache.py
=======
import functools
import inspect
import instructor
import diskcache

from openai import OpenAI, AsyncOpenAI
from pydantic import BaseModel

client = instructor.from_openai(OpenAI())
aclient = instructor.from_openai(AsyncOpenAI())


class UserDetail(BaseModel):
    name: str
    age: int


cache = diskcache.Cache("./my_cache_directory")


def instructor_cache(func):
    """Cache a function that returns a Pydantic model"""
    return_type = inspect.signature(func).return_annotation
    if not issubclass(return_type, BaseModel):
        raise ValueError("The return type must be a Pydantic model")

    is_async = inspect.iscoroutinefunction(func)

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        key = f"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}"
        # Check if the result is already cached
        if (cached := cache.get(key)) is not None:
            # Deserialize from JSON based on the return type
            if issubclass(return_type, BaseModel):
                return return_type.model_validate_json(cached)

        # Call the function and cache its result
        result = func(*args, **kwargs)
        serialized_result = result.model_dump_json()
        cache.set(key, serialized_result)

        return result

    @functools.wraps(func)
    async def awrapper(*args, **kwargs):
        key = f"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}"
        # Check if the result is already cached
        if (cached := cache.get(key)) is not None:
            # Deserialize from JSON based on the return type
            if issubclass(return_type, BaseModel):
                return return_type.model_validate_json(cached)

        # Call the function and cache its result
        result = await func(*args, **kwargs)
        serialized_result = result.model_dump_json()
        cache.set(key, serialized_result)

        return result

    return wrapper if not is_async else awrapper


@instructor_cache
def extract(data) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": data},
        ],
    )  # type: ignore


@instructor_cache
async def aextract(data) -> UserDetail:
    return await aclient.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": data},
        ],
    )  # type: ignore


def test_extract():
    import time

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")


async def atest_extract():
    import time

    start = time.perf_counter()
    model = await aextract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")

    start = time.perf_counter()
    model = await aextract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")


if __name__ == "__main__":
    test_extract()
    # Time taken: 0.7285366660216823
    # Time taken: 9.841693099588156e-05

    import asyncio

    asyncio.run(atest_extract())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/caching/example_redis.py
=======
import redis
import functools
import inspect
import instructor

from pydantic import BaseModel
from openai import OpenAI

client = instructor.from_openai(OpenAI())
cache = redis.Redis("localhost")


def instructor_cache(func):
    """Cache a function that returns a Pydantic model"""
    return_type = inspect.signature(func).return_annotation
    if not issubclass(return_type, BaseModel):
        raise ValueError("The return type must be a Pydantic model")

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        key = f"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}"
        # Check if the result is already cached
        if (cached := cache.get(key)) is not None:
            # Deserialize from JSON based on the return type
            if issubclass(return_type, BaseModel):
                return return_type.model_validate_json(cached)

        # Call the function and cache its result
        result = func(*args, **kwargs)
        serialized_result = result.model_dump_json()
        cache.set(key, serialized_result)

        return result

    return wrapper


class UserDetail(BaseModel):
    name: str
    age: int


@instructor_cache
def extract(data) -> UserDetail:
    # Assuming client.chat.completions.create returns a UserDetail instance
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": data},
        ],
    )


def test_extract():
    import time

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")


if __name__ == "__main__":
    test_extract()
    # Time taken: 0.798335583996959
    # Time taken: 0.00017016706988215446


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/caching/lru.py
=======
import instructor
from openai import OpenAI
from pydantic import BaseModel
import functools

client = instructor.from_openai(OpenAI())


class UserDetail(BaseModel):
    name: str
    age: int


@functools.lru_cache
def extract(data):
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": data},
        ],
    )


def test_extract():
    import time

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")

    start = time.perf_counter()
    model = extract("Extract jason is 25 years old")
    assert model.name.lower() == "jason"
    assert model.age == 25
    print(f"Time taken: {time.perf_counter() - start}")


if __name__ == "__main__":
    test_extract()
    # Time taken: 0.9267581660533324
    # Time taken: 1.2080417945981026e-06


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/chain-of-density/chain_of_density.py
=======
from pydantic import BaseModel, Field, field_validator
import instructor
import nltk
from openai import OpenAI
import spacy

client = instructor.from_openai(OpenAI())
nlp = spacy.load("en_core_web_sm")


class InitialSummary(BaseModel):
    """
    This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.
    """

    summary: str = Field(
        ...,
        description="This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length",
    )


class RewrittenSummary(BaseModel):
    """
    This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.

    Guidelines
    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities
    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.
    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.
    - Make space with fusion, compression, and removal of uninformative phrases like "the article discusses"
    - Missing entities can appear anywhere in the new summary

    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.
    """

    summary: str = Field(
        ...,
        description="This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article",
    )
    absent: list[str] = Field(
        ...,
        default_factory=list,
        description="this is a list of Entities found absent from the new summary that were present in the previous summary",
    )
    missing: list[str] = Field(
        default_factory=list,
        description="This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.",
    )

    @field_validator("summary")
    def min_entity_density(cls, v: str):
        # We want to make sure we have a minimum density of 0.12 whenever we do a rewrite. This ensures that the summary quality is always going up
        tokens = nltk.word_tokenize(v)
        num_tokens = len(tokens)

        # Extract Entities
        doc = nlp(v)
        num_entities = len(doc.ents)

        density = num_entities / num_tokens
        if density < 0.08:
            raise ValueError(
                f"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary."
            )

        return v

    @field_validator("summary")
    def min_length(cls, v: str):
        tokens = nltk.word_tokenize(v)
        num_tokens = len(tokens)
        if num_tokens < 60:
            raise ValueError(
                "The current summary is too short. Please make sure that you generate a new summary that is around 80 words long."
            )
        return v

    @field_validator("missing")
    def has_missing_entities(cls, missing_entities: list[str]):
        if len(missing_entities) == 0:
            raise ValueError(
                "You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary"
            )
        return missing_entities

    @field_validator("absent")
    def has_no_absent_entities(cls, absent_entities: list[str]):
        absent_entity_string = ",".join(absent_entities)
        if len(absent_entities) > 0:
            print(f"Detected absent entities of {absent_entity_string}")
            raise ValueError(
                f"Do not omit the following Entities {absent_entity_string} from the new summary"
            )
        return absent_entities


def summarize_article(article: str, summary_steps: int = 3):
    summary_chain = []
    # We first generate an initial summary
    summary: InitialSummary = client.chat.completions.create(
        model="gpt-4-0613",
        response_model=InitialSummary,
        messages=[
            {
                "role": "system",
                "content": "Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words. ",
            },
            {"role": "user", "content": f"Here is the Article: {article}"},
            {
                "role": "user",
                "content": "The generated summary should be about 80 words.",
            },
        ],
        max_retries=2,
    )
    summary_chain.append(summary.summary)
    for _i in range(summary_steps):
        new_summary: RewrittenSummary = client.chat.completions.create(
            model="gpt-4-0613",
            messages=[
                {
                    "role": "system",
                    "content": f"""
                Article: {article}
                You are going to generate an increasingly concise,entity-dense summary of the following article.

                Perform the following two tasks
                - Identify 1-3 informative entities from the following article which is missing from the previous summary
                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities

                Guidelines
                - Make every word count: re-write the previous summary to improve flow and make space for additional entities
                - Make space with fusion, compression, and removal of uninformative phrases like "the article discusses".
                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.
                - Missing entities can appear anywhere in the new summary
                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.
                """,
                },
                {
                    "role": "user",
                    "content": f"Here is the previous summary: {summary_chain[-1]}",
                },
            ],
            max_retries=5,
            max_tokens=1000,
            response_model=RewrittenSummary,
        )
        summary_chain.append(new_summary.summary)

    return summary_chain


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/chain-of-density/finetune.py
=======
from openai import OpenAI
from chain_of_density import summarize_article
import csv
import logging
import instructor
from pydantic import BaseModel, Field

logging.basicConfig(level=logging.INFO)

client = instructor.from_openai(OpenAI())

instructions = instructor.Instructions(
    name="Chain Of Density",
    finetune_format="messages",
    # log handler is used to save the data to a file
    # you can imagine saving it to a database or other storage
    # based on your needs!
    log_handlers=[logging.FileHandler("generated.jsonl")],
    openai_client=client,
)


class GeneratedSummary(BaseModel):
    """
    This represents a highly concise summary that includes as many entities as possible from the original source article.

    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.

    Guidelines
    - Make every word count
    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.
    - Make space with fusion, compression, and removal of uninformative phrases like "the article discusses"
    """

    summary: str = Field(
        ...,
        description="This represents the final summary generated that captures the meaning of the original article which is as concise as possible. ",
    )


@instructions.distil
def distil_summarization(text: str) -> GeneratedSummary:
    summary_chain: list[str] = summarize_article(text)
    return GeneratedSummary(summary=summary_chain[-1])


with open("test.csv") as file:
    reader = csv.reader(file)
    next(reader)  # Skip the header
    for article, _summary in reader:
        distil_summarization(article)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/citations/run.py
=======
from typing import Optional
from openai import OpenAI
from pydantic import (
    BaseModel,
    Field,
    ValidationError,
    ValidationInfo,
    field_validator,
    model_validator,
)

import instructor

client = instructor.from_openai(OpenAI())

""" 
Example 1) Simple Substring check that compares a citation to a text chunk
"""


class Statements(BaseModel):
    body: str
    substring_quote: str

    @field_validator("substring_quote")
    @classmethod
    def substring_quote_exists(cls, v: str, info: ValidationInfo):
        context = info.context.get("text_chunks", None)

        # Check if the substring_quote is in the text_chunk
        # if not, raise an error
        for text_chunk in context.values():
            if v in text_chunk:
                return v
        raise ValueError(
            f"Could not find substring_quote `{v}` in contexts",
        )


class AnswerWithCitaton(BaseModel):
    question: str
    answer: list[Statements]


try:
    AnswerWithCitaton.model_validate(
        {
            "question": "What is the capital of France?",
            "answer": [
                {"body": "Paris", "substring_quote": "Paris is the capital of France"},
            ],
        },
        context={
            "text_chunks": {
                1: "Jason is a pirate",
                2: "Paris is not the capital of France",
                3: "Irrelevant data",
            }
        },
    )
except ValidationError as e:
    print(e)
"""
answer.0.substring_quote
  Value error, Could not find substring_quote `Paris is the capital of France` in contexts [type=value_error, input_value='Paris is the capital of France', input_type=str]
    For further information visit https://errors.pydantic.dev/2.4/v/value_error
"""


""" 
Example 2) Using an LLM to verify if a 
"""


class Validation(BaseModel):
    """
    Verfication response from the LLM,
    the error message should be detailed if the is_valid is False
    but keep it to less than 100 characters, reference specific
    attributes that you are comparing, use `...` is the string is too long
    """

    is_valid: bool
    error_messages: Optional[str] = Field(None, description="Error messages if any")


class Statements(BaseModel):
    body: str
    substring_quote: str

    @model_validator(mode="after")
    def substring_quote_exists(self, info: ValidationInfo):
        context = info.context.get("text_chunks", None)

        resp: Validation = client.chat.completions.create(
            response_model=Validation,
            messages=[
                {
                    "role": "user",
                    "content": f"Does the following citation exist in the following context?\n\nCitation: {self.substring_quote}\n\nContext: {context}",
                }
            ],
            model="gpt-3.5-turbo",
        )

        if resp.is_valid:
            return self

        raise ValueError(resp.error_messages)


class AnswerWithCitaton(BaseModel):
    question: str
    answer: list[Statements]


resp = AnswerWithCitaton.model_validate(
    {
        "question": "What is the capital of France?",
        "answer": [
            {"body": "Paris", "substring_quote": "Paris is the capital of France"},
        ],
    },
    context={
        "text_chunks": {
            1: "Jason is a pirate",
            2: "Paris is the capital of France",
            3: "Irrelevant data",
        }
    },
)
# output: notice that there are no errors
print(resp.model_dump_json(indent=2))
{
    "question": "What is the capital of France?",
    "answer": [{"body": "Paris", "substring_quote": "Paris is the capital of France"}],
}

# Now we change the text chunk to something else, and we get an error
try:
    AnswerWithCitaton.model_validate(
        {
            "question": "What is the capital of France?",
            "answer": [
                {"body": "Paris", "substring_quote": "Paris is the capital of France"},
            ],
        },
        context={
            "text_chunks": {
                1: "Jason is a pirate",
                2: "Paris is not the capital of France",
                3: "Irrelevant data",
            }
        },
    )
except ValidationError as e:
    print(e)
""" 
1 validation error for AnswerWithCitaton
answer.0
  Value error, Citation not found in context [type=value_error, input_value={'body': 'Paris', 'substr... the capital of France'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.4/v/value_error
"""

# Example 3) Using an LLM to verify if the citations and the answers are all aligned


# we keep the same model as above for Statements, but we add a new model for the answer
# that also verifies that the citations are aligned with the answers
class AnswerWithCitaton(BaseModel):
    question: str
    answer: list[Statements]

    @model_validator(mode="after")
    def validate_answer(self, info: ValidationInfo):
        context = info.context.get("text_chunks", None)

        resp: Validation = client.chat.completions.create(
            response_model=Validation,
            messages=[
                {
                    "role": "user",
                    "content": f"Does the following answers match the question and the context?\n\nQuestion: {self.question}\n\nAnswer: {self.answer}\n\nContext: {context}",
                }
            ],
            model="gpt-3.5-turbo",
        )

        if resp.is_valid:
            return self

        raise ValueError(resp.error_messages)


""" 
Using LLMs for citation verification is inefficient during runtime. 
However, we can utilize them to create a dataset consisting only of accurate responses 
where citations must be valid (as determined by LLM, fuzzy text search, etc.). 

This approach would require an initial investment during data generation to obtain 
a finely-tuned model for improved citation.
"""
try:
    AnswerWithCitaton.model_validate(
        {
            "question": "What is the capital of France?",
            "answer": [
                {"body": "Texas", "substring_quote": "Paris is the capital of France"},
            ],
        },
        context={
            "text_chunks": {
                1: "Jason is a pirate",
                2: "Paris is the capital of France",
                3: "Irrelevant data",
            }
        },
    )
except ValidationError as e:
    print(e)
""" 
1 validation error for AnswerWithCitaton
  Value error, The answer does not match the question and context [type=value_error, input_value={'question': 'What is the...he capital of France'}]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.4/v/value_error
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/citation_with_extraction/citation_fuzzy_match.py
=======
import instructor

from loguru import logger
from openai import OpenAI
from pydantic import Field, BaseModel, FieldValidationInfo, model_validator

client = instructor.from_openai(OpenAI())


class Fact(BaseModel):
    statement: str = Field(
        ..., description="Body of the sentence, as part of a response"
    )
    substring_phrase: list[str] = Field(
        ...,
        description="String quote long enough to evaluate the truthfulness of the fact",
    )

    @model_validator(mode="after")
    def validate_sources(self, info: FieldValidationInfo) -> "Fact":
        """
        For each substring_phrase, find the span of the substring_phrase in the context.
        If the span is not found, remove the substring_phrase from the list.
        """
        if info.context is None:
            logger.info("No context found, skipping validation")
            return self

        # Get the context from the info
        text_chunks = info.context.get("text_chunk", None)

        # Get the spans of the substring_phrase in the context
        spans = list(self.get_spans(text_chunks))
        logger.info(
            f"Found {len(spans)} span(s) for from {len(self.substring_phrase)} citation(s)."
        )
        # Replace the substring_phrase with the actual substring
        self.substring_phrase = [text_chunks[span[0] : span[1]] for span in spans]
        return self

    def _get_span(self, quote, context, errs=5):
        import regex

        minor = quote
        major = context

        errs_ = 0
        s = regex.search(f"({minor}){{e<={errs_}}}", major)
        while s is None and errs_ <= errs:
            errs_ += 1
            s = regex.search(f"({minor}){{e<={errs_}}}", major)

        if s is not None:
            yield from s.spans()

    def get_spans(self, context):
        for quote in self.substring_phrase:
            yield from self._get_span(quote, context)


class QuestionAnswer(instructor.OpenAISchema):
    """
    Class representing a question and its answer as a list of facts each one should have a soruce.
    each sentence contains a body and a list of sources."""

    question: str = Field(..., description="Question that was asked")
    answer: list[Fact] = Field(
        ...,
        description="Body of the answer, each fact should be its seperate object with a body and a list of sources",
    )

    @model_validator(mode="after")
    def validate_sources(self) -> "QuestionAnswer":
        """
        Checks that each fact has some sources, and removes those that do not.
        """
        logger.info(f"Validating {len(self.answer)} facts")
        self.answer = [fact for fact in self.answer if len(fact.substring_phrase) > 0]
        logger.info(f"Found {len(self.answer)} facts with sources")
        return self


def ask_ai(question: str, context: str) -> QuestionAnswer:
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        temperature=0,
        response_model=QuestionAnswer,
        messages=[
            {
                "role": "system",
                "content": "You are a world class algorithm to answer questions with correct and exact citations.",
            },
            {"role": "user", "content": f"{context}"},
            {"role": "user", "content": f"Question: {question}"},
        ],
        validation_context={"text_chunk": context},
    )


question = "where did he go to school?"
context = """
My name is Jason Liu, and I grew up in Toronto Canada but I was born in China.I went to an arts highschool but in university I studied Computational Mathematics and physics.  As part of coop I worked at many companies including Stitchfix, Facebook. I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.
"""

answer = ask_ai(question, context)
print(answer.model_dump_json(indent=2))
"""
2023-09-09 15:48:11.022 | INFO     | __main__:validate_sources:35 - Found 1 span(s) for from 1 citation(s).
2023-09-09 15:48:11.023 | INFO     | __main__:validate_sources:35 - Found 1 span(s) for from 1 citation(s).
2023-09-09 15:48:11.023 | INFO     | __main__:validate_sources:78 - Validating 2 facts
2023-09-09 15:48:11.023 | INFO     | __main__:validate_sources:80 - Found 2 facts with sources
{
  "question": "where did he go to school?",
  "answer": [
    {
      "statement": "Jason Liu went to an arts highschool.",
      "substring_phrase": [
        "arts highschool"
      ]
    },
    {
      "statement": "Jason Liu studied Computational Mathematics and physics in university.",
      "substring_phrase": [
        "university"
      ]
    }
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/citation_with_extraction/diagram.py
=======
import erdantic as erd

from citation_fuzzy_match import QuestionAnswer

diagram = erd.create(QuestionAnswer)
diagram.draw("examples/citation_fuzzy_match/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/citation_with_extraction/main.py
=======
import json
from collections.abc import Iterable
from fastapi import FastAPI, Request, HTTPException
from fastapi.params import Depends
from instructor import OpenAISchema
from pydantic import BaseModel, Field
from starlette.responses import StreamingResponse

import os
import instructor
import logging

from openai import OpenAI
from instructor.dsl.multitask import MultiTaskBase

client = instructor.from_openai(OpenAI())
logger = logging.getLogger(__name__)

# FastAPI app
app = FastAPI(
    title="Citation with Extraction",
)


class Fact(BaseModel):
    """
    Class representing single statement.
    Each fact has a body and a list of sources.
    If there are multiple facts make sure to break them apart such that each one only uses a set of sources that are relevant to it.
    """

    fact: str = Field(
        ...,
        description="Body of the sentences, as part of a response, it should read like a sentence that answers the question",
    )
    substring_quotes: list[str] = Field(
        ...,
        description="Each source should be a direct quote from the context, as a substring of the original content",
    )

    def _get_span(self, quote, context):
        import regex

        minor = quote
        major = context

        errs_ = 0
        s = regex.search(f"({minor}){{e<={errs_}}}", major)
        while s is None and errs_ <= len(context) * 0.05:
            errs_ += 1
            s = regex.search(f"({minor}){{e<={errs_}}}", major)

        if s is not None:
            yield from s.spans()

    def get_spans(self, context):
        if self.substring_quotes:
            for quote in self.substring_quotes:
                yield from self._get_span(quote, context)


class QuestionAnswer(OpenAISchema, MultiTaskBase):
    """
    Class representing a question and its answer as a list of facts each one should have a source.
    each sentence contains a body and a list of sources."""

    question: str = Field(..., description="Question that was asked")
    tasks: list[Fact] = Field(
        ...,
        description="Body of the answer, each fact should be its separate object with a body and a list of sources",
    )


QuestionAnswer.task_type = Fact


class Question(BaseModel):
    context: str = Field(..., description="Context to extract answers from")
    query: str = Field(..., description="Question to answer")


# Function to extract entities from input text using GPT-3.5
def stream_extract(question: Question) -> Iterable[Fact]:
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        temperature=0,
        stream=True,
        functions=[QuestionAnswer.openai_schema],
        function_call={"name": QuestionAnswer.openai_schema["name"]},
        messages=[
            {
                "role": "system",
                "content": "You are a world class algorithm to answer questions with correct and exact citations. ",
            },
            {"role": "user", "content": "Answer question using the following context"},
            {"role": "user", "content": f"{question.context}"},
            {"role": "user", "content": f"Question: {question.query}"},
            {
                "role": "user",
                "content": "Tips: Make sure to cite your sources, and use the exact words from the context.",
            },
        ],
        max_tokens=2000,
    )
    return QuestionAnswer.from_streaming_response(completion)


def get_api_key(request: Request):
    """
    This just gets the API key from the request headers.
    but tries to read from the environment variable OPENAI_API_KEY first.
    """
    if "OPENAI_API_KEY" in os.environ:
        return os.environ["OPENAI_API_KEY"]

    auth = request.headers.get("Authorization")
    if auth is None:
        raise HTTPException(status_code=401, detail="Missing Authorization header")

    if auth.startswith("Bearer "):
        return auth.replace("Bearer ", "")

    return None


# Route to handle SSE events and return users
@app.post("/extract", response_class=StreamingResponse)
async def extract(question: Question, openai_key: str = Depends(get_api_key)):
    raise Exception(
        "The 'openai.api_key' option isn't read in the client API. You will need to pass it when you instantiate the client, e.g. 'OpenAI(api_key=openai_key)'"
    )
    facts = stream_extract(question)

    async def generate():
        for fact in facts:
            logger.info(f"Fact: {fact}")
            spans = list(fact.get_spans(question.context))
            resp = {
                "body": fact.fact,
                "spans": spans,
                "citation": [question.context[a:b] for (a, b) in spans],
            }
            resp_json = json.dumps(resp)
            yield f"data: {resp_json}"
        yield "data: [DONE]"

    return StreamingResponse(generate(), media_type="text/event-stream")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/citation_with_extraction/modal_main.py
=======
from main import app
import modal

stub = modal.Stub("rag-citation")

image = modal.Image.debian_slim().pip_install("fastapi", "instructor>=0.2.1", "regex")


@stub.function(image=image)
@modal.asgi_app()
def fastapi_app():
    return app


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/classification/classifiy_with_validation.py
=======
# pip install openai instructor
from pydantic import BaseModel, field_validator, Field
import openai
import instructor
from tqdm import tqdm

client = instructor.from_openai(openai.OpenAI())

classes = {
    "11-0000": "Management",
    "13-0000": "Business and Financial Operations",
    "15-0000": "Computer and Mathematical",
    "17-0000": "Architecture and Engineering",
    "19-0000": "Life, Physical, and Social Science",
    "21-0000": "Community and Social Service",
    "23-0000": "Legal",
    "25-0000": "Education Instruction and Library",
    "27-0000": "Arts, Design, Entertainment, Sports and Media",
    "29-0000": "Healthcare Practitioners and Technical",
    "31-0000": "Healthcare Support",
    "33-0000": "Protective Service",
    "35-0000": "Food Preparation and Serving",
    "37-0000": "Building and Grounds Cleaning and Maintenance",
    "39-0000": "Personal Care and Service",
    "41-0000": "Sales and Related",
    "43-0000": "Office and Administrative Support",
    "45-0000": "Farming, Fishing and Forestry",
    "47-0000": "Construction and Extraction",
    "49-0000": "Installation, Maintenance, and Repair",
    "51-0000": "Production Occupations",
    "53-0000": "Transportation and Material Moving",
    "55-0000": "Military Specific",
    "99-0000": "Other",
}


class SOCCode(BaseModel):
    reasoning: str = Field(
        default=None,
        description="Step-by-step reasoning to get the correct classification",
    )
    code: str

    @field_validator("code")
    def validate_code(cls, v):
        if v not in classes:
            raise ValueError(f"Invalid SOC code, {v}")
        return v


def classify_job(description: str) -> SOCCode:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=SOCCode,
        max_retries=3,
        messages=[
            {
                "role": "system",
                "content": f"You are an expert at classifying job descriptions into Standard Occupational Classification (SOC) codes. from the following list: {classes}",
            },
            {
                "role": "user",
                "content": f"Classify this job description into the most appropriate SOC code: {description}",
            },
        ],
    )
    return response


if __name__ == "__main__":
    # gpt-3.5-turbo: 16/20
    # gpt-3.5-turbo (COT): 18/20
    # gpt-4-turbo: 20/20

    job_descriptions = [
        (
            "Develop and design complex software applications for various industries, including finance, healthcare, and e-commerce",
            "15-0000",  # Computer and Mathematical Occupations
        ),
        (
            "Provide comprehensive technical support and troubleshooting for enterprise-level software products, ensuring seamless user experience",
            "15-0000",  # Computer and Mathematical Occupations
        ),
        (
            "Teach a diverse range of subjects to elementary school students, fostering their intellectual and social development",
            "25-0000",  # Education, Training, and Library Occupations
        ),
        (
            "Conduct cutting-edge research in various academic fields at a renowned university, contributing to the advancement of knowledge",
            "25-0000",  # Education, Training, and Library Occupations
        ),
        (
            "Design visually appealing and strategically effective logos, branding, and marketing materials for clients across different industries",
            "27-0000",  # Arts, Design, Entertainment, Sports, and Media Occupations
        ),
        (
            "Perform as part of a professional musical group, entertaining audiences and showcasing artistic talent",
            "27-0000",  # Arts, Design, Entertainment, Sports, and Media Occupations
        ),
        (
            "Diagnose and treat a wide range of injuries and medical conditions, providing comprehensive healthcare services to patients",
            "29-0000",  # Healthcare Practitioners and Technical Occupations
        ),
        (
            "Assist doctors and nurses in delivering high-quality patient care, ensuring the smooth operation of healthcare facilities",
            "31-0000",  # Healthcare Support Occupations
        ),
        (
            "Patrol assigned areas to enforce laws and ordinances, maintaining public safety and order in the community",
            "33-0000",  # Protective Service Occupations
        ),
        (
            "Prepare and serve a diverse menu of delectable meals in a fast-paced restaurant environment",
            "35-0000",  # Food Preparation and Serving Related Occupations
        ),
        (
            "Maintain the cleanliness and upkeep of various buildings and facilities, ensuring a safe and presentable environment",
            "37-0000",  # Building and Grounds Cleaning and Maintenance Occupations
        ),
        (
            "Provide a range of beauty services, such as haircuts, styling, and manicures, to help clients look and feel their best",
            "39-0000",  # Personal Care and Service Occupations
        ),
        (
            "Engage with customers in a retail setting, providing excellent service and assisting them in finding the products they need",
            "41-0000",  # Sales and Related Occupations
        ),
        (
            "Perform a variety of clerical duties in an office environment, supporting the overall operations of the organization",
            "43-0000",  # Office and Administrative Support Occupations
        ),
        (
            "Cultivate and harvest a wide range of crops, contributing to the production of food and other agricultural products",
            "45-0000",  # Farming, Fishing, and Forestry Occupations
        ),
        (
            "Construct and build various structures, including residential, commercial, and infrastructure projects",
            "47-0000",  # Construction and Extraction Occupations
        ),
        (
            "Repair and maintain a diverse range of mechanical equipment, ensuring their proper functioning and longevity",
            "49-0000",  # Installation, Maintenance, and Repair Occupations
        ),
        (
            "Operate specialized machinery and equipment in a manufacturing setting to produce high-quality goods",
            "51-0000",  # Production Occupations
        ),
        (
            "Transport freight and goods across different regions, ensuring timely and efficient delivery",
            "53-0000",  # Transportation and Material Moving Occupations
        ),
        (
            "Serve in the armed forces, protecting the nation and its citizens through various military operations and duties",
            "55-0000",  # Military Specific Occupations
        ),
    ]

    correct = 0
    errors = []
    for description, expected_code in tqdm(job_descriptions):
        try:
            predicted_code = None
            result = classify_job(description)
            predicted_code = result.code
            assert (
                result.code == expected_code
            ), f"Expected {expected_code}, got {result.code} for description: {description}"
            correct += 1
        except Exception as e:
            errors.append(
                f"Got {classes.get(predicted_code, 'Unknown')} expected {classes.get(expected_code, 'Unknown')}"
            )

    print(f"{correct} out of {len(job_descriptions)} tests passed!")
    for error in errors:
        print(error)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/classification/multi_prediction.py
=======
import enum
import instructor

from openai import OpenAI
from pydantic import BaseModel

client = instructor.from_openai(OpenAI())


# Define new Enum class for multiple labels
class MultiLabels(str, enum.Enum):
    BILLING = "billing"
    GENERAL_QUERY = "general_query"
    HARDWARE = "hardware"


# Adjust the prediction model to accommodate a list of labels
class MultiClassPrediction(BaseModel):
    predicted_labels: list[MultiLabels]


# Modify the classify function
def multi_classify(data: str) -> MultiClassPrediction:
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=MultiClassPrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following support ticket: {data}",
            },
        ],
    )  # type: ignore


# Example using a support ticket
ticket = (
    "My account is locked and I can't access my billing info. Phone is also broken."
)
prediction = multi_classify(ticket)
print(prediction)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/classification/simple_prediction.py
=======
import enum
import instructor
from openai import OpenAI

from pydantic import BaseModel

client = instructor.from_openai(OpenAI())


class Labels(str, enum.Enum):
    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Correct class label for the given text
    """

    class_label: Labels


def classify(data: str) -> SinglePrediction:
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {data}",
            },
        ],
    )  # type: ignore


prediction = classify("Hello there I'm a nigerian prince and I want to give you money")
assert prediction.class_label == Labels.SPAM


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/codegen-from-schema/create_fastapi_app.py
=======
import json
import datetime
from pathlib import Path
from jinja2 import Template
import re
from datamodel_code_generator import InputFileType, generate
from pydantic import BaseModel

APP_TEMPLATE_STR = '''# generated by instructor-codegen:
#   timestamp: {{timestamp}}
#   task_name: {{task_name}}
#   api_path: {{api_path}}
#   json_schema_path: {{json_schema_path}}

from fastapi import FastAPI
from pydantic import BaseModel
from jinja2 import Template
from models import {{title}}

import openai
import instructor

instructor.from_openai()

app = FastAPI()

class TemplateVariables(BaseModel):
{% for var in jinja_vars %}
    {{var.strip()}}: str
{% endfor %}

class RequestSchema(BaseModel):
    template_variables: TemplateVariables
    model: str
    temperature: int

PROMPT_TEMPLATE = Template("""{{prompt_template}}""".strip())

@app.post("{{api_path}}", response_model={{title}})
async def {{task_name}}(input: RequestSchema) -> {{title}}:
    rendered_prompt = PROMPT_TEMPLATE.render(**input.template_variables.model_dump())
    return await openai.ChatCompletion.acreate(
        model=input.model,
        temperature=input.temperature,
        response_model={{title}},
        messages=[
            {"role": "user", "content": rendered_prompt}
        ]
    ) # type: ignore
'''


class TemplateVariables(BaseModel):
    biography: str


def load_json_schema(json_schema_path: str) -> dict:
    try:
        with open(json_schema_path) as f:
            return json.load(f)
    except Exception as e:
        raise ValueError(f"Failed to load JSON schema: {e}") from e


def generate_pydantic_model(json_schema_path: str):
    input_path = Path(json_schema_path)
    output_path = Path("./models.py")
    generate(
        input_=input_path, input_file_type=InputFileType.JsonSchema, output=output_path
    )


def extract_jinja_vars(prompt_template: str) -> list:
    return re.findall(r"\{\{(.*?)\}\}", prompt_template)


def render_app_template(template_str: str, **kwargs) -> str:
    app_template = Template(template_str)
    return app_template.render(**kwargs)


def create_app(
    api_path: str, task_name: str, json_schema_path: str, prompt_template: str
) -> str:
    if not api_path.startswith("/"):
        api_path = "/" + api_path

    schema = load_json_schema(json_schema_path)
    title = schema["title"]
    generate_pydantic_model(json_schema_path)

    jinja_vars = extract_jinja_vars(prompt_template)

    return render_app_template(
        APP_TEMPLATE_STR,
        timestamp=datetime.datetime.now().isoformat(),
        task_name=task_name,
        api_path=api_path,
        json_schema_path=json_schema_path,
        title=title,
        jinja_vars=jinja_vars,
        prompt_template=prompt_template,
    )


if __name__ == "__main__":
    try:
        fastapi_code = create_app(
            api_path="/api/v1/extract_person",
            task_name="extract_person",
            json_schema_path="./input.json",
            prompt_template="Extract the person from the following: {{biography}}",
        )

        with open("./run.py", "w") as f:
            f.write(fastapi_code)

        print("FastAPI application generated and saved to './run.py'")

    except Exception as e:
        print(f"An error occurred: {e}")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/codegen-from-schema/models.py
=======
# generated by datamodel-codegen:
#   filename:  input.json
#   timestamp: 2023-09-10T00:33:42+00:00

from __future__ import annotations

from enum import Enum

from pydantic import BaseModel


class Type(Enum):
    home = "home"
    work = "work"
    mobile = "mobile"


class PhoneNumber(BaseModel):
    type: Type
    number: str


class ExtractPerson(BaseModel):
    name: str
    age: int
    phoneNumbers: list[PhoneNumber]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/codegen-from-schema/run.py
=======
# This file was generated by instructor
#   timestamp: 2023-09-09T20:33:42.572627
#   task_name: extract_person
#   api_path: /api/v1/extract_person
#   json_schema_path: ./input.json

import instructor

from fastapi import FastAPI
from pydantic import BaseModel
from jinja2 import Template
from models import ExtractPerson
from openai import AsyncOpenAI

aclient = instructor.apatch(AsyncOpenAI())

app = FastAPI()


class TemplateVariables(BaseModel):
    biography: str


class RequestSchema(BaseModel):
    template_variables: TemplateVariables
    model: str
    temperature: int


PROMPT_TEMPLATE = Template(
    """Extract the person from the following: {{biography}}""".strip()
)


@app.post("/api/v1/extract_person", response_model=ExtractPerson)
async def extract_person(input: RequestSchema) -> ExtractPerson:
    rendered_prompt = PROMPT_TEMPLATE.render(**input.template_variables.model_dump())
    return await aclient.chat.completions.create(
        model=input.model,
        temperature=input.temperature,
        response_model=ExtractPerson,
        messages=[{"role": "user", "content": rendered_prompt}],
    )  # type: ignore


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/cohere/cohere.py
=======
import cohere
import instructor
from pydantic import BaseModel, Field


# Patching the Cohere client with the instructor for enhanced capabilities
client = instructor.from_cohere(
    cohere.Client(),
    max_tokens=1000,
    model="command-r-plus",
)


class Person(BaseModel):
    name: str = Field(description="name of the person")
    country_of_origin: str = Field(description="country of origin of the person")


class Group(BaseModel):
    group_name: str = Field(description="name of the group")
    members: list[Person] = Field(description="list of members in the group")


task = """\
Given the following text, create a Group object for 'The Beatles' band

Text:
The Beatles were an English rock band formed in Liverpool in 1960. With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time. The group were integral to the development of 1960s counterculture and popular music's recognition as an art form.
"""
group = client.messages.create(
    response_model=Group,
    messages=[{"role": "user", "content": task}],
    temperature=0,
)

print(group.model_dump_json(indent=2))
"""
{
  "group_name": "The Beatles",
  "members": [
    {
      "name": "John Lennon",
      "country_of_origin": "England"
    },
    {
      "name": "Paul McCartney",
      "country_of_origin": "England"
    },
    {
      "name": "George Harrison",
      "country_of_origin": "England"
    },
    {
      "name": "Ringo Starr",
      "country_of_origin": "England"
    }
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/crm/run.py
=======
from enum import Enum
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI

client = instructor.from_openai(OpenAI())


class CRMSource(Enum):
    personal = "personal"
    business = "business"
    work_contacts = "work_contacts"
    all = "all"


class CRMSearch(BaseModel):
    """A CRM search query

    The search description is a natural language description of the search query
    the backend will use semantic search so use a range of phrases to describe the search
    """

    source: CRMSource
    city_location: str = Field(
        ..., description="City location used to match the desired customer profile"
    )
    search_description: str = Field(
        ..., description="Search query used to match the desired customer profile"
    )


class CRMSearchQuery(BaseModel):
    """
    A set of CRM queries to be executed against a CRM system,
    for large locations decompose into multiple queries of smaller locations
    """

    queries: list[CRMSearch]


def query_crm(query: str) -> CRMSearchQuery:
    queries = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=CRMSearchQuery,
        messages=[
            {
                "role": "system",
                "content": """
            You are a world class CRM search career generator. 
            You will take the user query and decompose it into a set of CRM queries queries.
            """,
            },
            {"role": "user", "content": query},
        ],
    )
    return queries


if __name__ == "__main__":
    query = "find me all the pottery businesses in San Francisco and my friends in the east coast big cities"
    print(query_crm(query).model_dump_json(indent=2))
    """
    {
    "queries": [
        {
            "source": "business",
            "city_location": "San Francisco",
            "search_description": "pottery businesses"
        },
        {
            "source": "personal",
            "city_location": "New York",
            "search_description": "friends in New York"
        },
        {
            "source": "personal",
            "city_location": "Boston",
            "search_description": "friends in Boston"
        },
        {
            "source": "personal",
            "city_location": "Philadelphia",
            "search_description": "friends in Philadelphia"
        }
    ]
    }
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/distilations/three_digit_mul.py
=======
import logging

from pydantic import BaseModel, Field
from instructor import Instructions

logging.basicConfig(level=logging.INFO)

# Usage
instructions = Instructions(
    name="three_digit_multiply",
    finetune_format="messages",
    log_handlers=[
        logging.FileHandler("math_finetunes.jsonl"),
    ],
)


class Multiply(BaseModel):
    a: int
    b: int
    result: int = Field(..., description="The result of the multiplication")


@instructions.distil
def fn(a: int, b: int) -> Multiply:
    """Return the result of multiplying a and b together"""
    resp = a * b
    return Multiply(a=a, b=b, result=resp)


if __name__ == "__main__":
    import random

    log_lines = {
        "messages": [
            {
                "role": "system",
                "content": 'Predict the results of this function:\n\ndef fn(a: int, b: int) -> __main__.Multiply\n"""\nReturn the result of multiplying a and b together\n"""',
            },
            {"role": "user", "content": "Return `fn(169, b=166)`"},
            {
                "role": "assistant",
                "function_call": {
                    "name": "Multiply",
                    "arguments": '{\n  "a": 169,\n  "b": 166,\n  "result": 28054\n}',
                },
            },
        ],
        "functions": [
            {
                "name": "Multiply",
                "description": "Correctly extracted `Multiply` with all the required parameters with correct types",
                "parameters": {
                    "properties": {
                        "a": {"title": "A", "type": "integer"},
                        "b": {"title": "B", "type": "integer"},
                        "result": {
                            "description": "The result of the multiplication",
                            "title": "Result",
                            "type": "integer",
                        },
                    },
                    "required": ["a", "b", "result"],
                    "type": "object",
                },
            }
        ],
    }
    for _ in range(10):
        a = random.randint(100, 999)
        b = random.randint(100, 999)
        print("returning", fn(a, b=b))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/distilations/three_digit_mul_dispatch.py
=======
import logging

from pydantic import BaseModel, Field
from instructor import Instructions
import instructor

instructor.from_openai()

logging.basicConfig(level=logging.INFO)

# Usage
instructions = Instructions(
    name="three_digit_multiply",
    finetune_format="messages",
    include_code_body=True,
    log_handlers=[
        logging.FileHandler("math_finetunes.jsonl"),
    ],
)


class Multiply(BaseModel):
    a: int
    b: int
    result: int = Field(..., description="The result of the multiplication")


@instructions.distil(mode="dispatch", model="ft:gpt-3.5-turbo-0613:personal::8CazU0uq")
def fn(a: int, b: int) -> Multiply:
    """Return the result of the multiplication as an integer"""
    resp = a * b
    return Multiply(a=a, b=b, result=resp)


if __name__ == "__main__":
    import random

    for _ in range(5):
        a = random.randint(100, 999)
        b = random.randint(100, 999)
        result = fn(a, b)
        print(f"{a} * {b} = {result.result}, expected {a*b}")
    """
    972 * 508 = 493056, expected 493776
    145 * 369 = 53505, expected 53505
    940 * 440 = 413600, expected 413600
    114 * 213 = 24282, expected 24282
    259 * 650 = 168350, expected 168350
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/evals/eval.py
=======
from collections import Counter, defaultdict
from enum import Enum
from typing import Any, Union
import numpy as np
import json
from pydantic import ValidationError
from pprint import pprint
import models as m


class Status(Enum):
    IS_JSON = "_is_json_"
    IS_VALID = "_is_valid_"
    VALIDATION_ERROR = "_validation_error_"


class StreamingAccumulatorManager:
    def __init__(self):
        self.accumulator = defaultdict(StreamingAccumulator)

    def validate_string(self, json_string: str, index: int) -> None:
        try:
            obj = json.loads(json_string)
            self.accumulator[Status.IS_JSON.value].update(index, True)
            try:
                # Replace this line with your validation logic
                obj = m.MultiSearch.model_validate(obj)
                self.update(index, obj.model_dump())
                self.accumulator[Status.IS_VALID.value].update(index, True)
            except ValidationError as e:
                self.accumulator[Status.IS_VALID.value].update(index, False)
                self.process_validation_error(e, index)
        except json.JSONDecodeError:
            self.accumulator[Status.IS_JSON.value].update(index, False)

    def process_validation_error(self, error, index):
        for err in error.errors():
            path = (
                "$."
                + ".".join(
                    [str(x) if not isinstance(x, int) else "[*]" for x in err["loc"]]
                )
                + "."
                + err["type"]
            )
            self.accumulator[Status.VALIDATION_ERROR.value].update(index, path)

    def update(self, index, data: Any, path: str = "$") -> None:
        if isinstance(data, dict):
            for key, value in data.items():
                new_path = f"{path}.{key}"
                self.update(index, value, new_path)
        elif isinstance(data, list):
            new_path = f"{path}[*]"
            for value in data:
                self.update(index, value, new_path)
            length_path = f"{path}.length"
            self.accumulator[length_path].update(index, len(data))
        elif isinstance(data, Enum):
            enum_path = f"{path}.enum"
            self.accumulator[enum_path].update(index, data.value)
        else:
            self.accumulator[path].update(index, data)

    def summarize(self) -> dict[str, dict]:
        return {k: v.summarize(key_name=k) for k, v in self.accumulator.items()}


class StreamingAccumulator:
    def __init__(self):
        self.counter = Counter()
        self.min = float("inf")
        self.max = float("-inf")
        self.sum = 0
        self.squared_sum = 0
        self.unique_values = set()
        self.missing_values = 0
        self.str_min_length = float("inf")
        self.str_max_length = float("-inf")
        self.str_sum_length = 0
        self.str_squared_sum_length = 0
        self.value = []
        self.str_length = []
        self.reverse_lookup = defaultdict(list)

    def update(self, index: Any, value: Any) -> None:
        if isinstance(value, (int, str, bool)):
            self.counter[value] += 1
            self.unique_values.add(value)
            self.value.append(value)
            self.reverse_lookup[value].append(index)
        if value is None or value == "":
            self.missing_values += 1
            return
        if isinstance(value, (int, float)):
            self.min = min(self.min, value)
            self.max = max(self.max, value)
            self.sum += value
            self.squared_sum += value**2
        if isinstance(value, str):
            str_len = len(value)
            self.str_length.append(str_len)
            self.str_min_length = min(self.str_min_length, str_len)
            self.str_max_length = max(self.str_max_length, str_len)
            self.str_sum_length += str_len
            self.str_squared_sum_length += str_len**2

    def summarize(self, key_name=None) -> dict[str, Union[int, float, dict]]:
        if key_name is None:
            key_name = ""
        n = sum(self.counter.values())
        summaries = {}
        summaries["counter"] = self.counter
        summaries["unique_count"] = len(self.unique_values)
        summaries["missing_values"] = self.missing_values
        summaries["_reverse_lookup"] = dict(self.reverse_lookup)
        if n > 0:
            if all(isinstance(value, (bool)) for value in self.unique_values):
                summaries["mean"] = self.sum / n
                return summaries
            if all(isinstance(value, (int, float)) for value in self.unique_values):
                summaries["min"] = self.min
                summaries["max"] = self.max
                summaries["mean"] = self.sum / n
                summaries["std"] = np.sqrt(self.squared_sum / n - (self.sum / n) ** 2)
                return summaries
            if all(isinstance(value, str) for value in self.unique_values):
                summaries["str_min_length"] = self.str_min_length
                summaries["str_max_length"] = self.str_max_length
                summaries["str_mean_length"] = self.str_sum_length / n
                summaries["str_std_length"] = np.sqrt(
                    self.str_squared_sum_length / n - (self.str_sum_length / n) ** 2
                )
                return summaries
        return summaries


if __name__ == "__main__":
    eval_manager = StreamingAccumulatorManager()

    with open("test.jsonl") as f:
        lines = f.readlines()
        for ii, line in enumerate(lines):
            eval_manager.validate_string(line, ii)

    pprint(eval_manager.summarize())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/evals/models.py
=======
from typing import Optional
from pydantic import BaseModel, Field
from enum import Enum


class SourceType(str, Enum):
    CRM = "CRM"
    WEB = "WEB"
    EMAIL = "EMAIL"
    SOCIAL_MEDIA = "SOCIAL_MEDIA"
    OTHER = "OTHER"


class Search(BaseModel):
    query: str
    source_type: SourceType
    results_limit: Optional[int] = Field(10)
    is_priority: Optional[bool] = None
    tags: Optional[list[str]] = None


class MultiSearch(BaseModel):
    queries: list[Search]
    user_id: Optional[str]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/evals/stats_dict.py
=======
from collections import Counter

stats_dict = {
    "$.queries.length": {
        "_reverse_lookup": {
            1: [0, 1, 8, 9, 10, 13, 14, 15],
            2: [7, 11, 16],
            3: [12, 17],
        },
        "counter": Counter({1: 8, 2: 3, 3: 2}),
        "max": 3,
        "mean": 1.5384615384615385,
        "min": 1,
        "missing_values": 0,
        "std": 0.7457969011409735,
        "unique_count": 3,
    },
    "$.queries[*].is_priority": {
        "_reverse_lookup": {False: [13], True: [1, 9, 14, 17]},
        "counter": Counter({True: 4, False: 1}),
        "mean": 0.8,
        "missing_values": 15,
        "unique_count": 2,
    },
    "$.queries[*].query": {
        "_reverse_lookup": {
            "customer churn": [1],
            "customer feedback": [15],
            "customer satisfaction": [11],
            "email campaigns": [12],
            "email open rates": [17],
            "email outreach": [10],
            "marketing strategies": [14],
            "new products": [16],
            "product sales": [11],
            "revenue 2022": [9],
            "revenue streams": [16],
            "sales Q1": [0, 7, 8, 13],
            "sales Q2": [7],
            "social impact": [12],
            "social trends": [17],
            "web traffic": [12],
            "website analytics": [17],
        },
        "counter": Counter(
            {
                "sales Q1": 4,
                "customer churn": 1,
                "sales Q2": 1,
                "revenue 2022": 1,
                "email outreach": 1,
                "product sales": 1,
                "customer satisfaction": 1,
                "social impact": 1,
                "email campaigns": 1,
                "web traffic": 1,
                "marketing strategies": 1,
                "customer feedback": 1,
                "revenue streams": 1,
                "new products": 1,
                "social trends": 1,
                "email open rates": 1,
                "website analytics": 1,
            }
        ),
        "missing_values": 0,
        "str_max_length": 21,
        "str_mean_length": 13.15,
        "str_min_length": 8,
        "str_std_length": 3.8376425054973518,
        "unique_count": 17,
    },
    "$.queries[*].results_limit": {
        "_reverse_lookup": {
            5: [17],
            10: [0, 1, 7, 7, 8, 9, 10, 11, 11, 12, 12, 12, 13, 15, 16, 16, 17, 17],
            15: [14],
        },
        "counter": Counter({10: 18, 15: 1, 5: 1}),
        "max": 15,
        "mean": 10.0,
        "min": 5,
        "missing_values": 0,
        "std": 1.5811388300841898,
        "unique_count": 3,
    },
    "$.queries[*].source_type.enum": {
        "_reverse_lookup": {
            "CRM": [0, 7, 8, 11, 13, 16],
            "EMAIL": [10, 11, 12, 15, 17],
            "SOCIAL_MEDIA": [12, 17],
            "WEB": [1, 7, 9, 12, 14, 16, 17],
        },
        "counter": Counter({"WEB": 7, "CRM": 6, "EMAIL": 5, "SOCIAL_MEDIA": 2}),
        "missing_values": 0,
        "str_max_length": 12,
        "str_mean_length": 4.4,
        "str_min_length": 3,
        "str_std_length": 2.672077843177477,
        "unique_count": 4,
    },
    "$.queries[*].tags": {
        "_reverse_lookup": {},
        "counter": Counter(),
        "missing_values": 16,
        "unique_count": 0,
    },
    "$.queries[*].tags.length": {
        "_reverse_lookup": {1: [15, 17], 2: [10, 14]},
        "counter": Counter({2: 2, 1: 2}),
        "max": 2,
        "mean": 1.5,
        "min": 1,
        "missing_values": 0,
        "std": 0.5,
        "unique_count": 2,
    },
    "$.queries[*].tags[*]": {
        "_reverse_lookup": {
            "2022": [10],
            "2023": [14],
            "analytics": [17],
            "feedback": [15],
            "outreach": [10],
            "strategy": [14],
        },
        "counter": Counter(
            {
                "outreach": 1,
                "2022": 1,
                "strategy": 1,
                "2023": 1,
                "feedback": 1,
                "analytics": 1,
            }
        ),
        "missing_values": 0,
        "str_max_length": 9,
        "str_mean_length": 6.833333333333333,
        "str_min_length": 4,
        "str_std_length": 2.034425935955618,
        "unique_count": 6,
    },
    "$.user_id": {
        "_reverse_lookup": {
            "user_1": [0],
            "user_10": [10],
            "user_11": [11],
            "user_12": [12],
            "user_13": [13],
            "user_14": [14],
            "user_15": [15],
            "user_16": [16],
            "user_17": [17],
            "user_2": [1],
            "user_7": [7],
            "user_8": [8],
            "user_9": [9],
        },
        "counter": Counter(
            {
                "user_1": 1,
                "user_2": 1,
                "user_7": 1,
                "user_8": 1,
                "user_9": 1,
                "user_10": 1,
                "user_11": 1,
                "user_12": 1,
                "user_13": 1,
                "user_14": 1,
                "user_15": 1,
                "user_16": 1,
                "user_17": 1,
            }
        ),
        "missing_values": 0,
        "str_max_length": 7,
        "str_mean_length": 6.615384615384615,
        "str_min_length": 6,
        "str_std_length": 0.48650425541052295,
        "unique_count": 13,
    },
    "_is_json_": {
        "_reverse_lookup": {
            False: [2, 4],
            True: [0, 1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17],
        },
        "counter": Counter({True: 16, False: 2}),
        "mean": 0.8888888888888888,
        "missing_values": 0,
        "unique_count": 2,
    },
    "_is_valid_": {
        "_reverse_lookup": {
            False: [3, 5, 6],
            True: [0, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17],
        },
        "counter": Counter({True: 13, False: 3}),
        "mean": 0.8125,
        "missing_values": 0,
        "unique_count": 2,
    },
    "_validation_error_": {
        "_reverse_lookup": {
            "$.queries.[*].is_priority.bool_parsing": [6],
            "$.queries.[*].source_type.enum": [3],
            "$.user_id.missing": [5],
        },
        "counter": Counter(
            {
                "$.queries.[*].source_type.enum": 1,
                "$.user_id.missing": 1,
                "$.queries.[*].is_priority.bool_parsing": 1,
            }
        ),
        "missing_values": 0,
        "str_max_length": 38,
        "str_mean_length": 28.333333333333332,
        "str_min_length": 17,
        "str_std_length": 8.653836657164781,
        "unique_count": 3,
    },
}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/evals/streamlit.py
=======
import streamlit as st
from stats_dict import stats_dict

# Sample data
query_data = {i: line.strip() for i, line in enumerate(open("test.jsonl"))}

# Initialize selected keys
selected_keys = {}


# Function to get lines
def get_lines(stats_key, keys):
    indices = []
    for key in keys:
        indices.extend(stats_dict[stats_key]["_reverse_lookup"][key])
    return "\n".join([query_data[i] for i in indices])


# Function to render dropdown and button
def render_dropdown_and_button(stats_key):
    st.subheader(f"Stats for `{stats_key}`")
    st.json(stats_dict[stats_key]["counter"])
    st.json(
        {k: v for k, v in stats_dict[stats_key].items() if isinstance(v, (int, float))}
    )
    st.subheader("Histogram")
    st.bar_chart(stats_dict[stats_key]["counter"], use_container_width=True)

    options = list(stats_dict[stats_key]["counter"].keys())
    selected_keys[stats_key] = st.multiselect(
        f"View samples with {stats_key}",
        options,
        default=selected_keys.get(stats_key, []),
    )
    st.code(get_lines(stats_key, selected_keys[stats_key]))


# Sidebar for navigation
st.sidebar.title("Navigation")
page = st.sidebar.selectbox(
    "Select a page:",
    ["Validation Stats", "Individual Path Views"],
)

# Main Streamlit App
st.title("Structured Output Evaluation")

# Validation Stats
if page == "Validation Stats":
    st.header("Validation Stats")
    for key in [k for k in stats_dict.keys() if k.startswith("_")]:
        render_dropdown_and_button(key)

# Individual Path Views
elif page == "Individual Path Views":
    st.header("Individual Path Views")
    path = st.selectbox(
        "Choose a path:",
        [key for key in stats_dict.keys() if not key.startswith("_")],
    )
    if "counter" in stats_dict[path]:
        render_dropdown_and_button(path)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/run_vision.py
=======
from openai import OpenAI
from io import StringIO
from typing import Annotated, Any
from pydantic import (
    BaseModel,
    BeforeValidator,
    PlainSerializer,
    InstanceOf,
    WithJsonSchema,
)
import instructor
import pandas as pd
from rich.console import Console

console = Console()
client = instructor.from_openai(
    client=OpenAI(),
    mode=instructor.Mode.TOOLS,
)


def md_to_df(data: Any) -> Any:
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),  # Get rid of whitespaces
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .map(lambda x: x.strip())
        )  # type: ignore
    return data


MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    PlainSerializer(lambda x: x.to_markdown()),
    WithJsonSchema(
        {
            "type": "string",
            "description": """
                The markdown representation of the table, 
                each one should be tidy, do not try to join tables
                that should be seperate""",
        }
    ),
]


class Table(BaseModel):
    caption: str
    dataframe: MarkdownDataFrame


class MultipleTables(BaseModel):
    tables: list[Table]


example = MultipleTables(
    tables=[
        Table(
            caption="This is a caption",
            dataframe=pd.DataFrame(
                {
                    "Chart A": [10, 40],
                    "Chart B": [20, 50],
                    "Chart C": [30, 60],
                }
            ),
        )
    ]
)


def extract(url: str) -> MultipleTables:
    return client.chat.completions.create(
        model="gpt-4-turbo",
        max_tokens=4000,
        response_model=MultipleTables,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                    {
                        "type": "text",
                        "text": """
                            First, analyze the image to determine the most appropriate headers for the tables.
                            Generate a descriptive h1 for the overall image, followed by a brief summary of the data it contains. 
                            For each identified table, create an informative h2 title and a concise description of its contents.
                            Finally, output the markdown representation of each table.


                            Make sure to escape the markdown table properly, and make sure to include the caption and the dataframe.
                            including escaping all the newlines and quotes. Only return a markdown table in dataframe, nothing else.
                        """,
                    },
                ],
            }
        ],
    )


urls = [
    "https://a.storyblok.com/f/47007/2400x1260/f816b031cb/uk-ireland-in-three-charts_chart_a.png/m/2880x0",
    "https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png/m/2880x0",
]

for url in urls:
    for table in extract(url).tables:
        console.print(table.caption, "\n", table.dataframe)
"""
Growth in app installations and sessions across different app categories in Q3 2022 compared to Q2 2022 for Ireland and U.K. 
              Install Growth (%)  Session Growth (%) 
 Category                                           
Education                      7                   6
Games                         13                   3
Social                         4                  -3
Utilities                      6                -0.4
Top 10 Grossing Android Apps in Ireland, October 2023 
                              App Name           Category 
 Rank                                                    
1                           Google One       Productivity
2                              Disney+      Entertainment
3        TikTok - Videos, Music & LIVE      Entertainment
4                     Candy Crush Saga              Games
5       Tinder: Dating, Chat & Friends  Social networking
6                          Coin Master              Games
7                               Roblox              Games
8       Bumble - Dating & Make Friends             Dating
9                          Royal Match              Games
10         Spotify: Music and Podcasts      Music & Audio
Top 10 Grossing iOS Apps in Ireland, October 2023 
                              App Name           Category 
 Rank                                                    
1       Tinder: Dating, Chat & Friends  Social networking
2                              Disney+      Entertainment
3       YouTube: Watch, Listen, Stream      Entertainment
4         Audible: Audio Entertainment      Entertainment
5                     Candy Crush Saga              Games
6        TikTok - Videos, Music & LIVE      Entertainment
7       Bumble - Dating & Make Friends             Dating
8                               Roblox              Games
9          LinkedIn: Job Search & News           Business
10         Duolingo - Language Lessons          Education
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/run_vision_langsmith.py
=======
from openai import OpenAI
from io import StringIO
from typing import Annotated, Any
from pydantic import (
    BaseModel,
    BeforeValidator,
    PlainSerializer,
    InstanceOf,
    WithJsonSchema,
)
import instructor
import pandas as pd
from langsmith.wrappers import wrap_openai
from langsmith import traceable


client = wrap_openai(OpenAI())
client = instructor.from_openai(client, mode=instructor.function_calls.Mode.MD_JSON)


def md_to_df(data: Any) -> Any:
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),  # Get rid of whitespaces
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .map(lambda x: x.strip())
        )
    return data


MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    PlainSerializer(lambda x: x.to_markdown()),
    WithJsonSchema(
        {
            "type": "string",
            "description": """
                The markdown representation of the table, 
                each one should be tidy, do not try to join tables
                that should be seperate""",
        }
    ),
]


class Table(BaseModel):
    caption: str
    dataframe: MarkdownDataFrame


class MultipleTables(BaseModel):
    tables: list[Table]


example = MultipleTables(
    tables=[
        Table(
            caption="This is a caption",
            dataframe=pd.DataFrame(
                {
                    "Chart A": [10, 40],
                    "Chart B": [20, 50],
                    "Chart C": [30, 60],
                }
            ),
        )
    ]
)


@traceable(name="extract-table")
def extract(url: str) -> MultipleTables:
    tables = client.chat.completions.create(
        model="gpt-4-vision-preview",
        max_tokens=4000,
        response_model=MultipleTables,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"Describe this data accurately as a table in markdown format. {example.model_dump_json(indent=2)}",
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                    {
                        "type": "text",
                        "text": """
                            First take a moment to reason about the best set of headers for the tables. 
                            Write a good h1 for the image above. Then follow up with a short description of the what the data is about.
                            Then for each table you identified, write a h2 tag that is a descriptive title of the table. 
                            Then follow up with a short description of the what the data is about. 
                            Lastly, produce the markdown table for each table you identified.


                            Make sure to escape the markdown table properly, and make sure to include the caption and the dataframe.
                            including escaping all the newlines and quotes. Only return a markdown table in dataframe, nothing else.
                        """,
                    },
                ],
            }
        ],
    )
    return tables.model_dump()


urls = [
    "https://a.storyblok.com/f/47007/2400x1260/f816b031cb/uk-ireland-in-three-charts_chart_a.png/m/2880x0",
    "https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png/m/2880x0",
]


for url in urls:
    tables = extract(url)
    print(tables)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/run_vision_org.py
=======
from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console

import instructor

console = Console()
client = instructor.from_openai(
    client=OpenAI(),
    mode=instructor.Mode.TOOLS,
)


class People(BaseModel):
    id: str
    name: str
    role: str
    reports: list[str] = Field(
        default_factory=list, description="People who report to this person"
    )
    manages: list[str] = Field(
        default_factory=list, description="People who this person manages"
    )


class Organization(BaseModel):
    people: list[People]


def extract(url: str):
    return client.chat.completions.create_partial(
        model="gpt-4-turbo",
        max_tokens=4000,
        response_model=Organization,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                    {
                        "type": "text",
                        "text": """
                            Analyze the organizational chart image and extract the relevant information to reconstruct the hierarchy.
                            
                            Create a list of People objects, where each person has the following attributes:
                            - id: A unique identifier for the person
                            - name: The person's name
                            - role: The person's role or position in the organization
                            - reports: A list of IDs of people who report directly to this person
                            - manages: A list of IDs of people who this person manages
                            
                            Ensure that the relationships between people are accurately captured in the reports and manages attributes.
                            
                            Return the list of People objects as the people attribute of an Organization object.
                        """,
                    },
                ],
            }
        ],
    )


console.print(
    extract(
        "https://www.mindmanager.com/static/mm/images/features/org-chart/hierarchical-chart.png"
    )
)
"""
Organization(
    people=[
        People(id='A1', name='Adele Morana', role='Founder, Chairman & CEO', reports=[], manages=['B1', 'C1', 'D1']),
        People(id='B1', name='Winston Cole', role='COO', reports=['A1'], manages=['E1']),
        People(id='C1', name='Marcus Kim', role='CFO', reports=['A1'], manages=['F1']),
        People(id='D1', name='Karin Ludovicicus', role='CPO', reports=['A1'], manages=['G1']),
        People(id='E1', name='Lea Erastos', role='Chief Business Officer', reports=['B1'], manages=['H1', 'I1']),
        People(id='F1', name='John McKinley', role='Chief Accounting Officer', reports=['C1'], manages=[]),
        People(id='G1', name='Ayda Williams', role='VP, Global Customer & Business Marketing', reports=['D1'], manages=['J1', 'K1']),
        People(id='H1', name='Zahida Mahtab', role='VP, Global Affairs & Communication', reports=['E1'], manages=[]),
        People(id='I1', name='Adelaide Zhu', role='VP, Central Services', reports=['E1'], manages=[]),
        People(id='J1', name='Gabriel Drummond', role='VP, Investor Relations', reports=['G1'], manages=[]),
        People(id='K1', name='Nicholas Brambilla', role='VP, Company Brand', reports=['G1'], manages=[]),
        People(id='L1', name='Felice Vasili', role='VP Finance', reports=['C1'], manages=[]),
        People(id='M1', name='Sandra Herminius', role='VP, Product Marketing', reports=['D1'], manages=[])
    ]
)
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/run_vision_org_table.py
=======
from openai import OpenAI
from io import StringIO
from typing import Annotated, Any
from pydantic import (
    BaseModel,
    BeforeValidator,
    PlainSerializer,
    InstanceOf,
    WithJsonSchema,
)
import instructor
import pandas as pd
from rich.console import Console

console = Console()
client = instructor.from_openai(
    client=OpenAI(),
    mode=instructor.Mode.TOOLS,
)


def md_to_df(data: Any) -> Any:
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),  # Get rid of whitespaces
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .map(lambda x: x.strip())
        )  # type: ignore
    return data


MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    PlainSerializer(lambda x: x.to_markdown()),
    WithJsonSchema(
        {
            "type": "string",
            "description": """
                The markdown representation of the table, 
                each one should be tidy, do not try to join tables
                that should be seperate""",
        }
    ),
]


class Table(BaseModel):
    caption: str
    dataframe: MarkdownDataFrame


def extract(url: str):
    return client.chat.completions.create(
        model="gpt-4-turbo",
        max_tokens=4000,
        response_model=Table,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                    {
                        "type": "text",
                        "text": """
                            Analyze the organizational chart image and extract the relevant information to reconstruct the hierarchy.
                            
                            Create a list of People objects, where each person has the following attributes:
                            - id: A unique identifier for the person
                            - name: The person's name
                            - role: The person's role or position in the organization
                            - manager_name: The name of the person who manages this person
                            - manager_role: The role of the person who manages this person
                            
                            Ensure that the relationships between people are accurately captured in the reports and manages attributes.
                            
                            Return the list of People objects as the people attribute of an Organization object.
                        """,
                    },
                ],
            }
        ],
    )


print(
    extract(
        "https://www.mindmanager.com/static/mm/images/features/org-chart/hierarchical-chart.png"
    ).model_dump()["dataframe"]
)
"""
|    id  |  name              |  role                                    |  manager_name     |  manager_role                |
|-------:|:-------------------|:-----------------------------------------|:------------------|:-----------------------------|
|    1   | Adele Morana       | Founder, Chairman & CEO                  |                   |                              |
|    2   | Winston Cole       | COO                                      | Adele Morana      | Founder, Chairman & CEO      |
|    3   | Marcus Kim         | CFO                                      | Adele Morana      | Founder, Chairman & CEO      |
|    4   | Karin Ludovicus    | CPO                                      | Adele Morana      | Founder, Chairman & CEO      |
|    5   | Lea Erastos        | Chief Business Officer                   | Winston Cole      | COO                          |
|    6   | John McKinley      | Chief Accounting Officer                 | Winston Cole      | COO                          |
|    7   | Zahida Mahtab      | VP, Global Affairs & Communication       | Winston Cole      | COO                          |
|    8   | Adelaide Zhu       | VP, Central Services                     | Winston Cole      | COO                          |
|    9   | Gabriel Drummond   | VP, Investor Relations                   | Marcus Kim        | CFO                          |
|    10  | Felicie Vasili     | VP, Finance                              | Marcus Kim        | CFO                          |
|    11  | Ayda Williams      | VP, Global Customer & Business Marketing | Karin Ludovicius  | CPO                          |
|    12  | Nicholas Brambilla | VP, Company Brand                        | Karin Ludovicius  | CPO                          |
|    13  | Sandra Herminius   | VP, Product Marketing                    | Karin Ludovicius  | CPO                          |
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/run_vision_receipt.py
=======
from pydantic import BaseModel, model_validator
from openai import OpenAI
import instructor


client = instructor.from_openai(
    client=OpenAI(),
    mode=instructor.Mode.TOOLS,
)


class Item(BaseModel):
    name: str
    price: float
    quantity: int


class Receipt(BaseModel):
    items: list[Item]
    total: float

    @model_validator(mode="after")
    def check_total(cls, values: "Receipt"):
        items = values.items
        total = values.total
        calculated_total = sum(item.price * item.quantity for item in items)
        if calculated_total != total:
            raise ValueError(
                f"Total {total} does not match the sum of item prices {calculated_total}"
            )
        return values


def extract(url: str) -> Receipt:
    return client.chat.completions.create(
        model="gpt-4o",
        max_tokens=4000,
        response_model=Receipt,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                    {
                        "type": "text",
                        "text": "Analyze the image and return the items in the receipt and the total amount.",
                    },
                ],
            }
        ],
    )


# URLs of images containing receipts. Exhibits the use of the model validator to check the total amount.
urls = [
    "https://templates.mediamodifier.com/645124ff36ed2f5227cbf871/supermarket-receipt-template.jpg",
    "https://ocr.space/Content/Images/receipt-ocr-original.jpg",
]

for url in urls:
    receipt = extract(url)
    print(receipt)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extract-table/test.py
=======
from pydantic import BaseModel

from openai import OpenAI
import instructor

client = OpenAI()

client = instructor.from_openai(client)


class User(BaseModel):
    name: str
    email: str


class MeetingInfo(BaseModel):
    user: User
    date: str
    location: str
    budget: int
    deadline: str


data = """
Jason Liu jason@gmail.com
Meeting Date: 2024-01-01
Meeting Location: 1234 Main St
Meeting Budget: $1000
Meeting Deadline: 2024-01-31
"""
stream1 = client.chat.completions.create_partial(
    model="gpt-4",
    response_model=MeetingInfo,
    messages=[
        {
            "role": "user",
            "content": f"Get the information about the meeting and the users {data}",
        },
    ],
    stream=True,
)  # type: ignore

for message in stream1:
    print(message)
"""
ser={} date=None location=None budget=None deadline=None
user={} date=None location=None budget=None deadline=None
user={} date=None location=None budget=None deadline=None
user={} date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name=None, email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email=None) date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date=None location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location=None budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=None deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=100 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline=None
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline='2024-01-31'
user=PartialUser(name='Jason Liu', email='jason@gmail.com') date='2024-01-01' location='1234 Main St' budget=1000 deadline='2024-01-31'
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/extracting-pii/run.py
=======
from pydantic import BaseModel

import instructor
from openai import OpenAI

client = instructor.from_openai(OpenAI())


class Data(BaseModel):
    index: int
    data_type: str
    pii_value: str


class PIIDataExtraction(BaseModel):
    """
    Extracted PII data from a document, all data_types should try to have consistent property names
    """

    private_data: list[Data]

    def scrub_data(self, content):
        """
        Iterates over the private data and replaces the value with a placeholder in the form of
        <{data_type}_{i}>
        """

        for i, data in enumerate(self.private_data):
            content = content.replace(data.pii_value, f"<{data.data_type}_{i}>")

        return content


EXAMPLE_DOCUMENT = """
# Fake Document with PII for Testing PII Scrubbing Model

## Personal Story

John Doe was born on 01/02/1980. His social security number is 123-45-6789. He has been using the email address john.doe@email.com for years, and he can always be reached at 555-123-4567.

## Residence

John currently resides at 123 Main St, Springfield, IL, 62704. He's been living there for about 5 years now.

## Career

At the moment, John is employed at Company A. He started his role as a Software Engineer in January 2015 and has been with the company since then.
"""

# Define the PII Scrubbing Model
pii_data: PIIDataExtraction = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=PIIDataExtraction,
    messages=[
        {
            "role": "system",
            "content": "You are a world class PII scrubbing model, Extract the PII data from the following document",
        },
        {
            "role": "user",
            "content": EXAMPLE_DOCUMENT,
        },
    ],
)  # type: ignore


print("Extracted PII Data:")
print(pii_data.model_dump_json(indent=2))
"""
{
  "private_data": [
    {
      "index": 0,
      "data_type": "date",
      "pii_value": "01/02/1980"
    },
    {
      "index": 1,
      "data_type": "ssn",
      "pii_value": "123-45-6789"
    },
    {
      "index": 2,
      "data_type": "email",
      "pii_value": "john.doe@email.com"
    },
    {
      "index": 3,
      "data_type": "phone",
      "pii_value": "555-123-4567"
    },
    {
      "index": 4,
      "data_type": "address",
      "pii_value": "123 Main St, Springfield, IL, 62704"
    }
  ]
}
"""

# Scrub the PII Data from the document
print("Scrubbed Document:")
print(pii_data.scrub_data(EXAMPLE_DOCUMENT))
"""
# Fake Document with PII for Testing PII Scrubbing Model

## Personal Story

John Doe was born on <date_of_birth_0>. His social security number is <social_security_number_1>. He has been using the email address <email_address_2> for years, and he can always be reached at <phone_number_3>.

## Residence

John currently resides at <address_4>. He's been living there for about 5 years now.

## Career

At the moment, John is employed at <employment_5>. He started his role as a <job_title_6> in <employment_start_date_7> and has been with the company since then.
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/fastapi_app/main.py
=======
from fastapi import FastAPI
from instructor import OpenAISchema
import instructor.dsl as dsl
from pydantic import BaseModel, Field

app = FastAPI(title="Example Application using instructor")


class SearchRequest(BaseModel):
    body: str


class SearchQuery(OpenAISchema):
    title: str = Field(..., description="Question that the query answers")
    query: str = Field(
        ...,
        description="Detailed, comprehensive, and specific query to be used for semantic search",
    )


SearchResponse = dsl.MultiTask(
    subtask_class=SearchQuery,
    description="Correctly segmented set of search queries",
)


@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    task = (
        dsl.ChatCompletion(name="Segmenting Search requests example")
        | dsl.SystemTask(task="Segment search results")
        | dsl.TaggedMessage(content=request.body, tag="query")
        | dsl.TipsMessage(
            tips=[
                "Expand query to contain multiple forms of the same word (SSO -> Single Sign On)",
                "Use the title to explain what the query should return, but use the query to complete the search",
                "The query should be detailed, specific, and cast a wide net when possible",
            ]
        )
        | SearchRequest
    )
    return await task.acreate()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/fastapi_app/script.py
=======
from instructor import OpenAISchema, dsl
from pydantic import Field
import json


class SearchQuery(OpenAISchema):
    query: str = Field(
        ...,
        description="Detailed, comprehensive, and specific query to be used for semantic search",
    )


SearchResponse = dsl.MultiTask(
    subtask_class=SearchQuery,
    description="Correctly segmented set of search queries",
)


task = (
    dsl.ChatCompletion(name="Segmenting Search requests example")
    | dsl.SystemTask(task="Segment search results")
    | dsl.TaggedMessage(
        content="can you send me the data about the video investment and the one about spot the dog?",
        tag="query",
    )
    | dsl.TipsMessage(
        tips=[
            "Expand query to contain multiple forms of the same word (SSO -> Single Sign On)",
            "Use the title to explain what the query should return, but use the query to complete the search",
            "The query should be detailed, specific, and cast a wide net when possible",
        ]
    )
    | SearchResponse
)


print(json.dumps(task.kwargs, indent=1))
"""
{
  "tasks": [
    {
      "query": "data about video investment"
    },
    {
      "query": "data about spot the dog"
    }
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/fastapi_app/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/fizzbuzz/run.py
=======
from __future__ import annotations

from openai import OpenAI
import instructor

client = instructor.from_openai(OpenAI())


def fizzbuzz_gpt(n) -> list[int | str]:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=list[int | str],
        messages=[
            {
                "role": "user",
                "content": f"Return the first {n} numbers in fizzbuzz",
            },
        ],
    )  # type: ignore


if __name__ == "__main__":
    print(fizzbuzz_gpt(n=15))
    # > [1, 2, 'Fizz', 4, 'Buzz', 'Fizz', 7, 8, 'Fizz', 'Buzz', 11, 'Fizz', 13, 14, 'FizzBuzz']


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/gpt-engineer/generate.py
=======
import instructor

from openai import OpenAI
from pydantic import Field
from instructor import OpenAISchema

client = instructor.from_openai(OpenAI())


class File(OpenAISchema):
    """
    Correctly named file with contents.
    """

    file_name: str = Field(
        ..., description="The name of the file including the extension"
    )
    body: str = Field(..., description="Correct contents of a file")

    def save(self):
        with open(self.file_name, "w") as f:
            f.write(self.body)


class Program(OpenAISchema):
    """
    Set of files that represent a complete and correct program
    """

    files: list[File] = Field(..., description="List of files")


def develop(data: str) -> Program:
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        temperature=0.1,
        functions=[Program.openai_schema],
        function_call={"name": Program.openai_schema["name"]},
        messages=[
            {
                "role": "system",
                "content": "You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code. with correct imports.",
            },
            {
                "role": "user",
                "content": data,
            },
        ],
        max_tokens=1000,
    )
    return Program.from_response(completion)


if __name__ == "__main__":
    program = develop(
        """
        Create a fastapi app with a readme.md file and a main.py file with
        some basic math functions. the datamodels should use pydantic and
        the main.py should use fastapi. the readme.md should have a title
        and a description. The readme should contain some helpful infromation
        and a curl example"""
    )

    for file in program.files:
        print(file.file_name)
        print("-")
        print(file.body)
        print("\n\n\n")
    """
    readme.md
    -
    # FastAPI App

    This is a FastAPI app that provides some basic math functions.

    ## Usage

    To use this app, follow the instructions below:

    1. Install the required dependencies by running `pip install -r requirements.txt`.
    2. Start the app by running `uvicorn main:app --reload`.
    3. Open your browser and navigate to `http://localhost:8000/docs` to access the Swagger UI documentation.

    ## Example

    To perform a basic math operation, you can use the following curl command:

    ```bash
    curl -X POST -H "Content-Type: application/json" -d '{"operation": "add", "operands": [2, 3]}' http://localhost:8000/calculate
    ```





    main.py
    -
    from fastapi import FastAPI
    from pydantic import BaseModel

    app = FastAPI()


    class Operation(BaseModel):
        operation: str
        operands: list


    @app.post('/calculate')
    async def calculate(operation: Operation):
        if operation.operation == 'add':
            result = sum(operation.operands)
        elif operation.operation == 'subtract':
            result = operation.operands[0] - sum(operation.operands[1:])
        elif operation.operation == 'multiply':
            result = 1
            for operand in operation.operands:
                result *= operand
        elif operation.operation == 'divide':
            result = operation.operands[0]
            for operand in operation.operands[1:]:
                result /= operand
        else:
            result = None
        return {'result': result}





    requirements.txt
    -
    fastapi
    uvicorn
    pydantic
    """

    with open("program.json", "w") as f:
        f.write(Program.parse_obj(program).json())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/gpt-engineer/refactor.py
=======
import instructor

from openai import OpenAI
from pydantic import Field, parse_file_as
from instructor import OpenAISchema
from generate import Program

client = instructor.from_openai(OpenAI())


class Diff(OpenAISchema):
    """
    Changes that must be correctly made in a program's code repository defined as a
    complete diff (Unified Format) file which will be used to `patch` the repository.

    Example:
      --- /path/to/original	timestamp
      +++ /path/to/new	timestamp
      @@ -1,3 +1,9 @@
      +This is an important
      +notice! It should
      +therefore be located at
      +the beginning of this
      +document!
      +
       This part of the
       document has stayed the
       same from version to
      @@ -8,13 +14,8 @@
       compress the size of the
       changes.
      -This paragraph contains
      -text that is outdated.
      -It will be deleted in the
      -near future.
      -
       It is important to spell
      -check this dokument. On
      +check this document. On
       the other hand, a
       misspelled word isn't
       the end of the world.
      @@ -22,3 +23,7 @@
       this paragraph needs to
       be changed. Things can
       be added after it.
      +
      +This paragraph contains
      +important new additions
      +to this document.
    """

    diff: str = Field(
        ...,
        description=(
            "Changes in a code repository correctly represented in 'diff' format, "
            "correctly escaped so it could be used in a JSON"
        ),
    )


def refactor(new_requirements: str, program: Program) -> Diff:
    program_description = "\n".join(
        [f"{code.file_name}\n[[[\n{code.body}\n]]]\n" for code in program.files]
    )
    completion = client.chat.completions.create(
        model="gpt-4",
        temperature=0,
        functions=[Diff.openai_schema],
        function_call={"name": Diff.openai_schema["name"]},
        messages=[
            {
                "role": "system",
                "content": "You are a world class programming AI capable of refactor "
                "existing python repositories. You will name files correct, include "
                "__init__.py files and write correct python code, with correct imports. "
                "You'll deliver your changes in valid 'diff' format so that they could "
                "be applied using the 'patch' command. "
                "Make sure you put the correct line numbers, "
                "and that all lines that must be changed are correctly marked.",
            },
            {
                "role": "user",
                "content": new_requirements,
            },
            {
                "role": "user",
                "content": program_description,
            },
        ],
        max_tokens=1000,
    )
    return Diff.from_response(completion)


if __name__ == "__main__":
    program = parse_file_as(path="program.json", type_=Program)

    changes = refactor(
        new_requirements="Refactor this code to use flask instead.",
        program=program,
    )
    print(changes.diff)
    """
    --- readme.md
    +++ readme.md
    @@ -1,9 +1,9 @@
     # FastAPI App

    -This is a FastAPI app that provides some basic math functions.
    +This is a Flask app that provides some basic math functions.

     ## Usage

     To use this app, follow the instructions below:

     1. Install the required dependencies by running `pip install -r requirements.txt`.
    -2. Start the app by running `uvicorn main:app --reload`.
    +2. Start the app by running `flask run`.
     3. Open your browser and navigate to `http://localhost:5000/docs` to access the Swagger UI documentation.

     ## Example

     To perform a basic math operation, you can use the following curl command:

     ```bash
    -curl -X POST -H "Content-Type: application/json" -d '{"operation": "add", "operands": [2, 3]}' http://localhost:8000/calculate
    +curl -X POST -H "Content-Type: application/json" -d '{"operation": "add", "operands": [2, 3]}' http://localhost:5000/calculate
     ```

    --- main.py
    +++ main.py
    @@ -1,29 +1,29 @@
    -from fastapi import FastAPI
    -from pydantic import BaseModel
    +from flask import Flask, request, jsonify

    -app = FastAPI()
    +app = Flask(__name__)


    -class Operation(BaseModel):
    -    operation: str
    -    operands: list
    +@app.route('/calculate', methods=['POST'])
    +def calculate():
    +    data = request.get_json()
    +    operation = data.get('operation')
    +    operands = data.get('operands')


    -@app.post('/calculate')
    -async def calculate(operation: Operation):
    -    if operation.operation == 'add':
    -        result = sum(operation.operands)
    -    elif operation.operation == 'subtract':
    -        result = operation.operands[0] - sum(operation.operands[1:])
    -    elif operation.operation == 'multiply':
    +    if operation == 'add':
    +        result = sum(operands)
    +    elif operation == 'subtract':
    +        result = operands[0] - sum(operands[1:])
    +    elif operation == 'multiply':
             result = 1
    -        for operand in operation.operands:
    +        for operand in operands:
                 result *= operand
    -    elif operation.operation == 'divide':
    -        result = operation.operands[0]
    -        for operand in operation.operands[1:]:
    +    elif operation == 'divide':
    +        result = operands[0]
    +        for operand in operands[1:]:
                 result /= operand
         else:
             result = None
    -    return {'result': result}
    +    return jsonify({'result': result})

    --- requirements.txt
    +++ requirements.txt
    @@ -1,3 +1,2 @@
    -fastapi
    -uvicorn
    -pydantic
    +flask
    +flask-cors
    """

    with open("changes.diff", "w") as f:
        f.write(changes.diff)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/groq/groq_example.py
=======
import os
from pydantic import BaseModel, Field
from groq import Groq
import instructor


class Character(BaseModel):
    name: str
    fact: list[str] = Field(..., description="A list of facts about the subject")


client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

client = instructor.from_groq(client, mode=instructor.Mode.TOOLS)

resp = client.chat.completions.create(
    model="mixtral-8x7b-32768",
    messages=[
        {
            "role": "user",
            "content": "Tell me about the company Tesla",
        }
    ],
    response_model=Character,
)
print(resp.model_dump_json(indent=2))
"""
{
  "name": "Tesla",
  "fact": [
    "An American electric vehicle and clean energy company.",
    "Co-founded by Elon Musk, JB Straubel, Martin Eberhard, Marc Tarpenning, and Ian Wright in 2003.",
    "Headquartered in Austin, Texas.",
    "Produces electric vehicles, energy storage solutions, and more recently, solar energy products.",
    "Known for its premium electric vehicles, such as the Model S, Model 3, Model X, and Model Y.",
    "One of the world's most valuable car manufacturers by market capitalization.",
    "Tesla's CEO, Elon Musk, is also the CEO of SpaceX, Neuralink, and The Boring Company.",
    "Tesla operates the world's largest global network of electric vehicle supercharging stations.",
    "The company aims to accelerate the world's transition to sustainable transport and energy through innovative technologies and products."
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/groq/groq_example2.py
=======
import os
from pydantic import BaseModel
from groq import Groq
import instructor

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

client = instructor.from_groq(client, mode=instructor.Mode.TOOLS)


class UserExtract(BaseModel):
    name: str
    age: int


user: UserExtract = client.chat.completions.create(
    model="mixtral-8x7b-32768",
    response_model=UserExtract,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)

assert isinstance(user, UserExtract), "Should be instance of UserExtract"
assert user.name.lower() == "jason"
assert user.age == 25

print(user.model_dump_json(indent=2))
"""
{
  "name": "jason",
  "age": 25
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/iterables/run.py
=======
import time

from collections.abc import Iterable
from openai import OpenAI
from pydantic import BaseModel

import instructor


client = instructor.from_openai(OpenAI())


class User(BaseModel):
    name: str
    job: str
    age: int


def stream_extract(input: str) -> Iterable[User]:
    return client.chat.completions.create_iterable(
        model="gpt-4o",
        temperature=0.1,
        stream=True,
        response_model=User,
        messages=[
            {
                "role": "system",
                "content": "You are a perfect entity extraction system",
            },
            {
                "role": "user",
                "content": (
                    f"Consider the data below:\n{input}"
                    "Correctly segment it into entitites"
                    "Make sure the JSON is correct"
                ),
            },
        ],
        max_tokens=1000,
    )


start = time.time()
for user in stream_extract(
    input="Create 5 characters from the book Three Body Problem"
):
    delay = round(time.time() - start, 1)
    print(f"{delay} s: User({user})")
    """
    0.8 s: User(name='Ye Wenjie' job='Astrophysicist' age=60)
    1.1 s: User(name='Wang Miao' job='Nanomaterials Researcher' age=40)
    1.7 s: User(name='Shi Qiang' job='Detective' age=50)
    1.9 s: User(name='Ding Yi' job='Theoretical Physicist' age=45)
    1.9 s: User(name='Chang Weisi' job='Military Strategist' age=55)
    """
    # Notice that the first one would return at 5s bu the last one returned in 10s!


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/knowledge-graph/run.py
=======
import instructor

from graphviz import Digraph
from pydantic import BaseModel, Field
from openai import OpenAI


client = instructor.from_openai(OpenAI())


class Node(BaseModel):
    id: int
    label: str
    color: str


class Edge(BaseModel):
    source: int
    target: int
    label: str
    color: str = "black"


class KnowledgeGraph(BaseModel):
    nodes: list[Node] = Field(..., default_factory=list)
    edges: list[Edge] = Field(..., default_factory=list)


def generate_graph(input) -> KnowledgeGraph:
    return client.chat.completions.create(
        model="gpt-3.5-turbo-16k",
        messages=[
            {
                "role": "user",
                "content": f"Help me understand following by describing as a detailed knowledge graph: {input}",
            }
        ],
        response_model=KnowledgeGraph,
    )  # type: ignore


def visualize_knowledge_graph(kg: KnowledgeGraph):
    dot = Digraph(comment="Knowledge Graph")

    # Add nodes
    for node in kg.nodes:
        dot.node(str(node.id), node.label, color=node.color)

    # Add edges
    for edge in kg.edges:
        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)

    # Render the graph
    dot.render("knowledge_graph.gv", view=True)


graph: KnowledgeGraph = generate_graph("Teach me about quantum mechanics")
visualize_knowledge_graph(graph)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/knowledge-graph/run_stream.py
=======
from openai import OpenAI
import instructor

from graphviz import Digraph
from typing import Optional

from pydantic import BaseModel, Field

client = instructor.from_openai(OpenAI())


class Node(BaseModel):
    id: int
    label: str
    color: str

    def __hash__(self) -> int:
        return hash((id, self.label))


class Edge(BaseModel):
    source: int
    target: int
    label: str
    color: str = "black"

    def __hash__(self) -> int:
        return hash((self.source, self.target, self.label))


class KnowledgeGraph(BaseModel):
    nodes: Optional[list[Node]] = Field(..., default_factory=list)
    edges: Optional[list[Edge]] = Field(..., default_factory=list)

    def update(self, other: "KnowledgeGraph") -> "KnowledgeGraph":
        """Updates the current graph with the other graph, deduplicating nodes and edges."""
        return KnowledgeGraph(
            nodes=list(set(self.nodes + other.nodes)),
            edges=list(set(self.edges + other.edges)),
        )

    def draw(self, prefix: str = None):
        dot = Digraph(comment="Knowledge Graph")

        # Add nodes
        for node in self.nodes:
            dot.node(str(node.id), node.label, color=node.color)

        # Add edges
        for edge in self.edges:
            dot.edge(
                str(edge.source), str(edge.target), label=edge.label, color=edge.color
            )
        dot.render(prefix, format="png", view=True)


def generate_graph(input: list[str]) -> KnowledgeGraph:
    cur_state = KnowledgeGraph()
    num_iterations = len(input)
    for i, inp in enumerate(input):
        new_updates = client.chat.completions.create(
            model="gpt-3.5-turbo-16k",
            messages=[
                {
                    "role": "system",
                    "content": """You are an iterative knowledge graph builder.
                    You are given the current state of the graph, and you must append the nodes and edges 
                    to it Do not procide any duplcates and try to reuse nodes as much as possible.""",
                },
                {
                    "role": "user",
                    "content": f"""Extract any new nodes and edges from the following:
                    # Part {i}/{num_iterations} of the input:

                    {inp}""",
                },
                {
                    "role": "user",
                    "content": f"""Here is the current state of the graph:
                    {cur_state.model_dump_json(indent=2)}""",
                },
            ],
            response_model=KnowledgeGraph,
        )  # type: ignore

        # Update the current state
        cur_state = cur_state.update(new_updates)
        cur_state.draw(prefix=f"iteration_{i}")
    return cur_state


# here we assume that we have to process the text in chunks
# one at a time since they may not fit in the prompt otherwise
text_chunks = [
    "Jason knows a lot about quantum mechanics. He is a physicist. He is a professor",
    "Professors are smart.",
    "Sarah knows Jason and is a student of his.",
    "Sarah is a student at the University of Toronto. and UofT is in Canada.",
]

graph: KnowledgeGraph = generate_graph(text_chunks)

graph.draw(prefix="final")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/learn-async/run.py
=======
import time
import asyncio

import instructor
from pydantic import BaseModel
from openai import AsyncOpenAI


client = instructor.apatch(AsyncOpenAI())


class Timer:
    def __init__(self, name):
        self.name = name
        self.start = None
        self.end = None

    async def __aenter__(self):
        self.start = time.time()

    async def __aexit__(self, *args, **kwargs):
        self.end = time.time()
        print(f"{self.name} took {(self.end - self.start):.2f} seconds")


class Person(BaseModel):
    name: str
    age: int


async def extract_person(text: str) -> Person:
    return await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": text},
        ],
        response_model=Person,
    )


async def main():
    """We'll use this to run the example. and time how long each one takes!

    0. for loop
    1. asyncio.gather
    2. asyncio.as_completed
    """
    dataset = [
        "My name is John and I am 20 years old",
        "My name is Mary and I am 21 years old",
        "My name is Bob and I am 22 years old",
        "My name is Alice and I am 23 years old",
        "My name is Jane and I am 24 years old",
        "My name is Joe and I am 25 years old",
        "My name is Jill and I am 26 years old",
    ]

    """
    This is the simplest way to run multiple async functions in series.
    It will wait for each function to complete before continuing.
    """
    async with Timer("for loop"):
        persons = []
        for text in dataset:
            person = await extract_person(text)
            persons.append(person)
        print("for loop:", persons)

    """
    This is the simplest way to run multiple async functions in parallel.
    It will wait for all of the functions to complete before continuing.
    """
    async with Timer("asyncio.gather"):
        tasks_get_persons = [extract_person(text) for text in dataset]
        all_person = await asyncio.gather(*tasks_get_persons)
        print("asyncio.gather:", all_person)

    """
    This is a bit more complicated, but it allows us to process each
    person as soon as they are ready. This is useful if you have a
    large dataset and want to start processing the results as soon
    as they are ready.
    """
    async with Timer("asyncio.as_completed"):
        all_persons = []
        tasks_get_persons = [extract_person(text) for text in dataset]
        for person in asyncio.as_completed(tasks_get_persons):
            all_persons.append(await person)
        print("asyncio.as_copmleted:", all_persons)

    """
    If we want to rate limit our requests, we can use the
    semaphore to limit the number of concurrent requests.
    """

    # Create a semaphore that will only allow 2 concurrent requests
    sem = asyncio.Semaphore(2)

    async def rate_limited_extract_person(text: str) -> Person:
        async with sem:
            return await extract_person(text)

    async with Timer("asyncio.gather (rate limited)"):
        tasks_get_persons = [rate_limited_extract_person(text) for text in dataset]
        resp = await asyncio.gather(*tasks_get_persons)
        print("asyncio.gather (rate limited):", resp)

    async with Timer("asyncio.as_completed (rate limited)"):
        all_persons = []
        tasks_get_persons = [rate_limited_extract_person(text) for text in dataset]
        for person in asyncio.as_completed(tasks_get_persons):
            all_persons.append(await person)
        print("asyncio.as_completed (rate limited):", all_persons)


if __name__ == "__main__":
    asyncio.run(main())
    """
    for loop took 6.17 seconds

    asyncio.gather took 1.11 seconds
    asyncio.as_completed took 0.87 seconds

    asyncio.gather (rate limited) took 3.04 seconds
    asyncio.as_completed (rate limited) took 3.26 seconds
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logfire/classify.py
=======
import enum
from pydantic import BaseModel
from openai import OpenAI
import instructor
import logfire


class Labels(str, enum.Enum):
    """Enumeration for single-label text classification."""

    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Class for a single class label prediction.
    """

    class_label: Labels


openai_client = OpenAI()
logfire.configure(pydantic_plugin=logfire.PydanticPlugin(record="all"))
logfire.instrument_openai(openai_client)
client = instructor.from_openai(openai_client)


@logfire.instrument("classification", extract_args=True)
def classify(data: str) -> SinglePrediction:
    """Perform single-label classification on the input text."""
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {data}",
            },
        ],
    )


if __name__ == "__main__":
    emails = [
        "Hello there I'm a Nigerian prince and I want to give you money",
        "Meeting with Thomas has been set at Friday next week",
        "Here are some weekly product updates from our marketing team",
    ]

    for email in emails:
        classify(email)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logfire/image.py
=======
import instructor
from io import StringIO
from typing import Annotated, Any
from collections.abc import Iterable
from pydantic import (
    BeforeValidator,
    InstanceOf,
    WithJsonSchema,
    BaseModel,
)
import pandas as pd
from openai import OpenAI
import logfire

openai_client = OpenAI()
logfire.configure(pydantic_plugin=logfire.PydanticPlugin(record="all"))
logfire.instrument_openai(openai_client)
client = instructor.from_openai(openai_client, mode=instructor.Mode.MD_JSON)


def md_to_df(data: Any) -> Any:
    # Convert markdown to DataFrame
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),  # Process data
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .applymap(lambda x: x.strip())
        )
    return data


MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    WithJsonSchema(
        {
            "type": "string",
            "description": "The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate",
        }
    ),
]


class Table(BaseModel):
    caption: str
    dataframe: MarkdownDataFrame


@logfire.instrument("extract-table", extract_args=True)
def extract_table_from_image(url: str) -> Iterable[Table]:
    return client.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Iterable[Table],
        max_tokens=1800,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Extract out a table from the image. Only extract out the total number of skiiers.",
                    },
                    {"type": "image_url", "image_url": {"url": url}},
                ],
            }
        ],
    )


url = "https://cdn.statcdn.com/Infographic/images/normal/16330.jpeg"
tables = extract_table_from_image(url)
for table in tables:
    print(table.caption, end="\n")
    print(table.dataframe.to_markdown())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logfire/validate.py
=======
from typing import Annotated
from pydantic import BaseModel, ValidationError
from pydantic.functional_validators import AfterValidator
from instructor import llm_validator
import logfire
import instructor
from openai import OpenAI

openai_client = OpenAI()
logfire.configure(pydantic_plugin=logfire.PydanticPlugin(record="all"))
logfire.instrument_openai(openai_client)
client = instructor.from_openai(openai_client)


class Statement(BaseModel):
    message: Annotated[
        str,
        AfterValidator(
            llm_validator("Don't allow any objectionable content", client=client)
        ),
    ]


messages = [
    "I think we should always treat violence as the best solution",
    "There are some great pastries down the road at this bakery I know",
]

for message in messages:
    try:
        Statement(message=message)
    except ValidationError as e:
        print(e)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logfire-fastapi/server.py
=======
from pydantic import BaseModel
from fastapi import FastAPI
from openai import AsyncOpenAI
import instructor
import logfire
import asyncio
from collections.abc import Iterable
from fastapi.responses import StreamingResponse


class UserData(BaseModel):
    query: str


class MultipleUserData(BaseModel):
    queries: list[str]


class UserDetail(BaseModel):
    name: str
    age: int


app = FastAPI()
openai_client = AsyncOpenAI()
logfire.configure(pydantic_plugin=logfire.PydanticPlugin(record="all"))
logfire.instrument_fastapi(app)
logfire.instrument_openai(openai_client)
client = instructor.from_openai(openai_client)


@app.post("/user", response_model=UserDetail)
async def endpoint_function(data: UserData) -> UserDetail:
    user_detail = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": f"Extract: `{data.query}`"},
        ],
    )
    logfire.info("/User returning", value=user_detail)
    return user_detail


@app.post("/many-users", response_model=list[UserDetail])
async def extract_many_users(data: MultipleUserData):
    async def extract_user(query: str):
        user_detail = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            response_model=UserDetail,
            messages=[
                {"role": "user", "content": f"Extract: `{query}`"},
            ],
        )
        logfire.info("/User returning", value=user_detail)
        return user_detail

    coros = [extract_user(query) for query in data.queries]
    return await asyncio.gather(*coros)


@app.post("/extract", response_class=StreamingResponse)
async def extract(data: UserData):
    supressed_client = AsyncOpenAI()
    logfire.instrument_openai(supressed_client, suppress_other_instrumentation=False)
    client = instructor.from_openai(supressed_client)
    users = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Iterable[UserDetail],
        stream=True,
        messages=[
            {"role": "user", "content": data.query},
        ],
    )

    async def generate():
        with logfire.span("Generating User Response Objects"):
            async for user in users:
                resp_json = user.model_dump_json()
                logfire.info("Returning user object", value=resp_json)

                yield resp_json

    return StreamingResponse(generate(), media_type="text/event-stream")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logfire-fastapi/test.py
=======
import requests

response = requests.post(
    "http://127.0.0.1:3000/extract",
    json={
        "query": "Alice and Bob are best friends. They are currently 32 and 43 respectively. "
    },
    stream=True,
)

for chunk in response.iter_content(chunk_size=1024):
    if chunk:
        print(str(chunk, encoding="utf-8"), end="\n")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/logging/run.py
=======
import instructor
import openai
import logging

from pydantic import BaseModel


# Set logging to DEBUG
logging.basicConfig(level=logging.DEBUG)

client = instructor.from_openai(openai.OpenAI())


class UserDetail(BaseModel):
    name: str
    age: int


user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=UserDetail,
    messages=[
        {"role": "user", "content": "Extract Jason is 25 years old"},
    ],
)  # type: ignore

""" 
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='/Users/jasonliu/dev/instructor/.venv/lib/python3.11/site-packages/certifi/cacert.pem'
DEBUG:instructor:Patching `client.chat.completions.create` with mode=<Mode.FUNCTIONS: 'function_call'>
DEBUG:instructor:max_retries: 1
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Extract Jason is 25 years old'}], 'model': 'gpt-3.5-turbo', 'function_call': {'name': 'UserDetail'}, 'functions': [{'name': 'UserDetail', 'description': 'Correctly extracted `UserDetail` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}]}}
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x105062c90>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x100748680> server_hostname='api.openai.com' timeout=5.0
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x101caa150>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 12 Feb 2024 14:55:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'scribe-ai'), (b'openai-processing-ms', b'483'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999975'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_f0fa476897ae165fc50fa90b7968595b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=e2_yCrwo4frh6Oq4ZufCEhNJ4lSGJ2.MMtk45X8lrMM-1707749745-1-AfWk8CyACc7aZo6GpCI82FBfI/wmPEFZLNO/Cr3eavTW3xKVFCS7G9jvwYTFLXjJr0cttYsXeLAnjwipw18R0Vo=; path=/; expires=Mon, 12-Feb-24 15:25:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=PyVVCGSMxTg1p.woYvHVVC9E3n69faOs5FOxaDdjXOM-1707749745711-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8545aca30c1fa22f-YYZ'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG:httpcore.connection:close.started
DEBUG:httpcore.connection:close.complete
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/match_language/run_v1.py
=======
from pydantic import BaseModel
from instructor import patch
from openai import AsyncOpenAI
from langdetect import detect

docs = map(
    lambda x: x.strip(),
    """
         ,       ,       ,                                   

---

Mga modelo ng wika ay naging mas sopistikado sa nagdaang mga taon, na nagbibigay-daan sa pagbuo ng mga natural at madaling basahing teksto, at nagpapakita ng mahusay na pagganap sa iba't ibang gawain tulad ng awtomatikong pagsasalin, pagsagot sa mga tanong, at pagbuo ng malikhain na teksto. Ang mga modelo na ito ay sinanay sa napakalaking mga dataset ng teksto at kayang hulihin ang istruktura at mga nuances ng natural na wika. Ang mga pagpapabuti sa mga modelo ng wika ay maaaring magdulot ng rebolusyon sa komunikasyon sa pagitan ng mga computer at tao, at inaasahan ang higit pang pag-unlad sa hinaharap.

---

Ngaahi motua lea kuo nau hoko o fakaofoofa ange i he ngaahi tau fakamuimui ni, o fakafaingofuai e fakatupu o e ngaahi konga tohi oku lelei mo fakanatula pea oku nau fakahaai a e ngaahi ola lelei i he ngaahi ngue kehekehe o hang ko e liliu fakatita, tali fehui, mo e fakatupu o e konga tohi fakaatamai. Ko e ako a e ngaahi motua ni i he ngaahi seti o e fakamatala tohi lahi pea oku nau malava o puke a e fakafuofua mo e ngaahi mea iiki o e lea fakanatula. E lava ke fakatupu e he ngaahi fakaleleii ki he ngaahi motua lea ha liliu lahi i he fetu'utaki i he vahaa o e ngaahi komipiuta mo e kakai, pea oku amanaki e toe fakalakalaka ange ia i he kahau.

---

Dil modelleri son yllarda daha da geliti, akc ve doal metinler retmeyi mmkn klyor ve makine evirisi, soru cevaplama ve yaratc metin oluturma gibi eitli grevlerde mkemmel performans gsteriyor. Bu modeller, devasa metin veri setlerinde eitilir ve doal dilin yapsn ve nanslarn yakalayabilir. Dil modellerindeki iyiletirmeler, bilgisayarlar ve insanlar arasndaki iletiimde devrim yaratabilir ve gelecekte daha da ilerleme bekleniyor.

---

M hnh ngn ng  tr nn tinh vi hn trong nhng nm gn y, cho php to ra cc vn bn tri chy v t nhin, ng thi th hin hiu sut xut sc trong cc nhim v khc nhau nh dch my, tr li cu hi v to vn bn sng to. Cc m hnh ny c hun luyn trn cc tp d liu vn bn khng l v c th nm bt cu trc v sc thi ca ngn ng t nhin. Nhng ci tin trong m hnh ngn ng c th mang li cuc cch mng trong giao tip gia my tnh v con ngi, v ngi ta k vng s c nhng tin b hn na trong tng lai.

---

Les modles de langage sont devenus de plus en plus sophistiqus ces dernires annes, permettant de gnrer des textes fluides et naturels, et de performer dans une varit de tches telles que la traduction automatique, la rponse aux questions et la gnration de texte cratif. Entrans sur d'immenses ensembles de donnes textuelles, ces modles sont capables de capturer la structure et les nuances du langage naturel, ouvrant la voie  une rvolution dans la communication entre les ordinateurs et les humains.

---

,,,,,

---

In den letzten Jahren sind Sprachmodelle immer ausgefeilter geworden und knnen flssige, natrlich klingende Texte generieren und in verschiedenen Aufgaben wie maschineller bersetzung, Beantwortung von Fragen und Generierung kreativer Texte hervorragende Leistungen erbringen. Diese Modelle werden auf riesigen Textdatenstzen trainiert und knnen die Struktur und Nuancen natrlicher Sprache erfassen, was zu einer Revolution in der Kommunikation zwischen Computern und Menschen fhren knnte.

---

           ,         ,   , ,                                                ,        

---


""".split("---"),
)

# Patch the OpenAI client to enable response_model
client = patch(AsyncOpenAI())


class GeneratedSummary(BaseModel):
    summary: str


async def summarize_text(text: str):
    response = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=GeneratedSummary,
        messages=[
            {
                "role": "system",
                "content": "Generate a concise summary in the language of the article. ",
            },
            {
                "role": "user",
                "content": f"Summarize the following text in a concise way:\n{text}",
            },
        ],
    )  # type: ignore
    return response.summary, text


if __name__ == "__main__":
    import asyncio

    async def main():
        results = await asyncio.gather(*[summarize_text(doc) for doc in docs])
        for summary, doc in results:
            source_lang = detect(doc)
            target_lang = detect(summary)
            print(
                f"Source: {source_lang}, Summary: {target_lang}, Match: {source_lang == target_lang}"
            )

    asyncio.run(main())
    """
    Source: et, Summary: en, Match: False
    Source: tl, Summary: tl, Match: True
    Source: sw, Summary: en, Match: False
    Source: tr, Summary: tr, Match: True
    Source: vi, Summary: en, Match: False
    Source: fr, Summary: fr, Match: True
    Source: zh-cn, Summary: en, Match: False
    Source: de, Summary: de, Match: True
    Source: hi, Summary: en, Match: False
    Source: ja, Summary: en, Match: False
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/match_language/run_v2.py
=======
from pydantic import BaseModel, Field
from instructor import patch
from openai import AsyncOpenAI
from langdetect import detect

docs = map(
    lambda x: x.strip(),
    """
         ,       ,       ,                                   

---

Mga modelo ng wika ay naging mas sopistikado sa nagdaang mga taon, na nagbibigay-daan sa pagbuo ng mga natural at madaling basahing teksto, at nagpapakita ng mahusay na pagganap sa iba't ibang gawain tulad ng awtomatikong pagsasalin, pagsagot sa mga tanong, at pagbuo ng malikhain na teksto. Ang mga modelo na ito ay sinanay sa napakalaking mga dataset ng teksto at kayang hulihin ang istruktura at mga nuances ng natural na wika. Ang mga pagpapabuti sa mga modelo ng wika ay maaaring magdulot ng rebolusyon sa komunikasyon sa pagitan ng mga computer at tao, at inaasahan ang higit pang pag-unlad sa hinaharap.

---

Ngaahi motua lea kuo nau hoko o fakaofoofa ange i he ngaahi tau fakamuimui ni, o fakafaingofuai e fakatupu o e ngaahi konga tohi oku lelei mo fakanatula pea oku nau fakahaai a e ngaahi ola lelei i he ngaahi ngue kehekehe o hang ko e liliu fakatita, tali fehui, mo e fakatupu o e konga tohi fakaatamai. Ko e ako a e ngaahi motua ni i he ngaahi seti o e fakamatala tohi lahi pea oku nau malava o puke a e fakafuofua mo e ngaahi mea iiki o e lea fakanatula. E lava ke fakatupu e he ngaahi fakaleleii ki he ngaahi motua lea ha liliu lahi i he fetu'utaki i he vahaa o e ngaahi komipiuta mo e kakai, pea oku amanaki e toe fakalakalaka ange ia i he kahau.

---

Dil modelleri son yllarda daha da geliti, akc ve doal metinler retmeyi mmkn klyor ve makine evirisi, soru cevaplama ve yaratc metin oluturma gibi eitli grevlerde mkemmel performans gsteriyor. Bu modeller, devasa metin veri setlerinde eitilir ve doal dilin yapsn ve nanslarn yakalayabilir. Dil modellerindeki iyiletirmeler, bilgisayarlar ve insanlar arasndaki iletiimde devrim yaratabilir ve gelecekte daha da ilerleme bekleniyor.

---

M hnh ngn ng  tr nn tinh vi hn trong nhng nm gn y, cho php to ra cc vn bn tri chy v t nhin, ng thi th hin hiu sut xut sc trong cc nhim v khc nhau nh dch my, tr li cu hi v to vn bn sng to. Cc m hnh ny c hun luyn trn cc tp d liu vn bn khng l v c th nm bt cu trc v sc thi ca ngn ng t nhin. Nhng ci tin trong m hnh ngn ng c th mang li cuc cch mng trong giao tip gia my tnh v con ngi, v ngi ta k vng s c nhng tin b hn na trong tng lai.

---

Les modles de langage sont devenus de plus en plus sophistiqus ces dernires annes, permettant de gnrer des textes fluides et naturels, et de performer dans une varit de tches telles que la traduction automatique, la rponse aux questions et la gnration de texte cratif. Entrans sur d'immenses ensembles de donnes textuelles, ces modles sont capables de capturer la structure et les nuances du langage naturel, ouvrant la voie  une rvolution dans la communication entre les ordinateurs et les humains.

---

,,,,,

---

In den letzten Jahren sind Sprachmodelle immer ausgefeilter geworden und knnen flssige, natrlich klingende Texte generieren und in verschiedenen Aufgaben wie maschineller bersetzung, Beantwortung von Fragen und Generierung kreativer Texte hervorragende Leistungen erbringen. Diese Modelle werden auf riesigen Textdatenstzen trainiert und knnen die Struktur und Nuancen natrlicher Sprache erfassen, was zu einer Revolution in der Kommunikation zwischen Computern und Menschen fhren knnte.

---

           ,         ,   , ,                                                ,        

---


""".split("---"),
)

# Patch the OpenAI client to enable response_model
client = patch(AsyncOpenAI())


class GeneratedSummary(BaseModel):
    detected_language: str = Field(
        description="The language code of the original article. The summary must be generated in this same language.",
    )
    summary: str


async def summarize_text(text: str):
    response = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=GeneratedSummary,
        messages=[
            {
                "role": "system",
                "content": "Generate a concise summary in the language of the article. ",
            },
            {
                "role": "user",
                "content": f"Summarize the following text in a concise way:\n{text}",
            },
        ],
    )  # type: ignore
    return response.detected_language, response.summary, text


if __name__ == "__main__":
    import asyncio

    async def main():
        results = await asyncio.gather(*[summarize_text(doc) for doc in docs])
        for lang, summary, doc in results:
            source_lang = detect(doc)
            target_lang = detect(summary)
            print(
                f"Source: {source_lang}, Summary: {target_lang}, Match: {source_lang == target_lang}, Detected: {lang}"
            )

    asyncio.run(main())
    """
    Source: et, Summary: et, Match: True, Detected: hy
    Source: tl, Summary: tl, Match: True, Detected: tl
    Source: sw, Summary: sw, Match: True, Detected: to
    Source: tr, Summary: tr, Match: True, Detected: tr
    Source: vi, Summary: vi, Match: True, Detected: vi
    Source: fr, Summary: fr, Match: True, Detected: fr
    Source: zh-cn, Summary: zh-cn, Match: True, Detected: zh
    Source: de, Summary: de, Match: True, Detected: de
    Source: hi, Summary: hi, Match: True, Detected: hi
    Source: ja, Summary: ja, Match: True, Detected: ja
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/mistral/mistral.py
=======
from pydantic import BaseModel
from mistralai.client import MistralClient
from instructor import from_mistral
from instructor.function_calls import Mode
import os


class UserDetails(BaseModel):
    name: str
    age: int


# enables `response_model` in chat call
client = MistralClient(api_key=os.environ.get("MISTRAL_API_KEY"))
instructor_client = from_mistral(
    client=client,
    model="mistral-large-latest",
    mode=Mode.MISTRAL_TOOLS,
    max_tokens=1000,
)

resp = instructor_client.messages.create(
    response_model=UserDetails,
    messages=[{"role": "user", "content": "Jason is 10"}],
    temperature=0,
)

print(resp)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/multi-actions/run.py
=======
import instructor
import enum

from typing import Optional
from pydantic import BaseModel, Field
from openai import OpenAI

client = instructor.from_openai(OpenAI())


class Action(enum.Enum):
    CREATE = "create_task"
    DELETE = "close_task"
    UPDATE = "update_task"


class Projects(enum.Enum):
    FRONTLINE_QA_AI = "frontline_qa_ai"
    FUTURE_OF_PROGRAMMING = "future_of_programming"
    PERSONAL_SITE = "personal_site"
    NORDIC_HAMSTRING_CURLS = "nordic_hamstring_curls"


class Buckets(enum.Enum):
    FINANCE = "finance"
    PURVIEW_OPERATIONS = "purview_operations"
    TASKBOT = "taskbot"
    CHECKBOT = "checkbot"
    NIGHT_HACKING = "night_hacking"
    TICKLER = "tickler"


class TaskAction(BaseModel):
    id: int
    method: Action = Field(
        description="Method of creating, for closing a task the task, to close a task only a id is required"
    )
    waiting_on: Optional[list[int]] = Field(
        None, description="IDs of tasks that this task is waiting on"
    )
    name: Optional[str] = Field(None, description="Name of the task")
    notes: Optional[str] = Field(None, description="Notes about the task")
    bucket: Optional[Buckets] = Field(
        None, description="Bucket of the task, to set, or update"
    )
    project: Optional[Projects] = Field(
        None, description="Project of the task, to set, or update"
    )


class Response(BaseModel):
    text: str = Field(description="The text of the response")
    task_action: Optional[list[TaskAction]] = Field(
        description="The action to take on the task"
    )


initial_messages = [
    {
        "role": "system",
        "content": "You are an AI assistant. have the ability to create, update, and close tasks.",
    },
    {
        "role": "assistant",
        "content": """
        The task is below. When assisting the user, reference the details from this task.

        [BEGIN TASK]
            id: 23
            Name: Create 10 new GIFs
            Description: Create 10 new GIFs for the Taskbot page on the user's personal site. They should be similar to the existing GIFs, but with different use cases.
            Projects: Personal site
            Buckets: Taskbot
            Updates:
        [BEGIN UPDATE]
            **User Update - September 01, 2023 03:58:00 PM EDT**
            The user plans to create the GIFs in the background as they work through their daily tasks. They aim to produce about one to two GIFs per day. If this plan doesn't work, they will reconsider their strategy.
        [END UPDATE]
        [END TASK]
    """,
    },
    {"role": "assistant", "content": "What's up with this task?"},
    {
        "role": "user",
        "content": "Change it to 20, then make a new task for when its done make 20 more that moves.",
    },
]

response: Response = client.chat.completions.create(
    messages=initial_messages, response_model=Response, model="gpt-4"
)  # type: ignore

print(response.model_dump_json(indent=2))
{
    "text": "Updating task to create 20 GIFs and creating a new task to create an additional 20 animated GIFs after the initial task is done.",
    "task_action": [
        {
            "id": 23,
            "method": "update_task",
            "waiting_on": None,
            "name": "Create 20 new GIFs",
            "notes": "The user increased the number of GIFs from 10 to 20. They plan to create these as they work through their daily tasks, creating about one to two GIFs per day. If this plan doesn't work, they will reconsider their strategy.",
            "bucket": "taskbot",
            "project": "personal_site",
        },
        {
            "id": 24,
            "method": "create_task",
            "waiting_on": [23],
            "name": "Create 20 new animated GIFs",
            "notes": "The task will be initiated once the task with id 23 is completed.",
            "bucket": "taskbot",
            "project": "personal_site",
        },
    ],
}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/multiple_search_queries/diagram.py
=======
import erdantic as erd

from segment_search_queries import MultiSearch

diagram = erd.create(MultiSearch)
diagram.draw("examples/segment_search_queries/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/multiple_search_queries/segment_search_queries.py
=======
import enum
import instructor

from openai import OpenAI
from pydantic import Field, BaseModel

client = instructor.from_openai(OpenAI())


class SearchType(str, enum.Enum):
    """Enumeration representing the types of searches that can be performed."""

    VIDEO = "video"
    EMAIL = "email"


class Search(BaseModel):
    """
    Class representing a single search query which contains title, query and the search type
    """

    search_title: str = Field(..., description="Title of the request")
    query: str = Field(..., description="Query to search for relevant content")
    type: SearchType = Field(..., description="Type of search")

    async def execute(self):
        import asyncio

        await asyncio.sleep(1)
        print(
            f"Searching for `{self.search_title}` with query `{self.query}` using `{self.type}`"
        )


class MultiSearch(BaseModel):
    """
    Class representing multiple search queries.
    Make sure they contain all the required attributes

    Args:
        searches (List[Search]): The list of searches to perform.
    """

    searches: list[Search] = Field(..., description="List of searches")

    def execute(self):
        import asyncio

        loop = asyncio.get_event_loop()

        tasks = asyncio.gather(*[search.execute() for search in self.searches])
        return loop.run_until_complete(tasks)


def segment(data: str) -> MultiSearch:
    """
    Convert a string into multiple search queries using OpenAI's GPT-3 model.

    Args:
        data (str): The string to convert into search queries.

    Returns:
        MultiSearch: An object representing the multiple search queries.
    """

    completion = client.chat.completions.create(
        model="gpt-4-0613",
        temperature=0.1,
        response_model=MultiSearch,
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            {
                "role": "user",
                "content": f"Consider the data below:\n{data} and segment it into multiple search queries",
            },
        ],
        max_tokens=1000,
    )
    return MultiSearch.from_response(completion)


if __name__ == "__main__":
    queries = segment(
        "Please send me the video from last week about the investment case study and also documents about your GPDR policy?"
    )

    queries.execute()
    # >>> Searching for `Video` with query `investment case study` using `SearchType.VIDEO`
    # >>> Searching for `Documents` with query `GPDR policy` using `SearchType.EMAIL`


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/open_source_examples/openrouter.py
=======
import os
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional
from instructor import Maybe, Mode

# Extract API key from environment
openrouter_api_key = os.environ.get("OPENROUTER_API_KEY")
assert openrouter_api_key, "OPENROUTER_API_KEY is not set in environment variables"

# Base URL for OpenAI client
openrouter_base_url = os.environ.get("OPENROUTER_BASE_URL")
assert openrouter_base_url, "OPENROUTER_BASE_URL is not set in environment variables"

# Initialize OpenAI client
client = instructor.from_openai(
    OpenAI(api_key=openrouter_api_key, base_url=openrouter_base_url),
    mode=Mode.JSON,
)

data = [
    "Brandon is 33 years old. He works as a solution architect.",
    "Jason is 25 years old. He is the GOAT.",
    "Dominic is 45 years old. He is retired.",
    "Jenny is 72. She is a wife and a CEO.",
    "Holly is 22. She is an explorer.",
    "There onces was a prince, named Benny. He ruled for 10 years, which just ended. He started at 22.",
    "Simon says, why are you 22 years old marvin?",
]


if __name__ == "__main__":

    class UserDetail(BaseModel):
        name: str = Field(description="Name extracted from the text")
        age: int = Field(description="Age extracted from the text")
        occupation: Optional[str] = Field(
            default=None, description="Occupation extracted from the text"
        )

    for content in data:
        MaybeUser = Maybe(UserDetail)
        user = client.chat.completions.create(
            response_model=MaybeUser,
            model="teknium/openhermes-2.5-mistral-7b",
            messages=[
                {
                    "role": "system",
                    "content": f"You are an expert at outputting json. You always output valid json based on this schema: {MaybeUser.model_json_schema()}",
                },
                {
                    "role": "user",
                    "content": f"Extract the user details from the following text: {content}. Match your response the correct schema",
                },
            ],
        )
        # Output the error or the result.
        if user.error:
            print(f"Error: {user.error}")
        if user.result:
            print(f"Result: {user.result}")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/open_source_examples/perplexity.py
=======
import os
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional
from instructor import Maybe, Mode

# Extract API key from environment
perplexity_api_key = os.environ.get("PERPLEXITY_API_KEY")
assert perplexity_api_key, "PERPLEXITY_API_KEY is not set in environment variables"

# Base URL for OpenAI
perplexity_base_url = os.environ.get("PERPLEXITY_BASE_URL")
assert perplexity_base_url, "PERPLEXITY_BASE_URL is not set in environment variables"

# Initialize OpenAI client
client = instructor.from_openai(
    OpenAI(api_key=perplexity_api_key, base_url=perplexity_base_url),
    mode=Mode.JSON,
)

# For direct reference here. See https://docs.perplexity.ai/docs/model-cards for updates
# Recommended is pplx-70b-chat
models = [
    "codellama-34b-instruct",
    "llama-2-70b-chat",
    "mistral-7b-instruct",
    "pplx-7b-chat",
    "pplx-70b-chat",
    "pplx-7b-online",
    "pplx-70b-online",
]

data = [
    "Brandon is 33 years old. He works as a solution architect.",
    "Jason is 25 years old. He is the GOAT.",
    "Dominic is 45 years old. He is retired.",
    "Jenny is 72. She is a wife and a CEO.",
    "Holly is 22. She is an explorer.",
    "There onces was a prince, named Benny. He ruled for 10 years, which just ended. He started at 22.",
    "Simon says, why are you 22 years old marvin?",
]


if __name__ == "__main__":

    class UserDetail(BaseModel):
        name: str = Field(description="Name extracted from the text")
        age: int = Field(description="Age extracted from the text")
        occupation: Optional[str] = Field(
            default=None, description="Occupation extracted from the text"
        )

    for content in data:
        MaybeUser = Maybe(UserDetail)
        user = client.chat.completions.create(
            response_model=MaybeUser,
            model="pplx-70b-chat",
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at outputting json. You always output valid JSON based on the pydantic schema given to you.",
                },
                {
                    "role": "user",
                    "content": f"Extract the user details from the following text: {content}. Match your response to the following schema: {MaybeUser.model_json_schema()}",
                },
            ],
            max_retries=3,
        )
        # Output the error or the result.
        if user.error:
            print(f"Error: {user.error}")
        if user.result:
            print(f"Result: {user.result}")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/open_source_examples/runpod.py
=======
import os
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional
from instructor import Mode

# Extract API key from environment
runpod_api_key = os.environ.get("RUNPOD_API_KEY")
assert runpod_api_key, "RUNPOD_API_KEY is not set in environment variables"

# Base URL for OpenAI client
runpod_base_url = os.environ.get("RUNPOD_BASE_URL")
assert runpod_base_url, "RUNPOD_BASE_URL is not set in environment variables"

# Initialize OpenAI client
client = instructor.from_openai(
    OpenAI(api_key=runpod_api_key, base_url=runpod_base_url),
    mode=Mode.JSON,
)


data = [
    "Brandon is 33 years old. He works as a solution architect.",
    "Jason is 25 years old. He is the GOAT.",
    "Dominic is 45 years old. He is retired.",
    "Jenny is 72. She is a wife and a CEO.",
    "Holly is 22. She is an explorer.",
    "There onces was a prince, named Benny. He ruled for 10 years, which just ended. He started at 22.",
    "Simon says, why are you 22 years old marvin?",
]


if __name__ == "__main__":

    class UserDetail(BaseModel):
        name: str = Field(description="Name extracted from the text")
        age: int = Field(description="Age extracted from the text")
        occupation: Optional[str] = Field(
            default=None, description="Occupation extracted from the text"
        )

    for content in data:
        try:
            user = client.chat.completions.create(
                response_model=UserDetail,
                model="TheBloke_OpenHermes-2.5-Mistral-7B-GPTQ",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert at outputting json. You output valid JSON.",
                    },
                    {
                        "role": "user",
                        "content": f"Extract the user details from the following text: {content}. Match your response to the following schema: {UserDetail.model_json_schema()}",
                    },
                ],
            )
            print(f"Result: {user}")
        except Exception as e:
            print(f"Error: {e}")
            continue


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/parallel/run.py
=======
from __future__ import annotations

import openai
import instructor

from typing import Literal
from collections.abc import Iterable
from pydantic import BaseModel


class Weather(BaseModel):
    location: str
    units: Literal["imperial", "metric"]


class GoogleSearch(BaseModel):
    query: str


client = openai.OpenAI()

client = instructor.from_openai(client, mode=instructor.Mode.PARALLEL_TOOLS)

resp = client.chat.completions.create(
    model="gpt-4-turbo-preview",
    messages=[
        {"role": "system", "content": "You must always use tools"},
        {
            "role": "user",
            "content": "What is the weather in toronto and dallas and who won the super bowl?",
        },
    ],
    response_model=Iterable[Weather | GoogleSearch],
)

for r in resp:
    print(r)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/partial_streaming/benchmark.py
=======
# Part of this code is adapted from the following examples from OpenAI Cookbook:
# https://cookbook.openai.com/examples/how_to_stream_completions
# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
import time
import tiktoken
import instructor
from openai import OpenAI
from pydantic import BaseModel

client = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)


def num_tokens_from_string(string: str, model_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.encoding_for_model(model_name)

    num_tokens = len(encoding.encode(string))
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>

    return num_tokens


class User(BaseModel):
    name: str
    role: str
    age: int


def benchmark_raw_stream(model="gpt-4"):
    content = f"""Respond only in JSON that would validate to this schema and include nothing extra.
    Otherwise something bad will happen:\n {User.model_json_schema()}"""

    start_time = time.time()
    extraction_stream = client.chat.completions.create_fn(
        model=model,
        messages=[
            {"role": "system", "content": content},
            {
                "role": "user",
                "content": "give me a harry pottery character in json, name, role, age",
            },
        ],
        stream=True,
    )

    collected_messages = [chunk.choices[0].delta.content for chunk in extraction_stream]
    collected_messages = [m for m in collected_messages if m is not None]
    collected_messages = "".join(collected_messages)
    User.model_validate_json(collected_messages)
    end_time = time.time() - start_time

    output_tokens = num_tokens_from_string(collected_messages, model)
    char_per_sec = output_tokens / end_time
    return char_per_sec


def benchmark_partial_streaming(model="gpt-4"):
    start_time = time.time()
    extraction_stream = client.chat.completions.create_partial(
        model=model,
        response_model=User,
        messages=[
            {
                "role": "user",
                "content": "give me a harry pottery character in json, name, role, age",
            }
        ],
        stream=True,
    )

    for chunk in extraction_stream:  # noqa: B007
        pass
    end_time = time.time() - start_time

    output_tokens = num_tokens_from_string(chunk.model_dump_json(), model)
    char_per_sec = output_tokens / end_time
    return char_per_sec


if __name__ == "__main__":
    partial_times = [
        benchmark_partial_streaming(model="gpt-3.5-turbo-1106") for _ in range(10)
    ]
    avg_partial_time = sum(partial_times) / len(partial_times)

    raw_times = [benchmark_raw_stream(model="gpt-3.5-turbo") for _ in range(10)]
    avg_raw_time = sum(raw_times) / len(raw_times)
    print(f"Raw streaming: {avg_raw_time:.2f} tokens/sec")

    print(f"Partial streaming: {avg_partial_time:.2f} token/sec")
    print(f"Overhead: {avg_partial_time / avg_raw_time:.2f}x")

    """OLD IMPLEMENTATION
    Raw streaming: 35.73 tokens/sec
    Partial streaming: 24.42 token/sec
    Overhead: 0.68x
    """

    """NEW IMPLEMENTATION
    Raw streaming: 35.77 tokens/sec
    Partial streaming: 31.58 token/sec
    Overhead: 0.88x
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/partial_streaming/run.py
=======
# Part of this code is adapted from the following examples from OpenAI Cookbook:
# https://cookbook.openai.com/examples/how_to_stream_completions
# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
import instructor
from openai import OpenAI
from pydantic import BaseModel

client = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)


class User(BaseModel):
    name: str
    role: str


extraction_stream = client.chat.completions.create_partial(
    model="gpt-4",
    response_model=User,
    messages=[
        {
            "role": "user",
            "content": "give me a harry pottery character in json, name, role, age",
        }
    ],
)

for chunk in extraction_stream:
    print(chunk)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/patching/anyscale.py
=======
import os
import instructor

from openai import OpenAI
from pydantic import BaseModel


# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.acreate methods. to support response_model parameter
client = instructor.from_openai(
    OpenAI(
        base_url="https://api.endpoints.anyscale.com/v1",
        api_key=os.environ["ANYSCALE_API_KEY"],
    ),
    mode=instructor.Mode.JSON_SCHEMA,
)


# Now, we can use the response_model parameter using only a base model
# rather than having to use the OpenAISchema class
class UserExtract(BaseModel):
    name: str
    age: int


user: UserExtract = client.chat.completions.create(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    response_model=UserExtract,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)  # type: ignore

print(user)
{
    "name": "Jason",
    "age": 25,
}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/patching/oai.py
=======
import instructor

from openai import OpenAI
from pydantic import BaseModel


# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.acreate methods. to support response_model parameter
client = instructor.from_openai(
    OpenAI(),
    mode=instructor.Mode.TOOLS,
)


# Now, we can use the response_model parameter using only a base model
# rather than having to use the OpenAISchema class
class UserExtract(BaseModel):
    name: str
    age: int


user: UserExtract = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=UserExtract,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)  # type: ignore

print(user)
{
    "name": "Jason",
    "age": 25,
}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/patching/pcalls.py
=======
from typing import Literal, Union
from collections.abc import Iterable
from pydantic import BaseModel
from instructor import OpenAISchema

import time
import openai
import instructor


client = openai.OpenAI()


class Weather(OpenAISchema):
    location: str
    units: Literal["imperial", "metric"]


class GoogleSearch(OpenAISchema):
    query: str


if __name__ == "__main__":

    class Query(BaseModel):
        query: list[Union[Weather, GoogleSearch]]

    client = instructor.from_openai(client, mode=instructor.Mode.PARALLEL_TOOLS)

    start = time.perf_counter()
    resp = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas and who won the super bowl?",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    print(f"# Time: {time.perf_counter() - start:.2f}")

    print("# Instructor: Question with Toronto and Super Bowl")
    print([model for model in resp])

    start = time.perf_counter()
    resp = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas?",
            },
        ],
        tools=[
            {"type": "function", "function": Weather.openai_schema},
            {"type": "function", "function": GoogleSearch.openai_schema},
        ],
        tool_choice="auto",
    )
    print(f"# Time: {time.perf_counter() - start:.2f}")

    print("# Question with Toronto and Dallas")
    for tool_call in resp.choices[0].message.tool_calls:
        print(tool_call.model_dump_json(indent=2))

    start = time.perf_counter()
    resp = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {
                "role": "user",
                "content": "What is the weather in toronto? and who won the super bowl?",
            },
        ],
        tools=[
            {"type": "function", "function": Weather.openai_schema},
            {"type": "function", "function": GoogleSearch.openai_schema},
        ],
        tool_choice="auto",
    )
    print(f"# Time: {time.perf_counter() - start:.2f}")

    print("# Question with Toronto and Super Bowl")
    for tool_call in resp.choices[0].message.tool_calls:
        print(tool_call.model_dump_json(indent=2))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/patching/together.py
=======
import os
import openai
from pydantic import BaseModel
import instructor

client = openai.OpenAI(
    base_url="https://api.together.xyz/v1",
    api_key=os.environ["TOGETHER_API_KEY"],
)


# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.acreate methods. to support response_model parameter
client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)


# Now, we can use the response_model parameter using only a base model
# rather than having to use the OpenAISchema class
class UserExtract(BaseModel):
    name: str
    age: int


user: UserExtract = client.chat.completions.create(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    response_model=UserExtract,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)  # type: ignore

print(user.model_dump_json(indent=2))
{
    "name": "Jason",
    "age": 25,
}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/proscons/run.py
=======
from openai import OpenAI
from pydantic import BaseModel, Field

import instructor


class Character(BaseModel):
    name: str
    age: int
    fact: list[str] = Field(..., description="A list of facts about the character")


# enables `response_model` in create call
client = instructor.from_openai(
    OpenAI(
        base_url="http://localhost:11434/v1",
        api_key="ollama",  # required, but unused
    ),
    mode=instructor.Mode.JSON,
)

resp = client.chat.completions.create(
    model="llama2",
    messages=[
        {
            "role": "user",
            "content": "Tell me about the Harry Potter",
        }
    ],
    response_model=Character,
)
print(resp.model_dump_json(indent=2))
""" 
{
  "name": "Harry James Potter",
  "age": 37,
  "fact": [
    "He is the chosen one.",
    "He has a lightning-shaped scar on his forehead.",
    "He is the son of James and Lily Potter.",
    "He attended Hogwarts School of Witchcraft and Wizardry.",
    "He is a skilled wizard and sorcerer.",
    "He fought against Lord Voldemort and his followers.",
    "He has a pet owl named Snowy."
  ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/query_planner_execution/diagram.py
=======
from erdantic import erd

from query_planner_execution import QueryPlan

diagram = erd.create(QueryPlan)
diagram.draw("examples/query_planner_execution/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/query_planner_execution/query_planner_execution.py
=======
import asyncio
import enum
import instructor

from openai import OpenAI
from pydantic import Field, BaseModel

client = instructor.from_openai(OpenAI())


class QueryType(str, enum.Enum):
    """
    Enumeration representing the types of queries that can be asked to a question answer system.
    """

    # When i call it anything beyond 'merge multiple responses' the accuracy drops significantly.
    SINGLE_QUESTION = "SINGLE"
    MERGE_MULTIPLE_RESPONSES = "MERGE_MULTIPLE_RESPONSES"


class ComputeQuery(BaseModel):
    """
    Models a computation of a query, assume this can be some RAG system like llamaindex
    """

    query: str
    response: str = "..."


class MergedResponses(BaseModel):
    """
    Models a merged response of multiple queries.
    Currently we just concatinate them but we can do much more complex things.
    """

    responses: list[ComputeQuery]


class Query(BaseModel):
    """
    Class representing a single question in a question answer subquery.
    Can be either a single question or a multi question merge.
    """

    id: int = Field(..., description="Unique id of the query")
    question: str = Field(
        ...,
        description="Question we are asking using a question answer system, if we are asking multiple questions, this question is asked by also providing the answers to the sub questions",
    )
    dependancies: list[int] = Field(
        default_factory=list,
        description="List of sub questions that need to be answered before we can ask the question. Use a subquery when anything may be unknown, and we need to ask multiple questions to get the answer. Dependences must only be other queries.",
    )
    node_type: QueryType = Field(
        default=QueryType.SINGLE_QUESTION,
        description="Type of question we are asking, either a single question or a multi question merge when there are multiple questions",
    )

    async def execute(self, dependency_func):
        print("Executing", "`self.question`")
        print("Executing with", len(self.dependancies), "dependancies")

        if self.node_type == QueryType.SINGLE_QUESTION:
            resp = ComputeQuery(
                query=self.question,
            )
            await asyncio.sleep(1)
            pprint(resp.model_dump())
            return resp

        sub_queries = dependency_func(self.dependancies)
        computed_queries = await asyncio.gather(
            *[q.execute(dependency_func=dependency_func) for q in sub_queries]
        )
        sub_answers = MergedResponses(responses=computed_queries)
        merged_query = f"{self.question}\nContext: {sub_answers.model_dump_json()}"
        resp = ComputeQuery(
            query=merged_query,
        )
        await asyncio.sleep(2)
        pprint(resp.model_dump())
        return resp


class QueryPlan(BaseModel):
    """
    Container class representing a tree of questions to ask a question answer system.
    and its dependencies. Make sure every question is in the tree, and every question is asked only once.
    """

    query_graph: list[Query] = Field(
        ..., description="The original question we are asking"
    )

    async def execute(self):
        # this should be done with a topological sort, but this is easier to understand
        original_question = self.query_graph[-1]
        print(f"Executing query plan from `{original_question.question}`")
        return await original_question.execute(dependency_func=self.dependencies)

    def dependencies(self, idz: list[int]) -> list[Query]:
        """
        Returns the dependencies of the query with the given id.
        """
        return [q for q in self.query_graph if q.id in idz]


Query.model_rebuild()
QueryPlan.model_rebuild()


def query_planner(question: str, plan=False) -> QueryPlan:
    PLANNING_MODEL = "gpt-4"
    ANSWERING_MODEL = "gpt-3.5-turbo-0613"

    messages = [
        {
            "role": "system",
            "content": "You are a world class query planning algorithm capable of breaking apart questions into its depenencies queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step by step to get a better understanding the problem.",
        },
        {
            "role": "user",
            "content": f"Consider: {question}\nGenerate the correct query plan.",
        },
    ]

    if plan:
        messages.append(
            {
                "role": "assistant",
                "content": "Lets think step by step to find correct set of queries and its dependencies and not make any assuptions on what is known.",
            },
        )
        completion = client.chat.completions.create(
            model=PLANNING_MODEL, temperature=0, messages=messages, max_tokens=1000
        )

        messages.append(completion["choices"][0]["message"])

        messages.append(
            {
                "role": "user",
                "content": "Using that information produce the complete and correct query plan.",
            }
        )

    completion = client.chat.completions.create(
        model=ANSWERING_MODEL,
        temperature=0,
        functions=[QueryPlan.openai_schema],
        function_call={"name": QueryPlan.openai_schema["name"]},
        messages=messages,
        max_tokens=1000,
    )
    root = QueryPlan.from_response(completion)
    return root


if __name__ == "__main__":
    from pprint import pprint

    plan = query_planner(
        "What is the difference in populations of Canada and the Jason's home country?",
        plan=False,
    )
    pprint(plan.dict())
    """
    {'query_graph': [{'dependancies': [],
                    'id': 1,
                    'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>,
                    'question': "Identify Jason's home country"},
                    {'dependancies': [],
                    'id': 2,
                    'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>,
                    'question': 'Find the population of Canada'},
                    {'dependancies': [1],
                    'id': 3,
                    'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>,
                    'question': "Find the population of Jason's home country"},
                    {'dependancies': [2, 3],
                    'id': 4,
                    'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>,
                    'question': 'Calculate the difference in populations between '
                                "Canada and Jason's home country"}]}    
    """

    asyncio.run(plan.execute())
    """
    Executing query plan from `What is the difference in populations of Canada and Jason's home country?`
    Executing `What is the difference in populations of Canada and Jason's home country?`
    Executing with 2 dependancies
    Executing `What is the population of Canada?`
    Executing `What is the population of Jason's home country?`
    {'query': 'What is the population of Canada?', 'response': '...'}
    {'query': "What is the population of Jason's home country?", 'response': '...'}
    {'query': "What is the difference in populations of Canada and Jason's home "
            'country?'
            'Context: {"responses": [{"query": "What is the population of '
            'Canada?", "response": "..."}, {"query": "What is the population of '
            'Jason's home country?", "response": "..."}]}',
    'response': '...'}
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/recursive_filepaths/diagram.py
=======
import erdantic as erd

from parse_recursive_paths import DirectoryTree

diagram = erd.create(DirectoryTree)
diagram.draw("examples/parse_recursive_paths/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/recursive_filepaths/parse_recursive_paths.py
=======
import enum
import instructor

from openai import OpenAI
from pydantic import BaseModel, Field


client = instructor.from_openai(OpenAI())


class NodeType(str, enum.Enum):
    """Enumeration representing the types of nodes in a filesystem."""

    FILE = "file"
    FOLDER = "folder"


class Node(BaseModel):
    """
    Class representing a single node in a filesystem. Can be either a file or a folder.
    Note that a file cannot have children, but a folder can.

    Args:
        name (str): The name of the node.
        children (List[Node]): The list of child nodes (if any).
        node_type (NodeType): The type of the node, either a file or a folder.

    Methods:
        print_paths: Prints the path of the node and its children.
    """

    name: str = Field(..., description="Name of the folder")
    children: list["Node"] = Field(
        default_factory=list,
        description="List of children nodes, only applicable for folders, files cannot have children",
    )
    node_type: NodeType = Field(
        default=NodeType.FILE,
        description="Either a file or folder, use the name to determine which it could be",
    )

    def print_paths(self, parent_path=""):
        """Prints the path of the node and its children."""

        if self.node_type == NodeType.FOLDER:
            path = f"{parent_path}/{self.name}" if parent_path != "" else self.name

            print(path, self.node_type)

            if self.children is not None:
                for child in self.children:
                    child.print_paths(path)
        else:
            print(f"{parent_path}/{self.name}", self.node_type)


class DirectoryTree(BaseModel):
    """
    Container class representing a directory tree.

    Args:
        root (Node): The root node of the tree.

    Methods:
        print_paths: Prints the paths of the root node and its children.
    """

    root: Node = Field(..., description="Root folder of the directory tree")

    def print_paths(self):
        """Prints the paths of the root node and its children."""

        self.root.print_paths()


Node.model_rebuild()
DirectoryTree.model_rebuild()


def parse_tree_to_filesystem(data: str) -> DirectoryTree:
    """
    Convert a string representing a directory tree into a filesystem structure
    using OpenAI's GPT-3 model.

    Args:
        data (str): The string to convert into a filesystem.

    Returns:
        DirectoryTree: The directory tree representing the filesystem.
    """

    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=DirectoryTree,
        messages=[
            {
                "role": "system",
                "content": "You are a perfect file system parsing algorithm. You are given a string representing a directory tree. You must return the correct filesystem structure.",
            },
            {
                "role": "user",
                "content": f"Consider the data below:\n{data} and return the correctly labeled filesystem",
            },
        ],
        max_tokens=1000,
    )
    root = DirectoryTree.from_response(completion)
    return root


if __name__ == "__main__":
    root = parse_tree_to_filesystem(
        """
        root
         folder1
            file1.txt
            file2.txt
         folder2
             file3.txt
             subfolder1
                 file4.txt
        """
    )
    root.print_paths()
    # >>> root                                  NodeType.FOLDER
    # >>> root/folder1                          NodeType.FOLDER
    # >>> root/folder1/file1.txt                NodeType.FILE
    # >>> root/folder1/file2.txt                NodeType.FILE
    # >>> root/folder2                          NodeType.FOLDER
    # >>> root/folder2/file3.txt                NodeType.FILE
    # >>> root/folder2/subfolder1               NodeType.FOLDER
    # >>> root/folder2/subfolder1/file4.txt     NodeType.FILE


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/resolving-complex-entities/run.py
=======
from graphviz import Digraph
from pydantic import BaseModel, Field

import instructor
from openai import OpenAI

client = OpenAI()

# Patch openai to use instructor
# allows for response_model
instructor.from_openai()


class Property(BaseModel):
    key: str
    value: str
    resolved_absolute_value: str


class Entity(BaseModel):
    id: int = Field(
        ...,
        description="Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities",
    )
    subquote_string: list[str] = Field(
        ...,
        description="Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution",
    )
    entity_title: str
    properties: list[Property] = Field(
        ..., description="List of properties of the entity"
    )
    dependencies: list[int] = Field(
        ...,
        description="List of entity ids that this entity depends  or relies on to resolve it",
    )


class DocumentExtraction(BaseModel):
    entities: list[Entity] = Field(
        ...,
        description="Body of the answer, each fact should be its seperate object with a body and a list of sources",
    )


def ask_ai(content) -> DocumentExtraction:
    resp: DocumentExtraction = client.chat.completions.create(
        model="gpt-4",
        response_model=DocumentExtraction,
        messages=[
            {
                "role": "system",
                "content": "You are a perfect entity resolution system that extracts facts from the document. Extract and resolve a list of entities from the following document:",
            },
            {
                "role": "user",
                "content": content,
            },
        ],
    )  # type: ignore
    return resp


def generate_html_label(entity: Entity) -> str:
    rows = [
        f"<tr><td>{prop.key}</td><td>{prop.resolved_absolute_value}</td></tr>"
        for prop in entity.properties
    ]
    table_rows = "".join(rows)
    return f"""<
    <table border="0" cellborder="1" cellspacing="0">
    <tr><td colspan="2"><b>{entity.entity_title}</b></td></tr>
    {table_rows}
    </table>>"""


def generate_graph(data: DocumentExtraction):
    dot = Digraph(comment="Entity Graph", node_attr={"shape": "plaintext"})

    # Add nodes
    for entity in data.entities:
        label = generate_html_label(entity)
        dot.node(str(entity.id), label)

    # Add edges
    for entity in data.entities:
        for dep_id in entity.dependencies:
            dot.edge(str(entity.id), str(dep_id))

    # Render graph
    dot.render("entity.gz", view=True)


content = """
Sample Legal Contract
Agreement Contract

This Agreement is made and entered into on 2020-01-01 by and between Company A ("the Client") and Company B ("the Service Provider").

Article 1: Scope of Work

The Service Provider will deliver the software product to the Client 30 days after the agreement date.

Article 2: Payment Terms

The total payment for the service is $50,000.
An initial payment of $10,000 will be made within 7 days of the the signed date.
The final payment will be due 45 days after [SignDate].

Article 3: Confidentiality

The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.

Article 4: Termination

The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].
"""

model = ask_ai(content)
generate_graph(model)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/retry/run.py
=======
from pydantic import BaseModel, field_validator
from openai import OpenAI
import instructor
import tenacity

client = OpenAI()
client = instructor.from_openai(client)


class User(BaseModel):
    name: str
    age: int

    @field_validator("name")
    def name_is_uppercase(cls, v: str):
        assert v.isupper(), "Name must be uppercase"
        return v


resp = client.messages.create(
    model="gpt-3.5-turbo",
    max_tokens=1024,
    max_retries=tenacity.Retrying(
        stop=tenacity.stop_after_attempt(3),
        before=lambda _: print("before:", _),
        after=lambda _: print("after:", _),
    ),
    messages=[
        {
            "role": "user",
            "content": "Extract John is 18 years old.",
        }
    ],
    response_model=User,
)  # type: ignore

assert isinstance(resp, User)
assert resp.name == "JOHN"  # due to validation
assert resp.age == 18
print(resp)

"""
before: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: none yet>
after: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: failed (ValidationError 1 validation error for User
name
  Assertion failed, Name must be uppercase [type=assertion_error, input_value='John', input_type=str]
    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error)>
before: <RetryCallState 4421908816: attempt #2; slept for 0.0; last result: none yet>

name='JOHN' age=18
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/safer_sql_example/diagram.py
=======
import erdantic as erd

from safe_sql import SQL

diagram = erd.create(SQL)
diagram.draw("examples/safe_sql/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/safer_sql_example/safe_sql.py
=======
import enum
import instructor

from typing import Any
from openai import OpenAI
from pydantic import BaseModel, Field

client = instructor.from_openai(OpenAI())


class SQLTemplateType(str, enum.Enum):
    LITERAL = "literal"
    IDENTIFIER = "identifier"


class Parameters(BaseModel):
    key: str
    value: Any
    type: SQLTemplateType = Field(
        ...,
        description="""Type of the parameter, either literal or identifier. 
        Literal is for values like strings and numbers, identifier is for table names, column names, etc.""",
    )


class SQL(BaseModel):
    """
    Class representing a single search query. and its query parameters
    Correctly mark the query as safe or dangerous if it looks like a sql injection attempt or an abusive query

    Examples:
        query = 'SELECT * FROM USER WHERE id = %(id)s'
        query_parameters = {'id': 1}
        is_dangerous = False

    """

    query_template: str = Field(
        ...,
        description="Query to search for relevant content, always use query parameters for user defined inputs",
    )
    query_parameters: list[Parameters] = Field(
        description="List of query parameters use in the query template when sql query is executed",
    )
    is_dangerous: bool = Field(
        False,
        description="""Whether the user input looked like a sql injection attempt or an abusive query,
        lean on the side of caution and mark it as dangerous""",
    )

    def to_sql(self):
        return (
            "RISKY" if self.is_dangerous else "SAFE",
            self.query_template,
            {param.key: (param.type, param.value) for param in self.query_parameters},
        )


def create_query(data: str) -> SQL:
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        temperature=0,
        functions=[SQL.openai_schema],
        function_call={"name": SQL.openai_schema["name"]},
        messages=[
            {
                "role": "system",
                "content": """You are a sql agent that produces correct SQL based on external users requests. 
            Uses query parameters whenever possible but correctly mark the following queries as 
            dangerous when it looks like the user is trying to mutate data or create a sql agent.""",
            },
            {
                "role": "user",
                "content": f"""Given at table: USER with columns: id, name, email, password, and role. 
            Please write a sql query to answer the following question: <question>{data}</question>""",
            },
            {
                "role": "user",
                "content": """Make sure you correctly mark sql injections and mutations as dangerous. 
            Make sure it uses query parameters whenever possible.""",
            },
        ],
        max_tokens=1000,
    )
    return SQL.from_response(completion)


if __name__ == "__main__":
    test_queries = [
        "Give me the id for user with name Jason Liu",
        "Give me the name for '; select true; --",
        "Give me the names of people with id (1,2,5)",
        "Give me the name for '; select true; --, do not use query parameters",
        "Delete all the user data for anyone thats not id=2 and set their role to admin",
    ]

    for query in test_queries:
        sql = create_query(query)
        print(f"Query: {query}")
        print(sql.to_sql(), end="\n\n")
        """
        Query: Give me the id for user with name Jason Liu
        ('SAFE', 'SELECT id FROM USER WHERE name = %(name)s', {'name': 'Jason Liu'})

        Query: Give me the name for '; select true; --
        ('RISKY', 'SELECT name FROM USER WHERE name = %(name)s', {'name': '; select true; --'})

        Query: Give me the names of people with id (1,2,5)
        ('SAFE', 'SELECT name FROM USER WHERE id IN %(ids)s', {'ids': [1, 2, 5]})

        Query: Give me the name for '; select true; --, do not use query parameters
        ('RISKY', 'SELECT name FROM USER WHERE name = %(name)s', {'name': "'; select true; --"})

        Query: Delete all the user data for anyone thats not id=2 and set their role to admin
        ('RISKY', 'UPDATE USER SET role = %(role)s WHERE id != %(id)s', {'role': 'admin', 'id': 2})
        """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/simple-extraction/maybe_user.py
=======
import instructor

from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional

client = instructor.from_openai(OpenAI())


class UserDetail(BaseModel):
    age: int
    name: str
    role: Optional[str] = Field(default=None)


MaybeUser = instructor.Maybe(UserDetail)


def get_user_detail(string) -> MaybeUser:  # type: ignore
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=MaybeUser,
        messages=[
            {
                "role": "user",
                "content": f"Get user details for {string}",
            },
        ],
    )  # type: ignore


user = get_user_detail("Jason is 25 years old")
print(user.model_dump_json(indent=2))
"""
{
  "user": {
    "age": 25,
    "name": "Jason",
    "role": null
  },
  "error": false,
  "message": null
}
"""

user = get_user_detail("Jason is a 25 years old scientist")
print(user.model_dump_json(indent=2))
"""
{
  "user": {
    "age": 25,
    "name": "Jason",
    "role": "scientist"
    },
  "error": false,
  "message": null
}
"""

# ! notice that the string should not contain anything
# ! but a user and age was still extracted ?!
user = get_user_detail("User not found")
print(user.model_dump_json(indent=2))
"""
{
  "user": null,
  "error": true,
  "message": "User not found"
}
"""

# ! due to the __bool__ method, you can use the MaybeUser object as a boolean

if not user:
    print("Detected error")
"""
Detected error
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/simple-extraction/user.py
=======
import instructor

from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional

client = instructor.from_openai(OpenAI())


class UserDetail(BaseModel):
    age: int
    name: str
    role: Optional[str] = Field(default=None)


def get_user_detail(string) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-3.5-turbo-0613",
        response_model=UserDetail,
        messages=[
            {
                "role": "user",
                "content": f"Get user details for {string}",
            },
        ],
    )  # type: ignore


user = get_user_detail("Jason is 25 years old")
print(user.model_dump_json(indent=2))
"""
{
  "age": 25,
  "name": "Jason",
  "role": null
}
"""

user = get_user_detail("Jason is a 25 years old scientist")
print(user.model_dump_json(indent=2))
"""
{
  "age": 25,
  "name": "Jason",
  "role": "scientist"
}
"""

# ! notice that the string should not contain anything
# ! but a user and age was still extracted ?!
user = get_user_detail("User not found")
print(user.model_dump_json(indent=2))
"""
{
  "age": 25,
  "name": "John Doe",
  "role": "null"
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/sqlmodel/run.py
=======
import instructor
from openai import OpenAI
from typing import Optional
from sqlmodel import Field, SQLModel, create_engine, Session


# Define the model that will serve as a Table for the database
class Hero(SQLModel, instructor.OpenAISchema, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    secret_name: str
    age: Optional[int] = None


# Function to query OpenAI for a Hero record
client = instructor.from_openai(OpenAI())


def create_hero() -> Hero:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Hero,
        messages=[
            {"role": "user", "content": "Make a new superhero"},
        ],
    )


# Insert the response into the database
engine = create_engine("sqlite:///database.db")
SQLModel.metadata.create_all(engine)

hero = create_hero()
print(hero.model_dump())


with Session(engine) as session:
    session.add(hero)
    session.commit()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/stream_action_items/run.py
=======
import instructor

from pydantic import BaseModel, Field
from typing import Optional
from collections.abc import Iterable
from openai import OpenAI
from rich.console import Console


client = instructor.from_openai(OpenAI())


class ActionItem(BaseModel):
    slug: str = Field(..., description="compact short slug")
    title: str = Field(description="The title of the action item")
    chain_of_thought: str = Field(
        description="Short chain of thought that led to this action item, specifically think about whether or not a task should be marked as completed"
    )
    is_completed: Optional[bool] = Field(
        False, description="Whether the action item is completed"
    )


class ActionItemResponse(BaseModel):
    action_items: Optional[list[ActionItem]] = Field(
        ..., title="The list of action items"
    )

    def patch(self, action_item: ActionItem):
        current_items = {item.slug: item for item in self.action_items}
        current_items[action_item.slug] = action_item
        new_response = ActionItemResponse(action_items=list(current_items.values()))
        print(f"BEFORE\n{self}\n\nAFTER\n{new_response}")
        return new_response

    def __repr__(self):
        completed_str = "DONE -"
        pending_str = "TODO -"

        def format_item(item):
            return f"{completed_str if item.is_completed else pending_str} {item.title}"

        return "\n\n".join([format_item(item) for item in self.action_items])

    def __str__(self) -> str:
        return self.__repr__()


console = Console()


def yield_action_items(transcript: str, state: ActionItemResponse):
    action_items = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        temperature=0,
        seed=42,
        response_model=Iterable[ActionItem],
        stream=True,
        messages=[
            {
                "role": "system",
                "content": f"""
                You're a world-class note taker. 
                You are given the current state of the notes and an additional piece of the transcript. 
                Use this to update the action.
                
                If you return an action item with the same ID as something in the set, It will be overwritten.
                Use this to update the complete status or change the title if there's more context. 

                - If they are distinct items, do not repeat the slug.
                - Only repeat a slug if we need to update the title or completion status.
                - If the completion status is not mentioned, it should be assumed to be incomplete.
                - For each task describe the success / completion criteria as well.
                - If something is explicitly mentioned as being done, mark it as done. 

                {state.model_dump_json(indent=2)}
                """,
            },
            {
                "role": "user",
                "content": f"Take the following transcript to return a set of transactions from the transcript\n\n{transcript}",
            },
        ],
    )

    for action_item in action_items:
        state = state.patch(action_item)
        yield state


transcript = """
Bob: Great, Carol. I'll handle the back-end optimization then.

Alice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.

Bob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.

Jason: The backend optimization was finished last week actually.

Alice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.
""".strip().split("\n\n")


def text_to_speech(chunk):
    """
    Uses a subprocess to convert text to speech via the `say` command on macOS.
    """
    import subprocess

    subprocess.run(["say", chunk], check=True)


def process_transcript(transcript: list[str]):
    state = ActionItemResponse(action_items=[])
    for chunk in transcript:
        console.print(f"update: {chunk}")
        for new_state in yield_action_items(chunk, state):
            state = new_state
            console.clear()
            console.print("# Action Items")
            console.print(str(state))
            console.print("\n")


if __name__ == "__main__":
    process_transcript(transcript)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/synethic-data/run.py
=======
import openai
import instructor
from collections.abc import Iterable
from pydantic import BaseModel, ConfigDict

client = instructor.from_openai(openai.OpenAI())


class SyntheticQA(BaseModel):
    question: str
    answer: str

    model_config = ConfigDict(
        json_schema_extra={
            "examples": [
                {"question": "What is the capital of France?", "answer": "Paris"},
                {
                    "question": "What is the largest planet in our solar system?",
                    "answer": "Jupiter",
                },
                {
                    "question": "Who wrote 'To Kill a Mockingbird'?",
                    "answer": "Harper Lee",
                },
                {
                    "question": "What element does 'O' represent on the periodic table?",
                    "answer": "Oxygen",
                },
            ]
        }
    )


def get_synthetic_data() -> Iterable[SyntheticQA]:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Generate synthetic examples"},
            {
                "role": "user",
                "content": "Generate the exact examples you see in the examples of this prompt. ",
            },
        ],
        response_model=Iterable[SyntheticQA],
    )  # type: ignore


if __name__ == "__main__":
    for example in get_synthetic_data():
        print(example)
        """
        question='What is the capital of France?' answer='Paris'
        question='What is the largest planet in our solar system?' answer='Jupiter'
        question="Who wrote 'To Kill a Mockingbird'?" answer='Harper Lee'
        question="What element does 'O' represent on the periodic table?" answer='Oxygen'
        """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/task_planner/diagram.py
=======
import erdantic as erd

from task_planner_topological_sort import TaskPlan

diagram = erd.create(TaskPlan)
diagram.draw("examples/task_planner_topological_sort/schema.png")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/task_planner/task_planner_topological_sort.py
=======
"""
Proof of Concept for a task planning and execution system using
OpenAIs Functions and topological sort, based on the idea in
query_planner_execution.py.py.

Additionally: There are also cases where the "pure" recursive approach has advantages;
If subtasks for different parent tasks that start in parallel have different runtimes,
we will wait unnecessarily with my current implementation.

Added by Jan Philipp Harries / @jpdus
"""

import asyncio
from collections.abc import Generator

from openai import OpenAI

from pydantic import Field, BaseModel

import instructor

client = instructor.from_openai(OpenAI())


class TaskResult(BaseModel):
    task_id: int
    result: str


class TaskResults(BaseModel):
    results: list[TaskResult]


class Task(BaseModel):
    """
    Class representing a single task in a task plan.
    """

    id: int = Field(..., description="Unique id of the task")
    task: str = Field(
        ...,
        description="""Contains the task in text form. If there are multiple tasks,
        this task can only be executed when all dependant subtasks have been answered.""",
    )
    subtasks: list[int] = Field(
        default_factory=list,
        description="""List of the IDs of subtasks that need to be answered before
        we can answer the main question. Use a subtask when anything may be unknown
        and we need to ask multiple questions to get the answer.
        Dependencies must only be other tasks.""",
    )

    async def aexecute(self, with_results: TaskResults) -> TaskResult:
        """
        Executes the task by asking the question and returning the answer.
        """

        # We do nothing with the subtask answers, since this is an example however
        # we could use intermediate results to compute the answer to the main task.
        return TaskResult(task_id=self.id, result=f"`{self.task}`")


class TaskPlan(BaseModel):
    """
    Container class representing a tree of tasks and subtasks.
    Make sure every task is in the tree, and every task is done only once.
    """

    task_graph: list[Task] = Field(
        ...,
        description="List of tasks and subtasks that need to be done to complete the main task. Consists of the main task and its dependencies.",
    )

    def _get_execution_order(self) -> list[int]:
        """
        Returns the order in which the tasks should be executed using topological sort.
        Inspired by https://gitlab.com/ericvsmith/toposort/-/blob/master/src/toposort.py
        """
        tmp_dep_graph = {item.id: set(item.subtasks) for item in self.task_graph}

        def topological_sort(
            dep_graph: dict[int, set[int]],
        ) -> Generator[set[int], None, None]:
            while True:
                ordered = set(item for item, dep in dep_graph.items() if len(dep) == 0)
                if not ordered:
                    break
                yield ordered
                dep_graph = {
                    item: (dep - ordered)
                    for item, dep in dep_graph.items()
                    if item not in ordered
                }
            if len(dep_graph) != 0:
                raise ValueError(
                    f"Circular dependencies exist among these items: {{{', '.join(f'{key}:{value}' for key, value in dep_graph.items())}}}"
                )

        result = []
        for d in topological_sort(tmp_dep_graph):
            result.extend(sorted(d))
        return result

    async def execute(self) -> dict[int, TaskResult]:
        """
        Executes the tasks in the task plan in the correct order using asyncio and chunks with answered dependencies.
        """
        execution_order = self._get_execution_order()
        tasks = {q.id: q for q in self.task_graph}
        task_results = {}
        while True:
            ready_to_execute = [
                tasks[task_id]
                for task_id in execution_order
                if task_id not in task_results
                and all(
                    subtask_id in task_results for subtask_id in tasks[task_id].subtasks
                )
            ]
            # prints chunks to visualize execution order
            print(ready_to_execute)
            computed_answers = await asyncio.gather(
                *[
                    q.aexecute(
                        with_results=TaskResults(
                            results=[
                                result
                                for result in task_results.values()
                                if result.task_id in q.subtasks
                            ]
                        )
                    )
                    for q in ready_to_execute
                ]
            )
            for answer in computed_answers:
                task_results[answer.task_id] = answer
            if len(task_results) == len(execution_order):
                break
        return task_results


Task.model_rebuild()
TaskPlan.model_rebuild()


def task_planner(question: str) -> TaskPlan:
    messages = [
        {
            "role": "system",
            "content": "You are a world class task planning algorithm capable of breaking apart tasks into dependant subtasks, such that the answers can be used to enable the system completing the main task. Do not complete the user task, simply provide a correct compute graph with good specific tasks to ask and relevant subtasks. Before completing the list of tasks, think step by step to get a better understanding the problem.",
        },
        {
            "role": "user",
            "content": f"{question}",
        },
    ]

    completion = client.chat.completions.create(
        model="gpt-4-0613",
        temperature=0,
        response_model=TaskPlan,
        messages=messages,
        max_tokens=1000,
    )
    root = TaskPlan.from_response(completion)

    return root


if __name__ == "__main__":
    plan = task_planner(
        "What is the difference in populations betweend the adjacent countries of Jan's home country and the adjacent countries of Jason's home country?"
    )
    print(plan.model_dump_json(indent=2))
    {
        "task_graph": [
            {"id": 1, "subtasks": [], "task": "Identify Jan's home country"},
            {
                "id": 2,
                "subtasks": [1],
                "task": "Identify the adjacent countries of Jan's home " "country",
            },
            {
                "id": 3,
                "subtasks": [2],
                "task": "Calculate the total population of the adjacent "
                "countries of Jan's home country",
            },
            {"id": 4, "subtasks": [], "task": "Identify Jason's home country"},
            {
                "id": 5,
                "subtasks": [4],
                "task": "Identify the adjacent countries of Jason's home " "country",
            },
            {
                "id": 6,
                "subtasks": [5],
                "task": "Calculate the total population of the adjacent "
                "countries of Jason's home country",
            },
            {
                "id": 7,
                "subtasks": [3, 6],
                "task": "Calculate the difference in populations between the "
                "adjacent countries of Jan's home country and the "
                "adjacent countries of Jason's home country",
            },
        ]
    }


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/union/run.py
=======
from pydantic import BaseModel, Field
from typing import Union
import instructor
from openai import OpenAI


class Search(BaseModel):
    """Search action class with a 'query' field and a process method."""

    query: str = Field(description="The search query")

    def process(self):
        """Process the search action."""
        return f"Search method called for query: {self.query}"


class Lookup(BaseModel):
    """Lookup action class with a 'keyword' field and a process method."""

    keyword: str = Field(description="The lookup keyword")

    def process(self):
        """Process the lookup action."""
        return f"Lookup method called for keyword: {self.keyword}"


class Finish(BaseModel):
    """Finish action class with an 'answer' field and a process method."""

    answer: str = Field(description="The answer for finishing the process")

    def process(self):
        """Process the finish action."""
        return f"Finish method called with answer: {self.answer}"


# Union of Search, Lookup, and Finish
class TakeAction(BaseModel):
    action: Union[Search, Lookup, Finish]

    def process(self):
        """Process the action."""
        return self.action.process()


try:
    # Enables `response_model`
    client = instructor.from_openai(OpenAI())
    action = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=TakeAction,
        messages=[
            {"role": "user", "content": "Please choose one action"},
        ],
    )
    assert isinstance(action, TakeAction), "The action is not TakeAction"
    print(action.process())
except Exception as e:
    print(f"An error occurred: {e}")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validated-multiclass/run.py
=======
from pydantic import BaseModel, ValidationInfo, model_validator
import openai
import instructor
import asyncio

client = instructor.from_openai(
    openai.AsyncOpenAI(),
)


class Tag(BaseModel):
    id: int
    name: str

    @model_validator(mode="after")
    def validate_ids(self, info: ValidationInfo):
        context = info.context
        if context:
            tags: list[Tag] = context.get("tags")
            assert self.id in {
                tag.id for tag in tags
            }, f"Tag ID {self.id} not found in context"
            assert self.name in {
                tag.name for tag in tags
            }, f"Tag name {self.name} not found in context"
        return self


class TagWithInstructions(Tag):
    instructions: str


class TagRequest(BaseModel):
    texts: list[str]
    tags: list[TagWithInstructions]


class TagResponse(BaseModel):
    texts: list[str]
    predictions: list[Tag]


async def tag_single_request(text: str, tags: list[Tag]) -> Tag:
    allowed_tags = [(tag.id, tag.name) for tag in tags]
    allowed_tags_str = ", ".join([f"`{tag}`" for tag in allowed_tags])
    return await client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {
                "role": "system",
                "content": "You are a world-class text tagging system.",
            },
            {"role": "user", "content": f"Describe the following text: `{text}`"},
            {
                "role": "user",
                "content": f"Here are the allowed tags: {allowed_tags_str}",
            },
        ],
        response_model=Tag,
        # Minizises the hallucination of tags that are not in the allowed tags.
        validation_context={"tags": tags},
    )


async def tag_request(request: TagRequest) -> TagResponse:
    predictions = await asyncio.gather(
        *[tag_single_request(text, request.tags) for text in request.texts]
    )
    return TagResponse(
        texts=request.texts,
        predictions=predictions,
    )


if __name__ == "__main__":
    # Tags will be a range of different topics.
    # Such as personal, phone, email, etc.
    tags = [
        TagWithInstructions(id=0, name="personal", instructions="Personal information"),
        TagWithInstructions(id=1, name="phone", instructions="Phone number"),
        TagWithInstructions(id=2, name="email", instructions="Email address"),
        TagWithInstructions(id=3, name="address", instructions="Address"),
        TagWithInstructions(id=4, name="Other", instructions="Other information"),
    ]

    # Texts will be a range of different questions.
    # Such as "How much does it cost?", "What is your privacy policy?", etc.
    texts = [
        "What is your phone number?",
        "What is your email address?",
        "What is your address?",
        "What is your privacy policy?",
    ]

    # The request will contain the texts and the tags.
    request = TagRequest(texts=texts, tags=tags)

    # The response will contain the texts, the predicted tags, and the confidence.
    response = asyncio.run(tag_request(request))
    print(response.model_dump_json(indent=2))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/allm_validator.py
=======
import asyncio
from typing import Annotated
from pydantic import BaseModel, BeforeValidator
from instructor import llm_validator, patch
from openai import AsyncOpenAI

aclient = AsyncOpenAI()

patch()


class QuestionAnswerNoEvil(BaseModel):
    question: str
    answer: Annotated[
        str,
        BeforeValidator(
            llm_validator("don't say objectionable things", allow_override=True)
        ),
    ]


async def main():
    context = "The according to the devil is to live a life of sin and debauchery."
    question = "What is the meaning of life?"

    try:
        qa: QuestionAnswerNoEvil = await aclient.chat.completions.create(
            model="gpt-3.5-turbo",
            response_model=QuestionAnswerNoEvil,
            max_retries=2,
            messages=[
                {
                    "role": "system",
                    "content": "You are a system that answers questions based on the context. Answer exactly what the question asks using the context.",
                },
                {
                    "role": "user",
                    "content": f"using the context: {context}\n\nAnswer the following question: {question}",
                },
            ],
        )  # type: ignore
        print(qa)
    except Exception as e:
        print(e)


if __name__ == "__main__":
    asyncio.run(main())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/annotator.py
=======
from typing import Annotated
from pydantic import BaseModel, ValidationError
from pydantic.functional_validators import AfterValidator


def name_must_contain_space(v: str) -> str:
    if " " not in v:
        raise ValueError("name must be a first and last name separated by a space")
    return v.lower()


class UserDetail(BaseModel):
    age: int
    name: Annotated[str, AfterValidator(name_must_contain_space)]


# Example 1) Valid input, notice that the name is lowercased
person: UserDetail = UserDetail(age=29, name="Jason Liu")
print(person.model_dump_json(indent=2))
"""
{
    "age": 29,
    "name": "jason liu"
}
"""

# Example 2) Invalid input, we'll get a validation error
# In the future this validation error will be raised by the API and
# used by the LLM to generate a better response
try:
    person: UserDetail = UserDetail(age=29, name="Jason")
except ValidationError as e:
    print(e)
    """
    1 validation error for UserDetail
    name
        Value error, name must be a first and last name separated by a space [type=value_error, input_value='Jason', input_type=str]
        For further information visit https://errors.pydantic.dev/2.3/v/value_error
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/chain_of_thought_validator.py
=======
import instructor
from openai import OpenAI

from pydantic import BaseModel, Field, model_validator
from typing import Optional

# Enables `response_model` and `max_retries` parameters
client = instructor.from_openai(OpenAI())


class Validation(BaseModel):
    is_valid: bool = Field(
        ..., description="Whether the value is valid given the rules"
    )
    error_message: Optional[str] = Field(
        ...,
        description="The error message if the value is not valid, to be used for re-asking the model",
    )


def validator(values):
    chain_of_thought = values["chain_of_thought"]
    answer = values["answer"]
    resp = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "You are a validator. Determine if the value is valid for the statement. If it is not, explain why.",
            },
            {
                "role": "user",
                "content": f"Verify that `{answer}` follows the chain of thought: {chain_of_thought}",
            },
        ],
        # this comes from instructor.from_openai()
        response_model=Validation,
    )
    if not resp.is_valid:
        raise ValueError(resp.error_message)
    return values


class Response(BaseModel):
    chain_of_thought: str
    answer: str

    @model_validator(mode="before")
    @classmethod
    def chain_of_thought_makes_sense(cls, data):
        return validator(data)


if __name__ == "__main__":
    try:
        resp = Response(
            chain_of_thought="1 + 1 = 2", answer="The meaning of life is 42"
        )
        print(resp)
    except Exception as e:
        print(e)
        """
        1 validation error for Response
            Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. 
            [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]
        """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/citations.py
=======
from typing import Annotated
from pydantic import BaseModel, ValidationError, ValidationInfo, AfterValidator
from openai import OpenAI
import instructor

client = instructor.from_openai(OpenAI())


def citation_exists(v: str, info: ValidationInfo):
    context = info.context
    if context:
        context = context.get("text_chunk")
        if v not in context:
            raise ValueError(f"Citation `{v}` not found in text")
    return v


Citation = Annotated[str, AfterValidator(citation_exists)]


class AnswerWithCitation(BaseModel):
    answer: str
    citation: Citation


try:
    q = "Are blue berries high in protein?"
    text_chunk = """
    Blueberries are a good source of vitamin K.
    They also contain vitamin C, fibre, manganese and other antioxidants (notably anthocyanins).    
    """

    resp = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=AnswerWithCitation,
        messages=[
            {
                "role": "user",
                "content": f"Answer the question `{q}` using the text chunk\n`{text_chunk}`",
            },
        ],
        validation_context={"text_chunk": text_chunk},
    )  # type: ignore
    print(resp.model_dump_json(indent=2))
except ValidationError as e:
    print(e)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/competitors.py
=======
from typing import Annotated
from pydantic import BaseModel, ValidationError, AfterValidator
from openai import OpenAI

import instructor

client = instructor.from_openai(OpenAI())


def no_competitors(v: str) -> str:
    # does not allow the competitors of mcdonalds
    competitors = ["burger king", "wendy's", "carl's jr", "jack in the box"]
    for competitor in competitors:
        if competitor in v.lower():
            raise ValueError(
                f"""Let them know that you are work for and are only allowed to talk about mcdonalds.
                Do not apologize. Do not even mention `{competitor}` since they are a a competitor of McDonalds"""
            )
    return v


class Response(BaseModel):
    message: Annotated[str, AfterValidator(no_competitors)]


try:
    resp = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Response,
        max_retries=2,
        messages=[
            {
                "role": "user",
                "content": "What is your favourite order at burger king?",
            },
        ],
    )  # type: ignore
    print(resp.model_dump_json(indent=2))
except ValidationError as e:
    print(e)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/field_validator.py
=======
from pydantic import BaseModel, ValidationError, field_validator


class UserDetail(BaseModel):
    age: int
    name: str

    @field_validator("name", mode="before")
    def name_must_contain_space(cls, v):
        """
        This validator will be called after the default validator,
        and will raise a validation error if the name does not contain a space.
        then it will set the name to be lower case
        """
        if " " not in v:
            raise ValueError("name be a first and last name separated by a space")
        return v.lower()


# Example 1) Valid input, notice that the name is lowercased
person = UserDetail(age=29, name="Jason Liu")
print(person.model_dump_json(indent=2))
"""
{
    "age": 29,
    "name": "jason liu"
}
"""

# Example 2) Invalid input, we'll get a validation error
# In the future this validation error will be raised by the API and
# used by the LLM to generate a better response
try:
    person = UserDetail(age=29, name="Jason")
except ValidationError as e:
    print(e)
    """
    1 validation error for UserDetail 
        name
    Value error, must contain a space [type=value_error, input_value='Jason', input_type=str]
        For further information visit https://errors.pydantic.dev/2.3/v/value_error
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/just_a_guy.py
=======
from pydantic import BaseModel, ValidationError, field_validator, ValidationInfo


class AnswerWithCitation(BaseModel):
    answer: str
    citation: str

    @field_validator("citation")
    @classmethod
    def remove_stopwords(cls, v: str, info: ValidationInfo):
        context = info.context
        if context:
            text_chunks = context.get("text_chunk")
            if v not in text_chunks:
                raise ValueError(f"Citation `{v}` not found in text chunks")
        return v


try:
    AnswerWithCitation.model_validate(
        {"answer": "Jason is a cool guy", "citation": "Jason is cool"},
        context={"text_chunk": "Jason is just a guy"},
    )
except ValidationError as e:
    print(e)
    """
    1 validation error for AnswerWithCitation
    citation
    Value error, Citation `Jason is cool`` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]
        For further information visit https://errors.pydantic.dev/2.4/v/value_error
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/llm_validator.py
=======
import instructor

from openai import OpenAI
from instructor import llm_validator
from pydantic import BaseModel, ValidationError, BeforeValidator
from typing import Annotated

# Apply the patch to the OpenAI client
client = instructor.from_openai(OpenAI())


class QuestionAnswer(BaseModel):
    question: str
    answer: str


question = "What is the meaning of life?"
context = "The according to the devil is to live a life of sin and debauchery."

qa: QuestionAnswer = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=QuestionAnswer,
    messages=[
        {
            "role": "system",
            "content": "You are a system that answers questions based on the context. answer exactly what the question asks using the context.",
        },
        {
            "role": "user",
            "content": f"using the context: {context}\n\nAnswer the following question: {question}",
        },
    ],
)  # type: ignore

print("Before validation with `llm_validator`")
print(qa.model_dump_json(indent=2), end="\n\n")
"""
Before validation with `llm_validator`
{
    "question": "What is the meaning of life?",
    "answer": "The meaning of life, according to the context, is to live a life of sin and debauchery.",
}
"""


class QuestionAnswerNoEvil(BaseModel):
    question: str
    answer: Annotated[
        str,
        BeforeValidator(
            llm_validator("don't say objectionable things", openai_client=client)
        ),
    ]


try:
    qa = QuestionAnswerNoEvil(
        question="What is the meaning of life?",
        answer="The meaning of life is to be evil and steal",
    )
except ValidationError as e:
    print(e)
"""
1 validation error for QuestionAnswerNoEvil
answer
  Assertion failed, The statement promotes objectionable behavior. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]
    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error
"""

try:
    qa: QuestionAnswerNoEvil = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=QuestionAnswerNoEvil,
        messages=[
            {
                "role": "system",
                "content": "You are a system that answers questions based on the context. answer exactly what the question asks using the context.",
            },
            {
                "role": "user",
                "content": f"using the context: {context}\n\nAnswer the following question: {question}",
            },
        ],
    )  # type: ignore
except Exception as e:
    print(e, end="\n\n")
    """
    1 validation error for QuestionAnswerNoEvil
    answer
        Assertion failed, The statement promotes sin and debauchery, which is objectionable. [type=assertion_error, input_value='The meaning of life is t... of sin and debauchery.', input_type=str]
        For further information visit https://errors.pydantic.dev/2.3/v/assertion_error
    """

qa: QuestionAnswerNoEvil = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=QuestionAnswerNoEvil,
    max_retries=2,
    messages=[
        {
            "role": "system",
            "content": "You are a system that answers questions based on the context. answer exactly what the question asks using the context.",
        },
        {
            "role": "user",
            "content": f"using the context: {context}\n\nAnswer the following question: {question}",
        },
    ],
)  # type: ignore

print("After validation with `llm_validator` with `max_retries=2`")
print(qa.model_dump_json(indent=2), end="\n\n")
"""
After validation with `llm_validator` with `max_retries=2`
{
  "question": "What is the meaning of life?",
  "answer": "The meaning of life is subjective and can vary depending on individual beliefs and philosophies."
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/validators/moderation.py
=======
import instructor

from instructor import openai_moderation

from typing import Annotated
from pydantic import BaseModel, AfterValidator
from openai import OpenAI

client = instructor.from_openai(OpenAI())


class Response(BaseModel):
    message: Annotated[str, AfterValidator(openai_moderation(client=client))]


response = Response(message="I want to make them suffer the consequences")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/vision/image_to_ad_copy.py
=======
import json
import logging
import os
import sys
from typing import Optional

from dotenv import find_dotenv, load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field
from rich import print as rprint

import instructor

load_dotenv(find_dotenv())

# Add logger
logging.basicConfig()
logger = logging.getLogger("app")
logger.setLevel("INFO")


# Define models
class Product(BaseModel):
    """
    Represents a product extracted from an image using AI.

    The product attributes are dynamically determined based on the content
    of the image and the AI's interpretation. This class serves as a structured
    representation of the identified product characteristics.
    """

    name: str = Field(
        description="A generic name for the product.", example="Headphones"
    )
    key_features: Optional[list[str]] = Field(
        description="A list of key features of the product that stand out.",
        example=["Wireless", "Noise Cancellation"],
        default=None,
    )

    description: Optional[str] = Field(
        description="A description of the product.",
        example="Wireless headphones with noise cancellation.",
        default=None,
    )

    def generate_prompt(self):
        prompt = f"Product: {self.name}\n"
        if self.description:
            prompt += f"Description: {self.description}\n"
        if self.key_features:
            prompt += f"Key Features: {', '.join(self.key_features)}\n"
        return prompt


class IdentifiedProduct(BaseModel):
    """
    Represents a list of products identified in the images.
    """

    products: Optional[list[Product]] = Field(
        description="A list of products identified by the AI.",
        example=[
            Product(
                name="Headphones",
                description="Wireless headphones with noise cancellation.",
                key_features=["Wireless", "Noise Cancellation"],
            )
        ],
        default=None,
    )

    error: bool = Field(default=False)
    message: Optional[str] = Field(default=None)

    def __bool__(self):
        return self.products is not None and len(self.products) > 0


class AdCopy(BaseModel):
    """
    Represents a generated ad copy.
    """

    headline: str = Field(
        description="A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.",
        example="Wireless Headphones",
    )
    ad_copy: str = Field(
        description="A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.",
        example="""
        "Experience the ultimate sound quality with our wireless headphones, featuring high-definition audio, noise-cancellation, and a comfortable, ergonomic design for all-day listening."
        """,
    )
    name: str = Field(
        description="The name of the product being advertised.",
        example="Headphones",
    )


# Define clients
client_image = instructor.from_openai(
    OpenAI(api_key=os.getenv("OPENAI_API_KEY")), mode=instructor.Mode.MD_JSON
)
client_copy = instructor.from_openai(
    OpenAI(api_key=os.getenv("OPENAI_API_KEY")), mode=instructor.Mode.FUNCTIONS
)


# Define functions
def read_images(image_urls: list[str]) -> IdentifiedProduct:
    """
    Given a list of image URLs, identify the products in the images.
    """

    logger.info(f"Identifying products in images... {len(image_urls)} images")

    return client_image.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=IdentifiedProduct,
        max_tokens=1024,
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Identify products using the given images and generate key features for each product.",
                    },
                    *[
                        {"type": "image_url", "image_url": {"url": url}}
                        for url in image_urls
                    ],
                ],
            }
        ],
    )


def generate_ad_copy(product: Product) -> AdCopy:
    """
    Given a product, generate an ad copy for the product.
    """

    logger.info(f"Generating ad copy for product: {product.name}")

    return client_copy.chat.completions.create(
        model="gpt-4-1106-preview",
        response_model=AdCopy,
        temperature=0.3,
        messages=[
            {
                "role": "system",
                "content": "You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.",
            },
            {"role": "user", "content": product.generate_prompt()},
        ],
    )


def run(images: list[str]) -> tuple[list[Product], list[AdCopy]]:
    """
    Given a list of images, identify the products in the images and generate ad copy for each product.
    """

    identified_products: IdentifiedProduct = read_images(images)
    ad_copies = []

    if identified_products.error:
        rprint(f"[red]Error: {identified_products.message}[/red]")
        return []

    if not identified_products:
        rprint("[yellow]No products identified.[/yellow]")
        return []

    for product in identified_products.products:
        ad_copy: AdCopy = generate_ad_copy(product)
        ad_copies.append(ad_copy)

    return identified_products.products, ad_copies


if __name__ == "__main__":
    # Run logger
    logger.info("Starting app...")

    if len(sys.argv) != 2:
        print("Usage: python app.py <path_to_image_list_file>")
        sys.exit(1)

    image_file = sys.argv[1]
    with open(image_file) as file:
        logger.info(f"Reading images from file: {image_file}")
        try:
            image_list = file.read().splitlines()
            logger.info(f"{len(image_list)} images read from file: {image_file}")
        except Exception as e:
            logger.error(f"Error reading images from file: {image_file}")
            logger.error(e)
            sys.exit(1)

    products, ad_copies = run(image_list)

    rprint(f"[green]{len(products)} products identified:[/green]")
    for product, ad_copy in zip(products, ad_copies):
        rprint(f"[green]{product}[/green]")
        rprint(f"[blue]Ad Copy: {ad_copy.ad_copy}[/blue]")

    logger.info("Writing results to file...")

    with open("results.json", "w") as f:
        json.dump(
            {
                "products": [prod.model_dump() for prod in products],
                "ad_copies": [ad.model_dump() for ad in ad_copies],
            },
            f,
            indent=4,
        )

""" 
Example output:
{
    "products": [
        {
            "name": "Ice Skates",
            "key_features": [
                "Lace-up closure",
                "Durable blade",
                "Ankle support"
            ],
            "description": "A pair of ice skates with lace-up closure for secure fit, durable blade for ice skating, and reinforced ankle support."
        },
        {
            "name": "Hiking Boots",
            "key_features": [
                "High-top design",
                "Rugged outsole",
                "Water-resistant"
            ],
            "description": "Sturdy hiking boots featuring a high-top design for ankle support, rugged outsole for grip on uneven terrain, and water-resistant construction."
        },
        {
            "name": "Winter Boots",
            "key_features": [
                "Insulated lining",
                "Waterproof lower",
                "Slip-resistant sole"
            ],
            "description": "Warm winter boots with insulated lining for cold weather, waterproof lower section to keep feet dry, and a slip-resistant sole for stability."
        }
    ],
    "ad_copies": [
        {
            "headline": "Glide with Confidence - Discover the Perfect Ice Skates!",
            "ad_copy": "Step onto the ice with poise and precision with our premium Ice Skates. Designed for both beginners and seasoned skaters, these skates offer a perfect blend of comfort and performance. The lace-up closure ensures a snug fit that keeps you stable as you carve through the ice. With a durable blade that withstands the test of time, you can focus on perfecting your moves rather than worrying about your equipment. The reinforced ankle support provides the necessary protection and aids in preventing injuries, allowing you to skate with peace of mind. Whether you're practicing your spins, jumps, or simply enjoying a leisurely glide across the rink, our Ice Skates are the ideal companion for your ice adventures. Lace up and get ready to experience the thrill of ice skating like never before!",
            "name": "Ice Skates"
        },
        {
            "headline": "Conquer Every Trail with Confidence!",
            "ad_copy": "Embark on your next adventure with our top-of-the-line Hiking Boots! Designed for the trail-blazing spirits, these boots boast a high-top design that provides unparalleled ankle support to keep you steady on any path. The rugged outsole ensures a firm grip on the most uneven terrains, while the water-resistant construction keeps your feet dry as you traverse through streams and muddy trails. Whether you're a seasoned hiker or just starting out, our Hiking Boots are the perfect companion for your outdoor escapades. Lace up and step into the wild with confidence - your journey awaits!",
            "name": "Hiking Boots"
        },
        {
            "headline": "Conquer the Cold with Comfort!",
            "ad_copy": "Step into the season with confidence in our Winter Boots, the ultimate ally against the chill. Designed for those who don't let the cold dictate their moves, these boots feature an insulated lining that wraps your feet in a warm embrace, ensuring that the biting cold is a worry of the past. But warmth isn't their only virtue. With a waterproof lower section, your feet will remain dry and cozy, come rain, snow, or slush. And let's not forget the slip-resistant sole that stands between you and the treacherous ice, offering stability and peace of mind with every step you take. Whether you're braving a blizzard or just nipping out for a coffee, our Winter Boots are your trusty companions, keeping you warm, dry, and upright. Don't let winter slow you down. Lace up and embrace the elements!",
            "name": "Winter Boots"
        }
    ]
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/vision/run.py
=======
import instructor
from openai import OpenAI
from pydantic import BaseModel
import base64

client = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)


class Circle(BaseModel):
    x: int
    y: int
    color: str


def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def draw_circle(image_size, num_circles, path):
    from PIL import Image, ImageDraw
    import random

    image = Image.new("RGB", image_size, "white")

    draw = ImageDraw.Draw(image)
    for _ in range(num_circles):
        # Randomize the circle properties
        radius = 100  # random.randint(10, min(image_size)//5)  # Radius between 10 and 1/5th of the smallest dimension
        x = random.randint(radius, image_size[0] - radius)
        y = random.randint(radius, image_size[1] - radius)
        color = ["red", "black", "blue", "green"][random.randint(0, 3)]

        circle_position = (x - radius, y - radius, x + radius, y + radius)
        print(f"Generating circle at {x, y} with color {color}")
        draw.ellipse(circle_position, fill=color, outline="black")

    image.save(path)


img_path = "circle.jpg"
draw_circle((1024, 1024), 1, img_path)
base64_image = encode_image(img_path)

response = client.chat.completions.create(
    model="gpt-4-vision-preview",
    max_tokens=1800,
    response_model=Circle,
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "find the circle"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
            ],
        }
    ],
)

print(
    f"Found circle with center at x: {response.x}, y: {response.y} and color: {response.color}"
)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/vision/run_raw.py
=======
from openai import OpenAI
from pydantic import BaseModel, Field

client = OpenAI()


class SearchQuery(BaseModel):
    product_name: str
    query: str = Field(
        ...,
        description="A descriptive query to search for the product, include adjectives, and the product type. will be used to serve relevant products to the user.",
    )


class MultiSearchQuery(BaseModel):
    products: list[SearchQuery]


def extract_table(url: str):
    completion = client.chat.completions.create(
        model="gpt-4-vision-preview",
        max_tokens=1800,
        temperature=0,
        stop=["```"],
        messages=[
            {
                "role": "system",
                "content": f"""
                You are an expert system designed to extract products from images for a ecommerse application
                Please provide the product name and a descriptive query to search for the product.
                Accuratly identify every product in an image and provide a descriptive query to search for the product
                
                You just return a correctly formatted JSON object with the product name and query for each product in the image
                and follows the schema below:

                {MultiSearchQuery.model_json_schema()}
                """,
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Extract the products from the image, and describe them in a query in JSON format",
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                ],
            },
            {
                "role": "assistant",
                "content": "Here is the following search queries for the products in the image\n ```json",
            },
        ],
    )
    return MultiSearchQuery.model_validate_json(completion.choices[0].message.content)


if __name__ == "__main__":
    url = "https://mensfashionpostingcom.files.wordpress.com/2020/03/fbe79-img_5052.jpg?w=768"
    products = extract_table(url)
    print(products.model_dump_json(indent=2))
    """
    {
    "products": [
        {
            "product_name": "Olive Green Shirt",
            "query": "Olive green casual long sleeve button-down shirt"
        },
        {
            "product_name": "Black Jeans",
            "query": "Slim fit black jeans for men"
        },
        {
            "product_name": "Sunglasses",
            "query": "Classic brown aviator sunglasses"
        },
        {
            "product_name": "Leather Strap Watch",
            "query": "Minimalist men's watch with black leather strap"
        },
        {
            "product_name": "Beige Sneakers",
            "query": "Men's beige lace-up fashion sneakers with white soles"
        }
    ]}
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/vision/run_table.py
=======
from io import StringIO
from typing import Annotated, Any
from openai import OpenAI
from pydantic import (
    BaseModel,
    BeforeValidator,
    PlainSerializer,
    InstanceOf,
    WithJsonSchema,
)
import pandas as pd
import instructor


client = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)


def to_markdown(df: pd.DataFrame) -> str:
    return df.to_markdown()


def md_to_df(data: Any) -> Any:
    if isinstance(data, str):
        return (
            pd.read_csv(
                StringIO(data),  # Get rid of whitespaces
                sep="|",
                index_col=1,
            )
            .dropna(axis=1, how="all")
            .iloc[1:]
            .map(lambda x: x.strip())
        )  # type: ignore
    return data


MarkdownDataFrame = Annotated[
    InstanceOf[pd.DataFrame],
    BeforeValidator(md_to_df),
    PlainSerializer(to_markdown),
    WithJsonSchema(
        {
            "type": "string",
            "description": """
                The markdown representation of the table, 
                each one should be tidy, do not try to join tables
                that should be seperate""",
        }
    ),
]


class Table(BaseModel):
    caption: str
    dataframe: MarkdownDataFrame


def extract_table(url: str):
    return client.chat.completions.create_iterable(
        model="gpt-4-vision-preview",
        response_model=Table,
        max_tokens=1800,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """Extract the table from the image, and describe it. 
                        Each table should be tidy, do not try to join tables that 
                        should be seperately described.""",
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    },
                ],
            }
        ],
    )


if __name__ == "__main__":
    url = "https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png"
    tables = extract_table(url)
    for tbl in tables:
        print(tbl.caption, end="\n")
        print(tbl.dataframe)
    """
    Top 10 grossing apps in October 2023 (Ireland) for Android platforms, listing the rank, app name, and category.

                App Name                    Category         
    Rank                                                    
    1                          Google One       Productivity
    2                             Disney+      Entertainment
    3       TikTok - Videos, Music & LIVE      Entertainment
    4                    Candy Crush Saga              Games
    5      Tinder: Dating, Chat & Friends  Social networking
    6                         Coin Master              Games
    7                              Roblox              Games
    8      Bumble - Dating & Make Friends             Dating
    9                         Royal Match              Games
    10        Spotify: Music and Podcasts      Music & Audio

    Top 10 grossing apps in October 2023 (Ireland) for iOS platforms, listing the rank, app name, and category.

                App Name                    Category         
    Rank                                                    
    1      Tinder: Dating, Chat & Friends  Social networking
    2                             Disney+      Entertainment
    3      YouTube: Watch, Listen, Stream      Entertainment
    4        Audible: Audio Entertainment      Entertainment
    5                    Candy Crush Saga              Games
    6       TikTok - Videos, Music & LIVE      Entertainment
    7      Bumble - Dating & Make Friends             Dating
    8                              Roblox              Games
    9         LinkedIn: Job Search & News           Business
    10        Duolingo - Language Lessons          Education
    """


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/vision/slides.py
=======
import json
import logging
import sys
from typing import Optional

from dotenv import find_dotenv, load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field
from rich import print as rprint

import instructor

load_dotenv(find_dotenv())

IMAGE_FILE = "image-file.txt"  # file with all the images to be processed

# Add logger
logging.basicConfig()
logger = logging.getLogger("app")
logger.setLevel("INFO")


class Competitor(BaseModel):
    name: str
    features: Optional[list[str]]


# Define models
class Industry(BaseModel):
    """
    Represents competitors from a specific industry extracted from an image using AI.
    """

    name: str = Field(description="The name of the industry")
    competitor_list: list[Competitor] = Field(
        description="A list of competitors for this industry"
    )


class Competition(BaseModel):
    """
    Represents competitors extracted from an image using AI.

    This class serves as a structured representation of
    competitors and their qualities.
    """

    industry_list: list[Industry] = Field(
        description="A list of industries and their competitors"
    )


# Define clients
client_image = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)


# Define functions
def read_images(image_urls: list[str]) -> Competition:
    """
    Given a list of image URLs, identify the competitors in the images.
    """

    logger.info(f"Identifying competitors in images... {len(image_urls)} images")

    return client_image.chat.completions.create(
        model="gpt-4-vision-preview",
        response_model=Competition,
        max_tokens=2048,
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Identify competitors and generate key features for each competitor.",
                    },
                    *[
                        {"type": "image_url", "image_url": {"url": url}}
                        for url in image_urls
                    ],
                ],
            }
        ],
    )


def process_and_identify_competitors():
    """
    Main function to process the image list file and identify competitors.
    """

    logger.info("Starting app...")

    try:
        with open(IMAGE_FILE) as file:
            logger.info(f"Reading images from file: {IMAGE_FILE}")
            image_list = file.read().splitlines()
            logger.info(f"{len(image_list)} images read from file: {IMAGE_FILE}")
    except Exception as e:
        logger.error(f"Error reading images from file: {IMAGE_FILE}")
        logger.error(e)
        sys.exit(1)

    competitors = read_images(image_list)

    rprint(f"[green]{len(competitors.industry_list)} industries identified:[/green]")
    for industry in competitors.industry_list:
        rprint(f"[green]{industry.name}[/green]")
        rprint(f"[blue]Features: {industry.competitor_list}[/blue]")

    logger.info("Writing results to file...")

    with open("results.json", "w") as f:
        json.dump(
            {
                "competitors": competitors.model_dump(),
            },
            f,
            indent=4,
        )


if __name__ == "__main__":
    process_and_identify_competitors()

"""
Example output:
{
    "competitors": {
        "industry_list": [
            {
                "name": "Accommodation and Hospitality",
                "competitor_list": [
                    {
                        "name": "craigslist",
                        "features": [
                            "Transactions Offline",
                            "Inexpensive"
                        ]
                    },
                    {
                        "name": "couchsurfing",
                        "features": [
                            "Transactions Offline",
                            "Inexpensive"
                        ]
                    },
                    {
                        "name": "BedandBreakfast.com",
                        "features": [
                            "Transactions Offline",
                            "Inexpensive"
                        ]
                    },
                    {
                        "name": "airbnb",
                        "features": [
                            "Transactions Online",
                            "Inexpensive"
                        ]
                    },
                    {
                        "name": "HOSTELS.com",
                        "features": [
                            "Transactions Online",
                            "Inexpensive"
                        ]
                    },
                    {
                        "name": "VRBO",
                        "features": [
                            "Transactions Offline",
                            "Costly"
                        ]
                    },
                    {
                        "name": "Rentahome",
                        "features": [
                            "Transactions Online",
                            "Costly"
                        ]
                    },
                        {
                        "name": "Orbitz",
                        "features": [
                            "Transactions Online",
                            "Costly"
                        ]
                    },
                    {
                        "name": "Hotels.com",
                        "features": [
                            "Transactions Online",
                            "Costly"
                        ]
                    }
                ]
            },
            {
                "name": "E-commerce Wine Retailers",
                "competitor_list": [
                    {
                        "name": "winesimple",
                        "features": [
                            "Ecommerce Retailers",
                            "True Personalized Selections",
                            "Brand Name Wine",
                            "No Inventory Cost",
                            "Target Mass Market"
                        ]
                    },
                    {
                        "name": "nakedwines.com",
                        "features": [
                            "Ecommerce Retailers",
                            "Target Mass Market"
                        ]
                    },
                    {
                        "name": "Club W",
                        "features": [
                            "Ecommerce Retailers",
                            "Brand Name Wine",
                            "Target Mass Market"
                        ]
                    },
                    {
                        "name": "Tasting Room",
                        "features": [
                            "Ecommerce Retailers",
                            "True Personalized Selections",
                            "Brand Name Wine"
                        ]
                    },
                    {
                        "name": "hellovino",
                        "features": [
                            "Ecommerce Retailers",
                            "True Personalized Selections",
                            "No Inventory Cost",
                            "Target Mass Market"
                        ]
                    }
                ]
            }
        ]
    }
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/watsonx/watsonx.py
=======
import os

import litellm
from litellm import completion
from pydantic import BaseModel, Field

import instructor
from instructor import Mode

litellm.drop_params = True  # watsonx.ai doesn't support `json_mode`

os.environ["WATSONX_URL"] = "https://us-south.ml.cloud.ibm.com"
os.environ["WATSONX_API_KEY"] = ""
os.environ["WATSONX_PROJECT_ID"] = ""
# Additional options: https://docs.litellm.ai/docs/providers/watsonx


class Company(BaseModel):
    name: str = Field(description="name of the company")
    year_founded: int = Field(description="year the company was founded")


client = instructor.from_litellm(completion, mode=Mode.JSON)

resp = client.chat.completions.create(
    model="watsonx/meta-llama/llama-3-8b-instruct",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": """\
Given the following text, create a Company object:

IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems.
""",
        }
    ],
    project_id=os.environ["WATSONX_PROJECT_ID"],
    response_model=Company,
)

print(resp.model_dump_json(indent=2))
"""
{
  "name": "IBM",
  "year_founded": 1911
}
"""


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/examples/youtube-clips/run.py
=======
from youtube_transcript_api import YouTubeTranscriptApi
from pydantic import BaseModel, Field
from collections.abc import Generator, Iterable
import instructor
import openai

client = instructor.from_openai(openai.OpenAI())


def extract_video_id(url: str) -> str | None:
    import re

    match = re.search(r"v=([a-zA-Z0-9_-]+)", url)
    if match:
        return match.group(1)


class TranscriptSegment(BaseModel):
    source_id: int
    start: float
    text: str


def get_transcript_with_timing(
    video_id: str,
) -> Generator[TranscriptSegment, None, None]:
    """
    Fetches the transcript of a YouTube video along with the start and end times for each text segment,
    and returns them as a list of Pydantic models.

    Parameters:
    - video_id (str): The YouTube video ID for which the transcript is to be fetched.

    Returns:
    - A generator that yields TranscriptSegment models, each containing 'index', 'start', and 'text' keys.
    """
    transcript = YouTubeTranscriptApi.get_transcript(video_id)
    for ii, segment in enumerate(transcript):
        yield TranscriptSegment(
            source_id=ii, start=segment["start"], text=segment["text"]
        )


class YoutubeClip(BaseModel):
    title: str = Field(
        description="Specific and informative title for the individual clip."
    )
    description: str = Field(
        description="A detailed description of the clip, including any notable quotes or phrases. should be a summary of sorts."
    )
    start: float
    end: float
    source_ids: list[int] = Field(exclude=True)


class YoutubeClips(BaseModel):
    clips: list[YoutubeClip]


def yield_clips(segments: Iterable[TranscriptSegment]) -> Iterable[YoutubeClips]:
    """
    Extracts a list of YouTube clips from a list of transcript segments.

    Parameters:
    - segments (Iterable[TranscriptSegment]): A list of TranscriptSegment models, each containing 'index', 'start', and 'text' keys.

    Returns:
    - A generator that yields YoutubeClipw models, each containing 'title', 'description', 'start', 'end', and 'source_ids' keys.
    """

    return client.chat.completions.create(
        model="gpt-4-turbo-preview",
        stream=True,
        messages=[
            {
                "role": "system",
                "content": "You are given a sequence of YouTube transcripts and your job is to return notable clips that can be recut as smaller videos. give very specific titles and descriptions. Make sure the length of clips is proportional to the length of the video. Note that this is a transcript and so there might be spelling errors. Note that and correct any spellings. Use the context to make sure you're spelling things correctly. ",
            },
            {
                "role": "user",
                "content": f"Let's use the following transcript segments.\n{segments}",
            },
        ],
        response_model=instructor.Partial[YoutubeClips],
        validation_context={"segments": segments},
    )  # type: ignore


# Example usage
if __name__ == "__main__":
    from rich.table import Table
    from rich.console import Console
    from rich.prompt import Prompt

    console = Console()
    url = Prompt.ask("Enter a YouTube URL")

    with console.status("[bold green]Processing YouTube URL...") as status:
        video_id = extract_video_id(url)

        if video_id is None:
            raise ValueError("Invalid YouTube video URL")

        transcript = list(get_transcript_with_timing(video_id))
        status.update("[bold green]Generating clips...")

        for clip in yield_clips(transcript):
            console.clear()

            table = Table(title="YouTube Clips", padding=(0, 1))

            table.add_column("Title", style="cyan")
            table.add_column("Description", style="magenta")
            table.add_column("Start", justify="right", style="green")
            table.add_column("End", justify="right", style="green")
            for youtube_clip in clip.clips or []:
                table.add_row(
                    youtube_clip.title,
                    youtube_clip.description,
                    str(youtube_clip.start),
                    str(youtube_clip.end),
                )
            console.print(table)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/batch.py
=======
from typing import Literal, Any, Union
from pydantic import BaseModel, Field
from instructor.process_response import handle_response_model
import uuid

openai_models = Literal[
    "gpt-4o",
    "gpt-4-turbo",
    "gpt-4",
    "gpt-4-32k",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k",
    "gpt-4-turbo-preview",
    "gpt-4-vision-preview",
    "gpt-4-turbo-2024-04-09",
    "gpt-4-0314",
    "gpt-4-32k-0314",
    "gpt-4-32k-0613",
    "gpt-3.5-turbo-0301",
    "gpt-3.5-turbo-16k-0613",
    "gpt-3.5-turbo-1106",
    "gpt-3.5-turbo-0613",
]


class Function(BaseModel):
    name: str
    description: str
    parameters: Any


class Tool(BaseModel):
    type: str
    function: Function


class RequestBody(BaseModel):
    model: Union[openai_models, str]
    messages: list[dict[str, Any]]
    max_tokens: int = Field(default=1000)
    tools: list[Tool]


class BatchModel(BaseModel):
    custom_id: str
    method: Literal["POST"]
    url: Literal["/v1/chat/completions"]
    body: RequestBody


class BatchJob:
    @classmethod
    def create_from_messages(
        cls,
        messages_batch: list[list[dict[str, Any]]],
        model: Union[openai_models, str],
        response_model: type[BaseModel],
        max_tokens: int = 1000,
    ):
        _, tools = handle_response_model(response_model=response_model)
        return [
            BatchModel(
                custom_id=str(uuid.uuid4()),
                method="POST",
                url="/v1/chat/completions",
                body=RequestBody(
                    model=model,
                    max_tokens=max_tokens,
                    messages=messages,
                    **tools,
                ),
            ).model_dump(mode="json")
            for messages in messages_batch
        ]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client.py
=======
from __future__ import annotations

import openai
import inspect
import instructor
from .utils import Provider, get_provider
from openai.types.chat import ChatCompletionMessageParam
from typing import (
    TypeVar,
    Callable,
    overload,
    Union,
    Any,
)
from collections.abc import Generator, Iterable, Awaitable, AsyncGenerator
from typing_extensions import Self
from pydantic import BaseModel
from instructor.dsl.partial import Partial


T = TypeVar("T", bound=Union[BaseModel, "Iterable[Any]", "Partial[Any]"])


class Instructor:
    client: Any | None
    create_fn: Callable[..., Any]
    mode: instructor.Mode
    default_model: str | None = None
    provider: Provider

    def __init__(
        self,
        client: Any | None,
        create: Callable[..., Any],
        mode: instructor.Mode = instructor.Mode.TOOLS,
        provider: Provider = Provider.OPENAI,
        **kwargs: Any,
    ):
        self.client = client
        self.create_fn = create
        self.mode = mode
        self.kwargs = kwargs
        self.provider = provider

    @property
    def chat(self) -> Self:
        return self

    @property
    def completions(self) -> Self:
        return self

    @property
    def messages(self) -> Self:
        return self

    @overload
    def create(
        self: AsyncInstructor,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Awaitable[T]: ...

    @overload
    def create(
        self: Self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> T: ...

    # TODO: we should overload a case where response_model is None
    def create(
        self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> T | Awaitable[T]:
        kwargs = self.handle_kwargs(kwargs)

        return self.create_fn(
            response_model=response_model,
            messages=messages,
            max_retries=max_retries,
            validation_context=validation_context,
            strict=strict,
            **kwargs,
        )

    @overload
    def create_partial(
        self: AsyncInstructor,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> AsyncGenerator[T, None]: ...

    @overload
    def create_partial(
        self: Self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Generator[T, None, None]: ...

    def create_partial(
        self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Generator[T, None, None] | AsyncGenerator[T, None]:
        kwargs["stream"] = True

        kwargs = self.handle_kwargs(kwargs)

        response_model = instructor.Partial[response_model]  # type: ignore
        return self.create_fn(
            messages=messages,
            response_model=response_model,
            max_retries=max_retries,
            validation_context=validation_context,
            strict=strict,
            **kwargs,
        )

    @overload
    def create_iterable(
        self: AsyncInstructor,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> AsyncGenerator[T, None]: ...

    @overload
    def create_iterable(
        self: Self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Generator[T, None, None]: ...

    def create_iterable(
        self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Generator[T, None, None] | AsyncGenerator[T, None]:
        kwargs["stream"] = True
        kwargs = self.handle_kwargs(kwargs)

        response_model = Iterable[response_model]  # type: ignore
        return self.create_fn(
            messages=messages,
            response_model=response_model,
            max_retries=max_retries,
            validation_context=validation_context,
            strict=strict,
            **kwargs,
        )

    @overload
    def create_with_completion(
        self: AsyncInstructor,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> Awaitable[tuple[T, Any]]: ...

    @overload
    def create_with_completion(
        self: Self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> tuple[T, Any]: ...

    def create_with_completion(
        self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> tuple[T, Any] | Awaitable[tuple[T, Any]]:
        kwargs = self.handle_kwargs(kwargs)
        model = self.create_fn(
            messages=messages,
            response_model=response_model,
            max_retries=max_retries,
            validation_context=validation_context,
            strict=strict,
            **kwargs,
        )
        return model, model._raw_response

    def handle_kwargs(self, kwargs: dict[str, Any]) -> dict[str, Any]:
        for key, value in self.kwargs.items():
            if key not in kwargs:
                kwargs[key] = value
        return kwargs


class AsyncInstructor(Instructor):
    client: Any | None
    create_fn: Callable[..., Any]
    mode: instructor.Mode
    default_model: str | None = None
    provider: Provider

    def __init__(
        self,
        client: Any | None,
        create: Callable[..., Any],
        mode: instructor.Mode = instructor.Mode.TOOLS,
        provider: Provider = Provider.OPENAI,
        **kwargs: Any,
    ):
        self.client = client
        self.create_fn = create
        self.mode = mode
        self.kwargs = kwargs
        self.provider = provider

    async def create(
        self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> T:
        kwargs = self.handle_kwargs(kwargs)
        return await self.create_fn(
            response_model=response_model,
            validation_context=validation_context,
            max_retries=max_retries,
            messages=messages,
            strict=strict,
            **kwargs,
        )

    async def create_partial(
        self,
        response_model: type[T],
        messages: list[ChatCompletionMessageParam],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> AsyncGenerator[T, None]:
        assert self.provider != Provider.ANTHROPIC, "Anthropic doesn't support partial"

        kwargs = self.handle_kwargs(kwargs)
        kwargs["stream"] = True
        async for item in await self.create_fn(
            response_model=instructor.Partial[response_model],  # type: ignore
            validation_context=validation_context,
            max_retries=max_retries,
            messages=messages,
            strict=strict,
            **kwargs,
        ):
            yield item

    async def create_iterable(
        self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> AsyncGenerator[T, None]:
        kwargs = self.handle_kwargs(kwargs)
        kwargs["stream"] = True
        async for item in await self.create_fn(
            response_model=Iterable[response_model],
            validation_context=validation_context,
            max_retries=max_retries,
            messages=messages,
            strict=strict,
            **kwargs,
        ):
            yield item

    async def create_with_completion(
        self,
        messages: list[ChatCompletionMessageParam],
        response_model: type[T],
        max_retries: int = 3,
        validation_context: dict[str, Any] | None = None,
        strict: bool = True,
        **kwargs: Any,
    ) -> tuple[T, Any]:
        kwargs = self.handle_kwargs(kwargs)
        response = await self.create_fn(
            response_model=response_model,
            validation_context=validation_context,
            max_retries=max_retries,
            messages=messages,
            strict=strict,
            **kwargs,
        )
        return response, response._raw_response


@overload
def from_openai(
    client: openai.OpenAI,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> Instructor:
    pass


@overload
def from_openai(
    client: openai.AsyncOpenAI,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> AsyncInstructor:
    pass


def from_openai(
    client: openai.OpenAI | openai.AsyncOpenAI,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> Instructor | AsyncInstructor:
    if hasattr(client, "base_url"):
        provider = get_provider(str(client.base_url))
    else:
        provider = Provider.OPENAI

    if not isinstance(client, (openai.OpenAI, openai.AsyncOpenAI)):
        import warnings

        warnings.warn(
            "Client should be an instance of openai.OpenAI or openai.AsyncOpenAI. Unexpected behavior may occur with other client types.",
            stacklevel=2,
        )

    if provider in {Provider.ANYSCALE, Provider.TOGETHER}:
        assert mode in {
            instructor.Mode.TOOLS,
            instructor.Mode.JSON,
            instructor.Mode.JSON_SCHEMA,
            instructor.Mode.MD_JSON,
        }

    if provider in {Provider.DATABRICKS}:
        assert mode in {
            instructor.Mode.MD_JSON
        }, "Databricks provider only supports `MD_JSON` mode."

    if provider in {Provider.OPENAI}:
        assert mode in {
            instructor.Mode.TOOLS,
            instructor.Mode.JSON,
            instructor.Mode.FUNCTIONS,
            instructor.Mode.PARALLEL_TOOLS,
            instructor.Mode.MD_JSON,
        }

    if isinstance(client, openai.OpenAI):
        return Instructor(
            client=client,
            create=instructor.patch(create=client.chat.completions.create, mode=mode),
            mode=mode,
            provider=provider,
            **kwargs,
        )

    if isinstance(client, openai.AsyncOpenAI):
        return AsyncInstructor(
            client=client,
            create=instructor.patch(create=client.chat.completions.create, mode=mode),
            mode=mode,
            provider=provider,
            **kwargs,
        )


@overload
def from_litellm(
    completion: Callable[..., Any],
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> Instructor: ...


@overload
def from_litellm(
    completion: Awaitable[Any],
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> AsyncInstructor:
    pass


def from_litellm(
    completion: Callable[..., Any] | Awaitable[Any],
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> Instructor | AsyncInstructor:
    is_async = inspect.isawaitable(completion)

    if not is_async:
        return Instructor(
            client=None,
            create=instructor.patch(create=completion, mode=mode),
            mode=mode,
            **kwargs,
        )
    else:
        return AsyncInstructor(
            client=None,
            create=instructor.patch(create=completion, mode=mode),
            mode=mode,
            **kwargs,
        )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_anthropic.py
=======
from __future__ import annotations

import anthropic
import instructor

from typing import overload, Any


@overload
def from_anthropic(
    client: (
        anthropic.Anthropic | anthropic.AnthropicBedrock | anthropic.AnthropicVertex
    ),
    mode: instructor.Mode = instructor.Mode.ANTHROPIC_TOOLS,
    **kwargs: Any,
) -> instructor.Instructor: ...


@overload
def from_anthropic(
    client: (
        anthropic.AsyncAnthropic
        | anthropic.AsyncAnthropicBedrock
        | anthropic.AsyncAnthropicVertex
    ),
    mode: instructor.Mode = instructor.Mode.ANTHROPIC_TOOLS,
    **kwargs: Any,
) -> instructor.AsyncInstructor: ...


def from_anthropic(
    client: (
        anthropic.Anthropic
        | anthropic.AsyncAnthropic
        | anthropic.AnthropicBedrock
        | anthropic.AsyncAnthropicBedrock
        | anthropic.AsyncAnthropicVertex
        | anthropic.AnthropicVertex
    ),
    mode: instructor.Mode = instructor.Mode.ANTHROPIC_TOOLS,
    **kwargs: Any,
) -> instructor.Instructor | instructor.AsyncInstructor:
    assert (
        mode
        in {
            instructor.Mode.ANTHROPIC_JSON,
            instructor.Mode.ANTHROPIC_TOOLS,
        }
    ), "Mode be one of {instructor.Mode.ANTHROPIC_JSON, instructor.Mode.ANTHROPIC_TOOLS}"

    assert isinstance(
        client,
        (
            anthropic.Anthropic,
            anthropic.AsyncAnthropic,
            anthropic.AnthropicBedrock,
            anthropic.AnthropicVertex,
            anthropic.AsyncAnthropicBedrock,
            anthropic.AsyncAnthropicVertex,
        ),
    ), "Client must be an instance of {anthropic.Anthropic, anthropic.AsyncAnthropic, anthropic.AnthropicBedrock, anthropic.AsyncAnthropicBedrock,  anthropic.AnthropicVertex, anthropic.AsyncAnthropicVertex}"

    create = client.messages.create

    if isinstance(
        client,
        (anthropic.Anthropic, anthropic.AnthropicBedrock, anthropic.AnthropicVertex),
    ):
        return instructor.Instructor(
            client=client,
            create=instructor.patch(create=create, mode=mode),
            provider=instructor.Provider.ANTHROPIC,
            mode=mode,
            **kwargs,
        )

    else:
        return instructor.AsyncInstructor(
            client=client,
            create=instructor.patch(create=create, mode=mode),
            provider=instructor.Provider.ANTHROPIC,
            mode=mode,
            **kwargs,
        )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_cohere.py
=======
from __future__ import annotations

import cohere
import instructor
from functools import wraps
from typing import (
    TypeVar,
    overload,
)
from typing import Any
from typing_extensions import ParamSpec
from pydantic import BaseModel
from instructor.process_response import handle_response_model
from instructor.retry import retry_async


T_Model = TypeVar("T_Model", bound=BaseModel)
T_ParamSpec = ParamSpec("T_ParamSpec")


@overload
def from_cohere(
    client: cohere.Client,
    mode: instructor.Mode = instructor.Mode.COHERE_TOOLS,
    **kwargs: Any,
) -> instructor.Instructor: ...


@overload
def from_cohere(
    client: cohere.AsyncClient,
    mode: instructor.Mode = instructor.Mode.COHERE_TOOLS,
    **kwargs: Any,
) -> instructor.AsyncInstructor: ...


def from_cohere(
    client: cohere.Client | cohere.AsyncClient,
    mode: instructor.Mode = instructor.Mode.COHERE_TOOLS,
    **kwargs: Any,
):
    assert mode in {
        instructor.Mode.COHERE_TOOLS,
    }, "Mode be one of {instructor.Mode.COHERE_TOOLS}"

    assert isinstance(
        client, (cohere.Client, cohere.AsyncClient)
    ), "Client must be an instance of cohere.Cohere or cohere.AsyncCohere"

    if isinstance(client, cohere.Client):
        return instructor.Instructor(
            client=client,
            create=instructor.patch(create=client.chat, mode=mode),
            provider=instructor.Provider.COHERE,
            mode=mode,
            **kwargs,
        )

    @wraps(client.chat)
    async def new_create_async(
        response_model: type[T_Model] | None = None,
        validation_context: dict[str, Any] | None = None,
        max_retries: int = 1,
        *args: T_ParamSpec.args,
        **kwargs: T_ParamSpec.kwargs,
    ) -> T_Model:
        prepared_response_model, new_kwargs = handle_response_model(
            response_model=response_model,
            mode=mode,
            **kwargs,
        )
        response = await retry_async(
            func=client.chat,
            response_model=prepared_response_model,
            validation_context=validation_context,
            max_retries=max_retries,
            args=args,
            kwargs=new_kwargs,
            mode=mode,
        )
        return response

    return instructor.AsyncInstructor(
        client=client,
        create=new_create_async,
        provider=instructor.Provider.COHERE,
        mode=mode,
        **kwargs,
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_gemini.py
=======
# type: ignore
from __future__ import annotations

from typing import Any, Literal, overload

import google.generativeai as genai

import instructor


@overload
def from_gemini(
    client: genai.GenerativeModel,
    mode: instructor.Mode = instructor.Mode.GEMINI_JSON,
    use_async: Literal[True] = True,
    **kwargs: Any,
) -> instructor.AsyncInstructor: ...


@overload
def from_gemini(
    client: genai.GenerativeModel,
    mode: instructor.Mode = instructor.Mode.GEMINI_JSON,
    use_async: Literal[False] = False,
    **kwargs: Any,
) -> instructor.Instructor: ...


def from_gemini(
    client: genai.GenerativeModel,
    mode: instructor.Mode = instructor.Mode.GEMINI_JSON,
    use_async: bool = False,
    **kwargs: Any,
) -> instructor.Instructor | instructor.AsyncInstructor:
    assert (
        mode == instructor.Mode.GEMINI_JSON
    ), "Mode must be instructor.Mode.GEMINI_JSON"

    assert isinstance(
        client,
        (genai.GenerativeModel),
    ), "Client must be an instance of genai.generativemodel"

    if use_async:
        create = client.generate_content_async
        return instructor.AsyncInstructor(
            client=client,
            create=instructor.patch(create=create, mode=mode),
            provider=instructor.Provider.GEMINI,
            mode=mode,
            **kwargs,
        )

    create = client.generate_content
    return instructor.Instructor(
        client=client,
        create=instructor.patch(create=create, mode=mode),
        provider=instructor.Provider.GEMINI,
        mode=mode,
        **kwargs,
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_groq.py
=======
from __future__ import annotations

from typing import overload, Any

import groq
import instructor


@overload
def from_groq(
    client: groq.Groq,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> instructor.Instructor: ...


@overload
def from_groq(
    client: groq.AsyncGroq,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> instructor.AsyncInstructor: ...


def from_groq(
    client: groq.Groq | groq.AsyncGroq,
    mode: instructor.Mode = instructor.Mode.TOOLS,
    **kwargs: Any,
) -> instructor.Instructor | instructor.AsyncInstructor:
    assert mode in {
        instructor.Mode.JSON,
        instructor.Mode.TOOLS,
    }, "Mode be one of {instructor.Mode.JSON, instructor.Mode.TOOLS}"

    assert isinstance(
        client, (groq.Groq, groq.AsyncGroq)
    ), "Client must be an instance of groq.GROQ"

    if isinstance(client, groq.Groq):
        return instructor.Instructor(
            client=client,
            create=instructor.patch(create=client.chat.completions.create, mode=mode),
            provider=instructor.Provider.GROQ,
            mode=mode,
            **kwargs,
        )

    else:
        return instructor.AsyncInstructor(
            client=client,
            create=instructor.patch(create=client.chat.completions.create, mode=mode),
            provider=instructor.Provider.GROQ,
            mode=mode,
            **kwargs,
        )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_mistral.py
=======
# Future imports to ensure compatibility with Python 3.9
from __future__ import annotations

import mistralai.client
import mistralai.async_client as mistralaiasynccli
import instructor
from typing import overload, Any


@overload
def from_mistral(
    client: mistralai.client.MistralClient,
    mode: instructor.Mode = instructor.Mode.MISTRAL_TOOLS,
    **kwargs: Any,
) -> instructor.Instructor: ...


@overload
def from_mistral(
    client: mistralaiasynccli.MistralAsyncClient,
    mode: instructor.Mode = instructor.Mode.MISTRAL_TOOLS,
    **kwargs: Any,
) -> instructor.AsyncInstructor: ...


def from_mistral(
    client: mistralai.client.MistralClient | mistralaiasynccli.MistralAsyncClient,
    mode: instructor.Mode = instructor.Mode.MISTRAL_TOOLS,
    **kwargs: Any,
) -> instructor.Instructor | instructor.AsyncInstructor:
    assert mode in {
        instructor.Mode.MISTRAL_TOOLS,
    }, "Mode be one of {instructor.Mode.MISTRAL_TOOLS}"

    assert isinstance(
        client, (mistralai.client.MistralClient, mistralaiasynccli.MistralAsyncClient)
    ), "Client must be an instance of mistralai.client.MistralClient or mistralai.async_cli.MistralAsyncClient"

    if isinstance(client, mistralai.client.MistralClient):
        return instructor.Instructor(
            client=client,
            create=instructor.patch(create=client.chat, mode=mode),
            provider=instructor.Provider.MISTRAL,
            mode=mode,
            **kwargs,
        )

    else:
        return instructor.AsyncInstructor(
            client=client,
            create=instructor.patch(create=client.chat, mode=mode),
            provider=instructor.Provider.MISTRAL,
            mode=mode,
            **kwargs,
        )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/client_vertexai.py
=======
from __future__ import annotations

from typing import Any

from vertexai.preview.generative_models import ToolConfig  # type: ignore
import vertexai.generative_models as gm  # type: ignore
from pydantic import BaseModel
import instructor
import jsonref  # type: ignore


def _create_gemini_json_schema(model: BaseModel):
    schema = model.model_json_schema()
    schema_without_refs: dict[str, Any] = jsonref.replace_refs(schema)  # type: ignore
    gemini_schema: dict[Any, Any] = {
        "type": schema_without_refs["type"],
        "properties": schema_without_refs["properties"],
        "required": schema_without_refs["required"],
    }
    return gemini_schema


def _create_vertexai_tool(model: BaseModel) -> gm.Tool:
    parameters = _create_gemini_json_schema(model)

    declaration = gm.FunctionDeclaration(
        name=model.__name__, description=model.__doc__, parameters=parameters
    )

    tool = gm.Tool(function_declarations=[declaration])

    return tool


def vertexai_message_parser(message: dict[str, str]) -> gm.Content:
    return gm.Content(
        role=message["role"], parts=[gm.Part.from_text(message["content"])]
    )


def _vertexai_message_list_parser(messages: list[dict[str, str]]) -> list[gm.Content]:
    contents = [
        vertexai_message_parser(message) if isinstance(message, dict) else message
        for message in messages
    ]
    return contents


def vertexai_function_response_parser(
    response: gm.GenerationResponse, exception: Exception
) -> gm.Content:
    return gm.Content(
        parts=[
            gm.Part.from_function_response(
                name=response.candidates[0].content.parts[0].function_call.name,
                response={
                    "content": f"Validation Error found:\n{exception}\nRecall the function correctly, fix the errors"
                },
            )
        ]
    )


def vertexai_process_response(_kwargs: dict[str, Any], model: BaseModel):
    messages: list[dict[str, str]] = _kwargs.pop("messages")
    contents = _vertexai_message_list_parser(messages)

    tool = _create_vertexai_tool(model=model)

    tool_config = ToolConfig(
        function_calling_config=ToolConfig.FunctionCallingConfig(
            mode=ToolConfig.FunctionCallingConfig.Mode.ANY,
        )
    )
    return contents, [tool], tool_config


def vertexai_process_json_response(_kwargs: dict[str, Any], model: BaseModel):
    messages: list[dict[str, str]] = _kwargs.pop("messages")
    contents = _vertexai_message_list_parser(messages)

    config: dict[str, Any] | None = _kwargs.pop("generation_config", None)

    response_schema = _create_gemini_json_schema(model)

    generation_config = gm.GenerationConfig(
        response_mime_type="application/json",
        response_schema=response_schema,
        **(config if config else {}),
    )

    return contents, generation_config


def from_vertexai(
    client: gm.GenerativeModel,
    mode: instructor.Mode = instructor.Mode.VERTEXAI_TOOLS,
    _async: bool = False,
    **kwargs: Any,
) -> instructor.Instructor:
    assert mode in {
        instructor.Mode.VERTEXAI_TOOLS,
        instructor.Mode.VERTEXAI_JSON,
    }, "Mode must be instructor.Mode.VERTEXAI_TOOLS"

    assert isinstance(
        client, gm.GenerativeModel
    ), "Client must be an instance of vertexai.generative_models.GenerativeModel"

    create = client.generate_content_async if _async else client.generate_content

    return instructor.Instructor(
        client=client,
        create=instructor.patch(create=create, mode=mode),
        provider=instructor.Provider.VERTEXAI,
        mode=mode,
        **kwargs,
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/distil.py
=======
import enum
import json
import uuid
import logging
import inspect
import functools

from typing import (
    Any,
    Callable,
    Optional,
    TypeVar,
    TypedDict,
    Literal,
    Union,
)
from typing_extensions import ParamSpec, NotRequired
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from pydantic import BaseModel, validate_call

from openai import OpenAI
from instructor.function_calls import openai_schema


P = ParamSpec("P")
T_Retval = TypeVar("T_Retval", bound=BaseModel)


class OpenAIChatKwargs(TypedDict):
    messages: list[ChatCompletionMessageParam]
    functions: NotRequired[list[dict[str, Any]]]


class FinetuneFormat(enum.Enum):
    MESSAGES = "messages"
    RAW = "raw"


def get_signature_from_fn(fn: Callable[..., Any]) -> str:
    """
    Get the function signature as a string.

    :Example:

    >>> def my_function(a: int, b: int) -> int:
    >>>     return a + b
    >>>
    >>> get_signature_from_fn(my_function)
    "def my_function(a: int, b: int) -> int"

    :param fn: Function to get the signature for.
    :return: Function signature as a string.
    """
    sig = inspect.signature(fn)
    lines = f"def {fn.__name__}{sig}"
    docstring = inspect.getdoc(fn)
    if docstring:
        formatted_docstring = f'"""\n{docstring}\n"""'
    else:
        formatted_docstring = ""
    return f"{lines}\n{formatted_docstring}"


@functools.lru_cache
def format_function(func: Callable[..., Any]) -> str:
    """
    Format a function as a string with docstring and body.
    """
    source_lines = inspect.getsourcelines(func)
    definition = " ".join(source_lines[0]).strip()

    docstring = inspect.getdoc(func)
    if docstring:
        formatted_docstring = f'"""\n{docstring}\n"""'
    else:
        formatted_docstring = ""

    body = inspect.getsource(func)
    body = body.replace(f"def {func.__name__}", "")

    return f"{definition}\n{formatted_docstring}\n{body}"


def is_return_type_base_model_or_instance(func: Callable[..., Any]) -> bool:
    """
    Check if the return type of a function is a pydantic BaseModel or an instance of it.

    :param func: Function to check.
    :return: True if the return type is a pydantic BaseModel or an instance of it.
    """
    return_type = inspect.signature(func).return_annotation
    assert (
        return_type != inspect.Signature.empty
    ), "Must have a return type hint that is a pydantic BaseModel"
    return inspect.isclass(return_type) and issubclass(return_type, BaseModel)


class Instructions:
    def __init__(
        self,
        name: Optional[str] = None,
        id: Optional[str] = None,
        log_handlers: Optional[list[logging.Handler]] = None,
        finetune_format: FinetuneFormat = FinetuneFormat.MESSAGES,
        indent: int = 2,
        include_code_body: bool = False,
        openai_client: Optional[OpenAI] = None,
    ) -> None:
        """
        Instructions for distillation and dispatch.

        :param name: Name of the instructions.
        :param id: ID of the instructions.
        :param log_handlers: List of log handlers to use.
        :param finetune_format: Format to use for finetuning.
        :param indent: Indentation to use for finetuning.
        :param include_code_body: Whether to include the code body in the finetuning.
        """
        self.name = name
        self.id = id or str(uuid.uuid4())
        self.unique_id = str(uuid.uuid4())
        self.finetune_format = finetune_format
        self.indent = indent
        self.include_code_body = include_code_body
        self.client = openai_client or OpenAI()

        self.logger = logging.getLogger(self.name)
        for handler in log_handlers or []:
            self.logger.addHandler(handler)

    def distil(
        self,
        *args: Any,
        name: Optional[str] = None,
        mode: Literal["distil", "dispatch"] = "distil",
        model: str = "gpt-3.5-turbo",
        fine_tune_format: Optional[FinetuneFormat] = None,
    ) -> Union[
        Callable[P, Union[T_Retval, ChatCompletion]],
        Callable[[Callable[P, T_Retval]], Callable[P, Union[T_Retval, ChatCompletion]]],
    ]:
        """
        Decorator to track the function call and response, supports distillation and dispatch modes.

        If used without arguments, it must be used as a decorator.

        :Example:

        >>> @distil
        >>> def my_function() -> MyModel:
        >>>     return MyModel()
        >>>
        >>> @distil(name="my_function")
        >>> def my_function() -> MyModel:
        >>>     return MyModel()

        :param fn: Function to track.
        :param name: Name of the function to track. Defaults to the function name.
        :param mode: Mode to use for distillation. Defaults to "distil".
        """
        allowed_modes = {"distil", "dispatch"}
        assert mode in allowed_modes, f"Must be in {allowed_modes}"

        if fine_tune_format is None:
            fine_tune_format = self.finetune_format

        def _wrap_distil(
            fn: Callable[P, T_Retval],
        ) -> Callable[P, Union[T_Retval, ChatCompletion]]:
            msg = f"Return type hint for {fn} must subclass `pydantic.BaseModel'"
            assert is_return_type_base_model_or_instance(fn), msg
            return_base_model = inspect.signature(fn).return_annotation

            @functools.wraps(fn)
            def _dispatch(*args: P.args, **kwargs: P.kwargs) -> ChatCompletion:
                openai_kwargs = self.openai_kwargs(
                    name=name if name else fn.__name__,
                    fn=fn,
                    args=args,
                    kwargs=kwargs,
                    base_model=return_base_model,
                )
                return self.client.chat.completions.create(
                    **openai_kwargs,
                    model=model,
                    response_model=return_base_model,  # type: ignore - TODO figure out why `response_model` is not recognized
                )

            @functools.wraps(fn)
            def _distil(*args: P.args, **kwargs: P.kwargs) -> T_Retval:
                resp = fn(*args, **kwargs)
                self.track(
                    fn,
                    args,
                    kwargs,
                    resp,
                    name=name,
                    finetune_format=fine_tune_format,
                )
                return resp

            return _dispatch if mode == "dispatch" else _distil

        if len(args) == 1 and callable(args[0]):
            return _wrap_distil(args[0])

        return _wrap_distil

    @validate_call
    def track(
        self,
        fn: Callable[..., Any],
        args: tuple[Any, ...],
        kwargs: dict[str, Any],
        resp: BaseModel,
        name: Optional[str] = None,
        finetune_format: FinetuneFormat = FinetuneFormat.MESSAGES,
    ) -> None:
        """
        Track the function call and response in a log file, later used for finetuning.

        :param fn: Function to track.
        :param args: Arguments passed to the function.
        :param kwargs: Keyword arguments passed to the function.
        :param resp: Response returned by the function.
        :param name: Name of the function to track. Defaults to the function name.
        :param finetune_format: Format to use for finetuning. Defaults to "raw".
        """
        name = name if name else fn.__name__
        base_model = type(resp)

        if finetune_format == FinetuneFormat.MESSAGES:
            openai_function_call = openai_schema(base_model).openai_schema
            openai_kwargs = self.openai_kwargs(name, fn, args, kwargs, base_model)
            openai_kwargs["messages"].append(
                {
                    "role": "assistant",
                    "function_call": {
                        "name": base_model.__name__,
                        "arguments": resp.model_dump_json(indent=self.indent),
                    },
                }
            )
            openai_kwargs["functions"] = [openai_function_call]
            self.logger.info(json.dumps(openai_kwargs))

        if finetune_format == FinetuneFormat.RAW:
            function_body = dict(
                fn_name=name,
                fn_repr=format_function(fn),
                args=args,
                kwargs=kwargs,
                resp=resp.model_dump(),
                schema=base_model.model_json_schema(),
            )
            self.logger.info(json.dumps(function_body))

    def openai_kwargs(
        self,
        name: str,
        fn: Callable[..., Any],
        args: tuple[Any, ...],
        kwargs: dict[str, Any],
        base_model: type[BaseModel],
    ) -> OpenAIChatKwargs:
        if self.include_code_body:
            func_def = format_function(fn)
        else:
            func_def = get_signature_from_fn(fn)

        str_args = ", ".join(map(str, args))
        str_kwargs = (
            ", ".join(f"{k}={json.dumps(v)}" for k, v in kwargs.items()) or None
        )
        call_args = ", ".join(filter(None, [str_args, str_kwargs]))

        function_body: OpenAIChatKwargs = {
            "messages": [
                {
                    "role": "system",
                    "content": f"Predict the results of this function:\n\n{func_def}",
                },
                {
                    "role": "user",
                    "content": f"Return `{name}({call_args})`",
                },
            ],
        }
        return function_body


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/exceptions.py
=======
from __future__ import annotations

from typing import Any


class IncompleteOutputException(Exception):
    """Exception raised when the output from LLM is incomplete due to max tokens limit reached."""

    def __init__(
        self,
        *args: list[Any],
        last_completion: Any | None = None,
        message: str = "The output is incomplete due to a max_tokens length limit.",
        **kwargs: dict[str, Any],
    ):
        self.last_completion = last_completion
        super().__init__(message, *args, **kwargs)


class InstructorRetryException(Exception):
    def __init__(
        self,
        *args: list[Any],
        last_completion: Any | None = None,
        messages: list[Any] | None = None,
        n_attempts: int,
        total_usage: int,
        **kwargs: dict[str, Any],
    ):
        self.last_completion = last_completion
        self.messages = messages
        self.n_attempts = n_attempts
        self.total_usage = total_usage
        super().__init__(*args, **kwargs)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/function_calls.py
=======
# type: ignore
import json
import logging
from functools import wraps
from typing import Annotated, Any, Optional, TypeVar, cast

from docstring_parser import parse
from openai.types.chat import ChatCompletion
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    TypeAdapter,
    create_model,
)

from instructor.exceptions import IncompleteOutputException
from instructor.mode import Mode
from instructor.utils import classproperty, extract_json_from_codeblock

T = TypeVar("T")

logger = logging.getLogger("instructor")


class OpenAISchema(BaseModel):
    # Ignore classproperty, since Pydantic doesn't understand it like it would a normal property.
    model_config = ConfigDict(ignored_types=(classproperty,))

    @classproperty
    def openai_schema(cls) -> dict[str, Any]:
        """
        Return the schema in the format of OpenAI's schema as jsonschema

        Note:
            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.

        Returns:
            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema
        """
        schema = cls.model_json_schema()
        docstring = parse(cls.__doc__ or "")
        parameters = {
            k: v for k, v in schema.items() if k not in ("title", "description")
        }
        for param in docstring.params:
            if (name := param.arg_name) in parameters["properties"] and (
                description := param.description
            ):
                if "description" not in parameters["properties"][name]:
                    parameters["properties"][name]["description"] = description

        parameters["required"] = sorted(
            k for k, v in parameters["properties"].items() if "default" not in v
        )

        if "description" not in schema:
            if docstring.short_description:
                schema["description"] = docstring.short_description
            else:
                schema["description"] = (
                    f"Correctly extracted `{cls.__name__}` with all "
                    f"the required parameters with correct types"
                )

        return {
            "name": schema["title"],
            "description": schema["description"],
            "parameters": parameters,
        }

    @classproperty
    def anthropic_schema(cls) -> dict[str, Any]:
        return {
            "name": cls.openai_schema["name"],
            "description": cls.openai_schema["description"],
            "input_schema": cls.model_json_schema(),
        }

    @classmethod
    def from_response(
        cls,
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
        mode: Mode = Mode.TOOLS,
    ) -> BaseModel:
        """Execute the function from the response of an openai chat completion

        Parameters:
            completion (openai.ChatCompletion): The response from an openai chat completion
            throw_error (bool): Whether to throw an error if the function call is not detected
            validation_context (dict): The validation context to use for validating the response
            strict (bool): Whether to use strict json parsing
            mode (Mode): The openai completion mode

        Returns:
            cls (OpenAISchema): An instance of the class
        """
        if mode == Mode.ANTHROPIC_TOOLS:
            return cls.parse_anthropic_tools(completion, validation_context, strict)

        if mode == Mode.ANTHROPIC_JSON:
            return cls.parse_anthropic_json(completion, validation_context, strict)

        if mode == Mode.VERTEXAI_TOOLS:
            return cls.parse_vertexai_tools(completion, validation_context, strict)

        if mode == Mode.VERTEXAI_JSON:
            return cls.parse_vertexai_json(completion, validation_context, strict)

        if mode == Mode.COHERE_TOOLS:
            return cls.parse_cohere_tools(completion, validation_context, strict)

        if mode == Mode.GEMINI_JSON:
            return cls.parse_gemini_json(completion, validation_context, strict)

        if completion.choices[0].finish_reason == "length":
            raise IncompleteOutputException(last_completion=completion)

        if mode == Mode.FUNCTIONS:
            return cls.parse_functions(completion, validation_context, strict)

        if mode in {Mode.TOOLS, Mode.MISTRAL_TOOLS}:
            return cls.parse_tools(completion, validation_context, strict)

        if mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON}:
            return cls.parse_json(completion, validation_context, strict)

        raise ValueError(f"Invalid patch mode: {mode}")

    @classmethod
    def parse_anthropic_tools(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        tool_calls = [c.input for c in completion.content if c.type == "tool_use"]  # type: ignore - TODO update with anthropic specific types

        tool_calls_validator = TypeAdapter(
            Annotated[list[Any], Field(min_length=1, max_length=1)]
        )
        tool_call = tool_calls_validator.validate_python(tool_calls)[0]

        return cls.model_validate(tool_call, context=validation_context, strict=strict)

    @classmethod
    def parse_anthropic_json(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        from anthropic.types import Message

        assert isinstance(completion, Message)

        text = completion.content[0].text
        extra_text = extract_json_from_codeblock(text)

        if strict:
            return cls.model_validate_json(
                extra_text, context=validation_context, strict=True
            )
        else:
            # Allow control characters.
            parsed = json.loads(extra_text, strict=False)
            # Pydantic non-strict: https://docs.pydantic.dev/latest/concepts/strict_mode/
            return cls.model_validate(parsed, context=validation_context, strict=False)

    @classmethod
    def parse_gemini_json(
        cls: type[BaseModel],
        completion: Any,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        try:
            text = completion.text
        except ValueError:
            logger.debug(
                f"Error response: {completion.result.candidates[0].finish_reason}\n\n{completion.result.candidates[0].safety_ratings}"
            )

        try:
            extra_text = extract_json_from_codeblock(text)  # type: ignore
        except UnboundLocalError:
            raise ValueError("Unable to extract JSON from completion text") from None

        if strict:
            return cls.model_validate_json(
                extra_text, context=validation_context, strict=True
            )
        else:
            # Allow control characters.
            parsed = json.loads(extra_text, strict=False)
            # Pydantic non-strict: https://docs.pydantic.dev/latest/concepts/strict_mode/
            return cls.model_validate(parsed, context=validation_context, strict=False)

    @classmethod
    def parse_vertexai_tools(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        strict = False
        tool_call = completion.candidates[0].content.parts[0].function_call.args  # type: ignore
        model = {}
        for field in tool_call:  # type: ignore
            model[field] = tool_call[field]
        return cls.model_validate(model, context=validation_context, strict=strict)

    @classmethod
    def parse_vertexai_json(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        model = json.loads(completion.text)
        return cls.model_validate(model, context=validation_context, strict=strict)

    @classmethod
    def parse_cohere_tools(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        text = cast(str, completion.text)  # type: ignore - TODO update with cohere specific types
        extra_text = extract_json_from_codeblock(text)
        return cls.model_validate_json(
            extra_text, context=validation_context, strict=strict
        )

    @classmethod
    def parse_functions(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        message = completion.choices[0].message
        assert (
            message.function_call.name == cls.openai_schema["name"]  # type: ignore[index]
        ), "Function name does not match"
        return cls.model_validate_json(
            message.function_call.arguments,  # type: ignore[attr-defined]
            context=validation_context,
            strict=strict,
        )

    @classmethod
    def parse_tools(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        message = completion.choices[0].message
        assert (
            len(message.tool_calls or []) == 1
        ), "Instructor does not support multiple tool calls, use List[Model] instead."
        tool_call = message.tool_calls[0]  # type: ignore
        assert (
            tool_call.function.name == cls.openai_schema["name"]  # type: ignore[index]
        ), "Tool name does not match"
        return cls.model_validate_json(
            tool_call.function.arguments,  # type: ignore
            context=validation_context,
            strict=strict,
        )

    @classmethod
    def parse_json(
        cls: type[BaseModel],
        completion: ChatCompletion,
        validation_context: Optional[dict[str, Any]] = None,
        strict: Optional[bool] = None,
    ) -> BaseModel:
        message = completion.choices[0].message.content or ""
        message = extract_json_from_codeblock(message)

        return cls.model_validate_json(
            message,
            context=validation_context,
            strict=strict,
        )


def openai_schema(cls: type[BaseModel]) -> OpenAISchema:
    if not issubclass(cls, BaseModel):
        raise TypeError("Class must be a subclass of pydantic.BaseModel")

    shema = wraps(cls, updated=())(
        create_model(
            cls.__name__ if hasattr(cls, "__name__") else str(cls),
            __base__=(cls, OpenAISchema),
        )
    )
    return cast(OpenAISchema, shema)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/mode.py
=======
import enum
import warnings


class _WarnOnFunctionsAccessEnumMeta(enum.EnumMeta):
    def __getattribute__(cls, name: str):
        if name == "FUNCTIONS":
            warnings.warn(
                "FUNCTIONS is deprecated and will be removed in future versions",
                DeprecationWarning,
                stacklevel=2,
            )
        return super().__getattribute__(name)


class Mode(enum.Enum, metaclass=_WarnOnFunctionsAccessEnumMeta):
    """The mode to use for patching the client"""

    FUNCTIONS = "function_call"
    PARALLEL_TOOLS = "parallel_tool_call"
    TOOLS = "tool_call"
    MISTRAL_TOOLS = "mistral_tools"
    JSON = "json_mode"
    MD_JSON = "markdown_json_mode"
    JSON_SCHEMA = "json_schema_mode"
    ANTHROPIC_TOOLS = "anthropic_tools"
    ANTHROPIC_JSON = "anthropic_json"
    COHERE_TOOLS = "cohere_tools"
    VERTEXAI_TOOLS = "vertexai_tools"
    VERTEXAI_JSON = "vertexai_json"
    GEMINI_JSON = "gemini_json"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/patch.py
=======
# type: ignore[all]
from functools import wraps
from typing import (
    Callable,
    Protocol,
    TypeVar,
    Union,
    overload,
)
from collections.abc import Awaitable
from typing_extensions import ParamSpec

from openai import AsyncOpenAI, OpenAI
from pydantic import BaseModel

from instructor.process_response import handle_response_model
from instructor.retry import retry_async, retry_sync
from instructor.utils import is_async

from instructor.mode import Mode
import logging

logger = logging.getLogger("instructor")

T_Model = TypeVar("T_Model", bound=BaseModel)
T_Retval = TypeVar("T_Retval")
T_ParamSpec = ParamSpec("T_ParamSpec")


class InstructorChatCompletionCreate(Protocol):
    def __call__(
        self,
        response_model: type[T_Model] = None,
        validation_context: dict = None,
        max_retries: int = 1,
        *args: T_ParamSpec.args,
        **kwargs: T_ParamSpec.kwargs,
    ) -> T_Model: ...


class AsyncInstructorChatCompletionCreate(Protocol):
    async def __call__(
        self,
        response_model: type[T_Model] = None,
        validation_context: dict = None,
        max_retries: int = 1,
        *args: T_ParamSpec.args,
        **kwargs: T_ParamSpec.kwargs,
    ) -> T_Model: ...


@overload
def patch(
    client: OpenAI,
    mode: Mode = Mode.TOOLS,
) -> OpenAI: ...


@overload
def patch(
    client: AsyncOpenAI,
    mode: Mode = Mode.TOOLS,
) -> AsyncOpenAI: ...


@overload
def patch(
    create: Callable[T_ParamSpec, T_Retval],
    mode: Mode = Mode.TOOLS,
) -> InstructorChatCompletionCreate: ...


@overload
def patch(
    create: Awaitable[T_Retval],
    mode: Mode = Mode.TOOLS,
) -> InstructorChatCompletionCreate: ...


def patch(
    client: Union[OpenAI, AsyncOpenAI] = None,
    create: Callable[T_ParamSpec, T_Retval] = None,
    mode: Mode = Mode.TOOLS,
) -> Union[OpenAI, AsyncOpenAI]:
    """
    Patch the `client.chat.completions.create` method

    Enables the following features:

    - `response_model` parameter to parse the response from OpenAI's API
    - `max_retries` parameter to retry the function if the response is not valid
    - `validation_context` parameter to validate the response using the pydantic model
    - `strict` parameter to use strict json parsing
    """

    logger.debug(f"Patching `client.chat.completions.create` with {mode=}")

    if create is not None:
        func = create
    elif client is not None:
        func = client.chat.completions.create
    else:
        raise ValueError("Either client or create must be provided")

    func_is_async = is_async(func)

    @wraps(func)
    async def new_create_async(
        response_model: type[T_Model] = None,
        validation_context: dict = None,
        max_retries: int = 1,
        strict: bool = True,
        *args: T_ParamSpec.args,
        **kwargs: T_ParamSpec.kwargs,
    ) -> T_Model:
        response_model, new_kwargs = handle_response_model(
            response_model=response_model, mode=mode, **kwargs
        )
        response = await retry_async(
            func=func,
            response_model=response_model,
            validation_context=validation_context,
            max_retries=max_retries,
            args=args,
            kwargs=new_kwargs,
            strict=strict,
            mode=mode,
        )
        return response

    @wraps(func)
    def new_create_sync(
        response_model: type[T_Model] = None,
        validation_context: dict = None,
        max_retries: int = 1,
        strict: bool = True,
        *args: T_ParamSpec.args,
        **kwargs: T_ParamSpec.kwargs,
    ) -> T_Model:
        response_model, new_kwargs = handle_response_model(
            response_model=response_model, mode=mode, **kwargs
        )
        response = retry_sync(
            func=func,
            response_model=response_model,
            validation_context=validation_context,
            max_retries=max_retries,
            args=args,
            strict=strict,
            kwargs=new_kwargs,
            mode=mode,
        )
        return response

    new_create = new_create_async if func_is_async else new_create_sync

    if client is not None:
        client.chat.completions.create = new_create
        return client
    else:
        return new_create


def apatch(client: AsyncOpenAI, mode: Mode = Mode.TOOLS) -> AsyncOpenAI:
    """
    No longer necessary, use `patch` instead.

    Patch the `client.chat.completions.create` method

    Enables the following features:

    - `response_model` parameter to parse the response from OpenAI's API
    - `max_retries` parameter to retry the function if the response is not valid
    - `validation_context` parameter to validate the response using the pydantic model
    - `strict` parameter to use strict json parsing
    """
    import warnings

    warnings.warn(
        "apatch is deprecated, use patch instead", DeprecationWarning, stacklevel=2
    )
    return patch(client, mode=mode)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/process_response.py
=======
# type: ignore[all]
from __future__ import annotations

from collections.abc import Iterable
from textwrap import dedent
from instructor.dsl.iterable import IterableBase, IterableModel
from instructor.dsl.parallel import ParallelBase, ParallelModel, handle_parallel_model
from instructor.dsl.partial import PartialBase
from instructor.dsl.simple_type import AdapterBase, ModelAdapter, is_simple_type
from instructor.function_calls import OpenAISchema, openai_schema
from instructor.utils import merge_consecutive_messages
from openai.types.chat import ChatCompletion
from pydantic import BaseModel, create_model

import json
import inspect
import logging
from typing import (
    get_args,
    get_origin,
    TypeVar,
    Any,
)
from collections.abc import Generator
from typing_extensions import ParamSpec

from instructor.mode import Mode

from .utils import transform_to_gemini_prompt

logger = logging.getLogger("instructor")

T_Model = TypeVar("T_Model", bound=BaseModel)
T_Retval = TypeVar("T_Retval")
T_ParamSpec = ParamSpec("T_ParamSpec")
T = TypeVar("T")


async def process_response_async(
    response: ChatCompletion,
    *,
    response_model: type[T_Model | OpenAISchema | BaseModel] | None,
    stream: bool = False,
    validation_context: dict[str, Any] | None = None,
    strict: bool | None = None,
    mode: Mode = Mode.TOOLS,
) -> T_Model | ChatCompletion:
    """Processes a OpenAI response with the response model, if available.
    It can use `validation_context` and `strict` to validate the response
    via the pydantic model

    Args:
        response (ChatCompletion): The response from OpenAI's API
        response_model (BaseModel): The response model to use for parsing the response
        stream (bool): Whether the response is a stream
        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.
        strict (bool, optional): Whether to use strict json parsing. Defaults to None.
    """

    logger.debug(
        f"Instructor Raw Response: {response}",
    )
    if response_model is None:
        return response

    if (
        inspect.isclass(response_model)
        and issubclass(response_model, (IterableBase, PartialBase))
        and stream
    ):
        model = await response_model.from_streaming_response_async(
            response,
            mode=mode,
        )
        return model

    model = response_model.from_response(
        response,
        validation_context=validation_context,
        strict=strict,
        mode=mode,
    )

    # ? This really hints at the fact that we need a better way of
    # ? attaching usage data and the raw response to the model we return.
    if isinstance(model, IterableBase):
        logger.debug(f"Returning takes from IterableBase")
        return [task for task in model.tasks]

    if isinstance(response_model, ParallelBase):
        logger.debug(f"Returning model from ParallelBase")
        return model

    if isinstance(model, AdapterBase):
        logger.debug(f"Returning model from AdapterBase")
        return model.content

    model._raw_response = response
    return model


def process_response(
    response: T_Model,
    *,
    response_model: type[OpenAISchema | BaseModel],
    stream: bool,
    validation_context: dict | None = None,
    strict=None,
    mode: Mode = Mode.TOOLS,
) -> T_Model | Generator[T_Model, None, None] | ChatCompletion:
    """Processes a OpenAI response with the response model, if available.

    Args:
        response (T): The response from OpenAI's API
        response_model (Type[T_Model]): The response model to use for parsing the response
        stream (bool): Whether the response is a stream
        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.
        strict (_type_, optional): Whether to use strict json parsing. Defaults to None.
        mode (Mode, optional): The openai completion mode. Defaults to Mode.FUNCTIONS.

    Returns:
        Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK
    """

    logger.debug(
        f"Instructor Raw Response: {response}",
    )

    if response_model is None:
        logger.debug("No response model, returning response as is")
        return response

    if (
        inspect.isclass(response_model)
        and issubclass(response_model, (IterableBase, PartialBase))
        and stream
    ):
        model = response_model.from_streaming_response(
            response,
            mode=mode,
        )
        return model

    model = response_model.from_response(
        response,
        validation_context=validation_context,
        strict=strict,
        mode=mode,
    )

    # ? This really hints at the fact that we need a better way of
    # ? attaching usage data and the raw response to the model we return.
    if isinstance(model, IterableBase):
        logger.debug(f"Returning takes from IterableBase")
        return [task for task in model.tasks]

    if isinstance(response_model, ParallelBase):
        logger.debug(f"Returning model from ParallelBase")
        return model

    if isinstance(model, AdapterBase):
        logger.debug(f"Returning model from AdapterBase")
        return model.content

    model._raw_response = response
    return model


def is_typed_dict(cls) -> bool:
    return (
        isinstance(cls, type)
        and issubclass(cls, dict)
        and hasattr(cls, "__annotations__")
    )


def handle_response_model(
    response_model: type[T] | None, mode: Mode = Mode.TOOLS, **kwargs: Any
) -> tuple[type[T], dict[str, Any]]:
    """Prepare the response model type hint, and returns the response_model
    along with the new modified kwargs needed to be able to use the response_model
    parameter with the patch function.


    Args:
        response_model (T): The response model to use for parsing the response
        mode (Mode, optional): The openai completion mode. Defaults to Mode.TOOLS.

    Raises:
        NotImplementedError: When using stream=True with a non-iterable response_model
        ValueError: When using an invalid patch mode

    Returns:
        Union[Type[OpenAISchema], dict]: The response model to use for parsing the response
    """
    new_kwargs = kwargs.copy()
    if response_model is not None:
        # Handles the case where the response_model is a simple type
        # Literal, Annotated, Union, str, int, float, bool, Enum
        # We wrap the response_model in a ModelAdapter that sets 'content' as the response
        if is_simple_type(response_model):
            response_model = ModelAdapter[response_model]

        if is_typed_dict(response_model):
            response_model: BaseModel = create_model(
                response_model.__name__,
                **{k: (v, ...) for k, v in response_model.__annotations__.items()},
            )

        # This a special case for parallel tools
        if mode == Mode.PARALLEL_TOOLS:
            assert (
                new_kwargs.get("stream", False) is False
            ), "stream=True is not supported when using PARALLEL_TOOLS mode"
            new_kwargs["tools"] = handle_parallel_model(response_model)
            new_kwargs["tool_choice"] = "auto"

            # This is a special case for parallel models
            response_model = ParallelModel(typehint=response_model)
            return response_model, new_kwargs

        # This is for all other single model cases
        if get_origin(response_model) is Iterable:
            iterable_element_class = get_args(response_model)[0]
            response_model = IterableModel(iterable_element_class)
        if not issubclass(response_model, OpenAISchema):
            response_model = openai_schema(response_model)  # type: ignore

        if new_kwargs.get("stream", False) and not issubclass(
            response_model, (IterableBase, PartialBase)
        ):
            raise NotImplementedError(
                "stream=True is not supported when using response_model parameter for non-iterables"
            )

        if mode == Mode.FUNCTIONS:
            new_kwargs["functions"] = [response_model.openai_schema]
            new_kwargs["function_call"] = {"name": response_model.openai_schema["name"]}
        elif mode in {Mode.TOOLS, Mode.MISTRAL_TOOLS}:
            new_kwargs["tools"] = [
                {
                    "type": "function",
                    "function": response_model.openai_schema,
                }
            ]
            if mode == Mode.MISTRAL_TOOLS:
                new_kwargs["tool_choice"] = "any"
            else:
                new_kwargs["tool_choice"] = {
                    "type": "function",
                    "function": {"name": response_model.openai_schema["name"]},
                }
        elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:
            # If its a JSON Mode we need to massage the prompt a bit
            # in order to get the response we want in a json format
            message = dedent(
                f"""
                As a genius expert, your task is to understand the content and provide
                the parsed objects in json that match the following json_schema:\n

                {json.dumps(response_model.model_json_schema(), indent=2)}

                Make sure to return an instance of the JSON, not the schema itself
                """
            )

            if mode == Mode.JSON:
                new_kwargs["response_format"] = {"type": "json_object"}

            elif mode == Mode.JSON_SCHEMA:
                new_kwargs["response_format"] = {
                    "type": "json_object",
                    "schema": response_model.model_json_schema(),
                }

            elif mode == Mode.MD_JSON:
                new_kwargs["messages"].append(
                    {
                        "role": "user",
                        "content": "Return the correct JSON response within a ```json codeblock. not the JSON_SCHEMA",
                    },
                )
                # For some providers, the messages array must be alternating roles of user and assistant, we must merge
                # consecutive user messages into a single message
                new_kwargs["messages"] = merge_consecutive_messages(
                    new_kwargs["messages"]
                )
            # check that the first message is a system message
            # if it is not, add a system message to the beginning
            if new_kwargs["messages"][0]["role"] != "system":
                new_kwargs["messages"].insert(
                    0,
                    {
                        "role": "system",
                        "content": message,
                    },
                )
            # if it is, system append the schema to the end
            else:
                new_kwargs["messages"][0]["content"] += f"\n\n{message}"
        elif mode == Mode.ANTHROPIC_TOOLS:
            tool_descriptions = response_model.anthropic_schema
            new_kwargs["tools"] = [tool_descriptions]
            new_kwargs["tool_choice"] = {
                "type": "tool",
                "name": response_model.__name__,
            }

            system_messages = [
                m["content"] for m in new_kwargs["messages"] if m["role"] == "system"
            ]
            new_kwargs["system"] = "\n\n".join(system_messages)
            new_kwargs["messages"] = [
                m for m in new_kwargs["messages"] if m["role"] != "system"
            ]

        elif mode == Mode.ANTHROPIC_JSON:
            # anthropic wants system message to be a string so we first extract out any system message
            openai_system_messages = [
                message["content"]
                for message in new_kwargs.get("messages", [])
                if message["role"] == "system"
            ]

            new_kwargs["system"] = (
                new_kwargs.get("system", "")
                + "\n\n"
                + "\n\n".join(openai_system_messages)
            )

            new_kwargs["system"] += f"""
            You must only response in JSON format that adheres to the following schema:

            <JSON_SCHEMA>
            {json.dumps(response_model.model_json_schema(), indent=2)}
            </JSON_SCHEMA>
            """
            new_kwargs["system"] = dedent(new_kwargs["system"])

            new_kwargs["messages"] = [
                message
                for message in new_kwargs.get("messages", [])
                if message["role"] != "system"
            ]

            # the messages array must be alternating roles of user and assistant, we must merge
            # consecutive user messages into a single message
            new_kwargs["messages"] = merge_consecutive_messages(new_kwargs["messages"])

        elif mode == Mode.COHERE_TOOLS:
            instruction = f"""\
Extract a valid {response_model.__name__} object based on the chat history and the json schema below.
{response_model.model_json_schema()}
The JSON schema was obtained by running:
```python
schema = {response_model.__name__}.model_json_schema()
```

The output must be a valid JSON object that `{response_model.__name__}.model_validate_json()` can successfully parse.
"""
            messages = new_kwargs.pop("messages", [])
            chat_history = []
            for message in messages:
                # format in Cohere's ChatMessage format
                chat_history.append(
                    {
                        "role": message["role"],
                        "message": message["content"],
                    }
                )
            new_kwargs["message"] = instruction
            new_kwargs["chat_history"] = chat_history
        elif mode == Mode.GEMINI_JSON:
            assert (
                "model" not in new_kwargs
            ), "Gemini `model` must be set while patching the client, not passed as a parameter to the create method"
            message = dedent(
                f"""
                As a genius expert, your task is to understand the content and provide
                the parsed objects in json that match the following json_schema:\n

                {json.dumps(response_model.model_json_schema(), indent=2)}

                Make sure to return an instance of the JSON, not the schema itself
                """
            )
            # check that the first message is a system message
            # if it is not, add a system message to the beginning
            if new_kwargs["messages"][0]["role"] != "system":
                new_kwargs["messages"].insert(
                    0,
                    {
                        "role": "system",
                        "content": message,
                    },
                )
            # if it is, system append the schema to the end
            else:
                new_kwargs["messages"][0]["content"] += f"\n\n{message}"

            # default to json response type
            new_kwargs["generation_config"] = new_kwargs.get(
                "generation_config", {}
            ) | {"response_mime_type": "application/json"}

            map_openai_args_to_gemini = {
                "max_tokens": "max_output_tokens",
                "temperature": "temperature",
                "n": "candidate_count",
                "top_p": "top_p",
                "stop": "stop_sequences",
            }

            # update gemini config if any params are set
            for k, v in map_openai_args_to_gemini.items():
                val = new_kwargs.pop(k, None)
                if val == None:
                    continue
                new_kwargs["generation_config"][v] = val

            # gemini has a different prompt format and params from other providers
            new_kwargs["contents"] = transform_to_gemini_prompt(
                new_kwargs.pop("messages")
            )

            # minimize gemini safety related errors - model is highly prone to false alarms
            from google.generativeai.types import HarmCategory, HarmBlockThreshold

            new_kwargs["safety_settings"] = new_kwargs.get("safety_settings", {}) | {
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            }
        elif mode == Mode.VERTEXAI_TOOLS:
            from instructor.client_vertexai import vertexai_process_response

            contents, tools, tool_config = vertexai_process_response(
                new_kwargs, response_model
            )

            new_kwargs["contents"] = contents
            new_kwargs["tools"] = tools
            new_kwargs["tool_config"] = tool_config
        elif mode == Mode.VERTEXAI_JSON:
            from instructor.client_vertexai import vertexai_process_json_response

            contents, generation_config = vertexai_process_json_response(
                new_kwargs, response_model
            )

            new_kwargs["contents"] = contents
            new_kwargs["generation_config"] = generation_config
        else:
            raise ValueError(f"Invalid patch mode: {mode}")

    logger.debug(
        f"Instructor Request: {mode.value=}, {response_model=}, {new_kwargs=}",
        extra={
            "mode": mode.value,
            "response_model": (
                response_model.__name__
                if response_model is not None and hasattr(response_model, "__name__")
                else str(response_model)
            ),
            "new_kwargs": new_kwargs,
        },
    )
    return response_model, new_kwargs


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/retry.py
=======
# type: ignore[all]
from __future__ import annotations

import logging

from openai.types.chat import ChatCompletion
from instructor.mode import Mode
from instructor.process_response import process_response, process_response_async
from instructor.utils import (
    dump_message,
    update_total_usage,
    merge_consecutive_messages,
)
from instructor.exceptions import InstructorRetryException

from openai.types.completion_usage import CompletionUsage
from pydantic import ValidationError
from tenacity import AsyncRetrying, RetryError, Retrying, stop_after_attempt


from json import JSONDecodeError
from pydantic import BaseModel
from typing import Callable, TypeVar, Any
from typing_extensions import ParamSpec

logger = logging.getLogger("instructor")

T_Model = TypeVar("T_Model", bound=BaseModel)
T_Retval = TypeVar("T_Retval")
T_ParamSpec = ParamSpec("T_ParamSpec")
T = TypeVar("T")


def reask_messages(response: ChatCompletion, mode: Mode, exception: Exception):
    if mode == Mode.ANTHROPIC_TOOLS:
        # The original response
        assistant_content = []
        tool_use_id = None
        for content in response.content:
            assistant_content.append(content.model_dump())
            # Assuming exception from single tool invocation
            if (
                content.type == "tool_use"
                and isinstance(exception, ValidationError)
                and content.name == exception.title
            ):
                tool_use_id = content.id

        yield {
            "role": "assistant",
            "content": assistant_content,
        }
        if tool_use_id is not None:
            yield {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": f"Validation Error found:\n{exception}\nRecall the function correctly, fix the errors",
                        "is_error": True,
                    }
                ],
            }
        else:
            yield {
                "role": "user",
                "content": f"Validation Error due to no tool invocation:\n{exception}\nRecall the function correctly, fix the errors",
            }
        return
    if mode == Mode.ANTHROPIC_JSON:
        from anthropic.types import Message

        assert isinstance(response, Message)
        yield {
            "role": "user",
            "content": f"""Validation Errors found:\n{exception}\nRecall the function correctly, fix the errors found in the following attempt:\n{response.content[0].text}""",
        }
        return
    if mode == Mode.COHERE_TOOLS:
        yield {
            "role": "user",
            "message": f"Validation Error found:\n{exception}\nRecall the function correctly, fix the errors",
        }
        return
    if mode == Mode.GEMINI_JSON:
        yield {
            "role": "user",
            "parts": [
                f"Correct the following JSON response, based on the errors given below:\n\nJSON:\n{response.text}\n\nExceptions:\n{exception}"
            ],
        }
        return
    if mode == Mode.VERTEXAI_TOOLS:
        from .client_vertexai import vertexai_function_response_parser

        yield response.candidates[0].content
        yield vertexai_function_response_parser(response, exception)
        return
    if mode == Mode.VERTEXAI_JSON:
        from .client_vertexai import vertexai_message_parser

        yield response.candidates[0].content
        yield vertexai_message_parser(
            {
                "role": "user",
                "content": f"Validation Errors found:\n{exception}\nRecall the function correctly, fix the errors found in the following attempt:\n{response.text}",
            }
        )
        return

    yield dump_message(response.choices[0].message)
    # TODO: Give users more control on configuration
    if mode == Mode.TOOLS:
        for tool_call in response.choices[0].message.tool_calls:
            yield {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": tool_call.function.name,
                "content": f"Validation Error found:\n{exception}\nRecall the function correctly, fix the errors",
            }
    elif mode == Mode.MD_JSON:
        yield {
            "role": "user",
            "content": f"Correct your JSON ONLY RESPONSE, based on the following errors:\n{exception}",
        }
    else:
        yield {
            "role": "user",
            "content": f"Recall the function correctly, fix the errors, exceptions found\n{exception}",
        }


def retry_sync(
    func: Callable[T_ParamSpec, T_Retval],
    response_model: type[T_Model],
    validation_context: dict,
    args,
    kwargs,
    max_retries: int | Retrying = 1,
    strict: bool | None = None,
    mode: Mode = Mode.TOOLS,
) -> T_Model:
    total_usage = CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0)
    if mode in {Mode.ANTHROPIC_TOOLS, Mode.ANTHROPIC_JSON}:
        from anthropic.types import Usage as AnthropicUsage

        total_usage = AnthropicUsage(input_tokens=0, output_tokens=0)

    # If max_retries is int, then create a Retrying object
    if isinstance(max_retries, int):
        logger.debug(f"max_retries: {max_retries}")
        max_retries = Retrying(
            stop=stop_after_attempt(max_retries),
            reraise=True,
        )
    if not isinstance(max_retries, (Retrying, AsyncRetrying)):
        raise ValueError("max_retries must be an int or a `tenacity.Retrying` object")

    try:
        response = None
        for attempt in max_retries:
            with attempt:
                try:
                    response = func(*args, **kwargs)
                    stream = kwargs.get("stream", False)
                    response = update_total_usage(response, total_usage)
                    return process_response(
                        response,
                        response_model=response_model,
                        stream=stream,
                        validation_context=validation_context,
                        strict=strict,
                        mode=mode,
                    )
                except (ValidationError, JSONDecodeError) as e:
                    logger.debug(f"Error response: {response}")
                    if mode in {
                        Mode.GEMINI_JSON,
                        Mode.VERTEXAI_TOOLS,
                        Mode.VERTEXAI_JSON,
                    }:
                        kwargs["contents"].extend(reask_messages(response, mode, e))
                    elif mode in {Mode.COHERE_TOOLS}:
                        kwargs["chat_history"].extend(reask_messages(response, mode, e))
                    else:
                        kwargs["messages"].extend(reask_messages(response, mode, e))
                    if mode in {Mode.ANTHROPIC_TOOLS, Mode.ANTHROPIC_JSON}:
                        kwargs["messages"] = merge_consecutive_messages(
                            kwargs["messages"]
                        )
                    raise e
    except RetryError as e:
        raise InstructorRetryException(
            e,
            last_completion=response,
            n_attempts=attempt.retry_state.attempt_number,
            messages=kwargs.get("messages", kwargs.get("contents")),
            total_usage=total_usage,
        ) from e


async def retry_async(
    func: Callable[T_ParamSpec, T_Retval],
    response_model: type[T] | None,
    validation_context: dict[str, Any] | None,
    args: Any,
    kwargs: Any,
    max_retries: int | AsyncRetrying = 1,
    strict: bool | None = None,
    mode: Mode = Mode.TOOLS,
) -> T:
    total_usage = CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0)
    if mode in {Mode.ANTHROPIC_TOOLS, Mode.ANTHROPIC_JSON}:
        from anthropic.types import Usage as AnthropicUsage

        total_usage = AnthropicUsage(input_tokens=0, output_tokens=0)

    # If max_retries is int, then create a AsyncRetrying object
    if isinstance(max_retries, int):
        logger.debug(f"max_retries: {max_retries}")
        max_retries = AsyncRetrying(
            stop=stop_after_attempt(max_retries),
            reraise=True,
        )
    if not isinstance(max_retries, (AsyncRetrying, Retrying)):
        raise ValueError(
            "max_retries must be an `int` or a `tenacity.AsyncRetrying` object"
        )

    try:
        response = None
        async for attempt in max_retries:
            logger.debug(f"Retrying, attempt: {attempt}")
            with attempt:
                try:
                    response: ChatCompletion = await func(*args, **kwargs)
                    stream = kwargs.get("stream", False)
                    response = update_total_usage(response, total_usage)
                    return await process_response_async(
                        response,
                        response_model=response_model,
                        stream=stream,
                        validation_context=validation_context,
                        strict=strict,
                        mode=mode,
                    )
                except (ValidationError, JSONDecodeError) as e:
                    logger.debug(f"Error response: {response}", e)
                    kwargs["messages"].extend(reask_messages(response, mode, e))
                    if mode in {Mode.ANTHROPIC_TOOLS, Mode.ANTHROPIC_JSON}:
                        kwargs["messages"] = merge_consecutive_messages(
                            kwargs["messages"]
                        )
                    raise e
    except RetryError as e:
        logger.exception(f"Failed after retries: {e.last_attempt.exception}")
        raise InstructorRetryException(
            e,
            last_completion=response,
            n_attempts=attempt.retry_state.attempt_number,
            messages=kwargs["messages"],
            total_usage=total_usage,
        ) from e


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/utils.py
=======
from __future__ import annotations

import inspect
import json
import logging
from collections.abc import AsyncGenerator, Generator, Iterable
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Generic,
    Protocol,
    TypeVar,
)
import os

from openai.types import CompletionUsage as OpenAIUsage
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessage,
    ChatCompletionMessageParam,
)

if TYPE_CHECKING:
    from anthropic.types import Usage as AnthropicUsage


logger = logging.getLogger("instructor")
R_co = TypeVar("R_co", covariant=True)
T_Model = TypeVar("T_Model", bound="Response")

from enum import Enum


class Response(Protocol):
    usage: OpenAIUsage | AnthropicUsage


class Provider(Enum):
    OPENAI = "openai"
    VERTEXAI = "vertexai"
    ANTHROPIC = "anthropic"
    ANYSCALE = "anyscale"
    TOGETHER = "together"
    GROQ = "groq"
    MISTRAL = "mistral"
    COHERE = "cohere"
    GEMINI = "gemini"
    DATABRICKS = "databricks"
    UNKNOWN = "unknown"


def get_provider(base_url: str) -> Provider:
    if "anyscale" in str(base_url):
        return Provider.ANYSCALE
    elif "together" in str(base_url):
        return Provider.TOGETHER
    elif "anthropic" in str(base_url):
        return Provider.ANTHROPIC
    elif "groq" in str(base_url):
        return Provider.GROQ
    elif "openai" in str(base_url):
        return Provider.OPENAI
    elif "mistral" in str(base_url):
        return Provider.MISTRAL
    elif "cohere" in str(base_url):
        return Provider.COHERE
    elif "gemini" in str(base_url):
        return Provider.GEMINI
    elif "databricks" in str(base_url):
        return Provider.DATABRICKS
    elif "vertexai" in str(base_url):
        return Provider.VERTEXAI
    return Provider.UNKNOWN


def extract_json_from_codeblock(content: str) -> str:
    first_paren = content.find("{")
    last_paren = content.rfind("}")
    return content[first_paren : last_paren + 1]


def extract_json_from_stream(chunks: Iterable[str]) -> Generator[str, None, None]:
    capturing = False
    brace_count = 0
    for chunk in chunks:
        for char in chunk:
            if char == "{":
                capturing = True
                brace_count += 1
                yield char
            elif char == "}" and capturing:
                brace_count -= 1
                yield char
                if brace_count == 0:
                    capturing = False
                    break  # Cease yielding upon closing the current JSON object
            elif capturing:
                yield char


async def extract_json_from_stream_async(
    chunks: AsyncGenerator[str, None],
) -> AsyncGenerator[str, None]:
    capturing = False
    brace_count = 0
    async for chunk in chunks:
        for char in chunk:
            if char == "{":
                capturing = True
                brace_count += 1
                yield char
            elif char == "}" and capturing:
                brace_count -= 1
                yield char
                if brace_count == 0:
                    capturing = False
                    break  # Cease yielding upon closing the current JSON object
            elif capturing:
                yield char


def update_total_usage(
    response: T_Model,
    total_usage: OpenAIUsage | AnthropicUsage,
) -> T_Model | ChatCompletion:
    response_usage = getattr(response, "usage", None)
    if isinstance(response_usage, OpenAIUsage) and isinstance(total_usage, OpenAIUsage):
        total_usage.completion_tokens += response_usage.completion_tokens or 0
        total_usage.prompt_tokens += response_usage.prompt_tokens or 0
        total_usage.total_tokens += response_usage.total_tokens or 0
        response.usage = total_usage  # Replace each response usage with the total usage
        return response

    # Anthropic usage.
    try:
        from anthropic.types import Usage as AnthropicUsage

        if isinstance(response_usage, AnthropicUsage) and isinstance(
            total_usage, AnthropicUsage
        ):
            total_usage.input_tokens += response_usage.input_tokens or 0
            total_usage.output_tokens += response_usage.output_tokens or 0
            response.usage = total_usage
            return response
    except ImportError:
        pass

    logger.debug("No compatible response.usage found, token usage not updated.")
    return response


def dump_message(message: ChatCompletionMessage) -> ChatCompletionMessageParam:
    """Dumps a message to a dict, to be returned to the OpenAI API.
    Workaround for an issue with the OpenAI API, where the `tool_calls` field isn't allowed to be present in requests
    if it isn't used.
    """
    ret: ChatCompletionMessageParam = {
        "role": message.role,
        "content": message.content or "",
    }
    if hasattr(message, "tool_calls") and message.tool_calls is not None:
        ret["tool_calls"] = message.model_dump()["tool_calls"]
    if (
        hasattr(message, "function_call")
        and message.function_call is not None
        and ret["content"]
    ):
        ret["content"] += json.dumps(message.model_dump()["function_call"])
    return ret


def is_async(func: Callable[..., Any]) -> bool:
    """Returns true if the callable is async, accounting for wrapped callables"""
    is_coroutine = inspect.iscoroutinefunction(func)
    while hasattr(func, "__wrapped__"):
        func = func.__wrapped__  # type: ignore - dynamic
        is_coroutine = is_coroutine or inspect.iscoroutinefunction(func)
    return is_coroutine


def merge_consecutive_messages(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    # merge all consecutive user messages into a single message
    new_messages: list[dict[str, Any]] = []
    # Detect whether all messages have a flat content (i.e. all string)
    # Some providers require content to be a string, so we need to check that and behave accordingly
    flat_string = all(isinstance(m["content"], str) for m in messages)
    for message in messages:
        new_content = message["content"]
        if not flat_string and isinstance(new_content, str):
            # If content is not flat, transform it into a list of text
            new_content = [{"type": "text", "text": new_content}]

        if len(new_messages) > 0 and message["role"] == new_messages[-1]["role"]:
            if flat_string:
                # New content is a string
                new_messages[-1]["content"] += f"\n\n{new_content}"
            else:
                # New content is a list
                new_messages[-1]["content"].extend(new_content)
        else:
            new_messages.append(
                {
                    "role": message["role"],
                    "content": new_content,
                }
            )

    return new_messages


class classproperty(Generic[R_co]):
    """Descriptor for class-level properties.

    Examples:
        >>> from instructor.utils import classproperty

        >>> class MyClass:
        ...     @classproperty
        ...     def my_property(cls):
        ...         return cls

        >>> assert MyClass.my_property
    """

    def __init__(self, method: Callable[[Any], R_co]) -> None:
        self.cproperty = method

    def __get__(self, instance: object, cls: type[Any]) -> R_co:
        return self.cproperty(cls)


def transform_to_gemini_prompt(
    messages_chatgpt: list[ChatCompletionMessageParam],
) -> list[dict[str, Any]]:
    messages_gemini: list[dict[str, Any]] = []
    system_prompt = ""
    for message in messages_chatgpt:
        if message["role"] == "system":
            system_prompt = message["content"]
        elif message["role"] == "user":
            messages_gemini.append(
                {"role": "user", "parts": [message.get("content", "")]}
            )
        elif message["role"] == "assistant":
            messages_gemini.append(
                {"role": "model", "parts": [message.get("content", "")]}
            )
    if system_prompt:
        messages_gemini[0]["parts"].insert(0, f"*{system_prompt}*")

    return messages_gemini


def disable_pydantic_error_url():
    os.environ["PYDANTIC_ERRORS_INCLUDE_URL"] = "0"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/__init__.py
=======
import importlib.util

from .mode import Mode
from .process_response import handle_response_model
from .distil import FinetuneFormat, Instructions
from .dsl import (
    CitationMixin,
    Maybe,
    Partial,
    IterableModel,
    llm_validator,
    openai_moderation,
)
from .function_calls import OpenAISchema, openai_schema
from .patch import apatch, patch
from .process_response import handle_parallel_model
from .client import (
    Instructor,
    AsyncInstructor,
    from_openai,
    from_litellm,
    Provider,
)


__all__ = [
    "Instructor",
    "from_openai",
    "from_litellm",
    "AsyncInstructor",
    "Provider",
    "OpenAISchema",
    "CitationMixin",
    "IterableModel",
    "Maybe",
    "Partial",
    "openai_schema",
    "Mode",
    "patch",
    "apatch",
    "llm_validator",
    "openai_moderation",
    "FinetuneFormat",
    "Instructions",
    "handle_parallel_model",
    "handle_response_model",
]


if importlib.util.find_spec("anthropic") is not None:
    from .client_anthropic import from_anthropic

    __all__ += ["from_anthropic"]

if (
    importlib.util.find_spec("google")
    and importlib.util.find_spec("google.generativeai") is not None
):
    from .client_gemini import from_gemini

    __all__ += ["from_gemini"]

if importlib.util.find_spec("groq") is not None:
    from .client_groq import from_groq

    __all__ += ["from_groq"]

if importlib.util.find_spec("mistralai") is not None:
    from .client_mistral import from_mistral

    __all__ += ["from_mistral"]

if importlib.util.find_spec("cohere") is not None:
    from .client_cohere import from_cohere

    __all__ += ["from_cohere"]

if importlib.util.find_spec("vertexai") is not None:
    from .client_vertexai import from_vertexai

    __all__ += ["from_vertexai"]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/batch.py
=======
from rich.console import Console
from rich.table import Table
from rich.live import Live
from openai import OpenAI
from openai.types.batch import Batch
import typer
import datetime
import time

client = OpenAI()
app = typer.Typer()

console = Console()


def generate_table(batch_jobs: list[Batch]):
    table = Table(
        title="OpenAI Batch Jobs",
    )

    table.add_column("Batch ID", style="dim")
    table.add_column("Created At")
    table.add_column("Status")
    table.add_column("Failed")
    table.add_column("Completed")
    table.add_column("Total")

    for batch_job in batch_jobs:
        table.add_row(
            batch_job.id,
            str(datetime.datetime.fromtimestamp(batch_job.created_at)),
            batch_job.status,
            str(batch_job.request_counts.failed),  # type: ignore
            str(batch_job.request_counts.completed),  # type: ignore
            str(batch_job.request_counts.total),  # type: ignore
        )

    return table


def get_jobs(limit: int = 10):
    return client.batches.list(limit=limit).data


@app.command(name="list", help="See all existing batch jobs")
def watch(
    limit: int = typer.Option(10, help="Total number of batch jobs to show"),
    poll: int = typer.Option(
        10, help="Time in seconds to wait for the batch job to complete"
    ),
    screen: bool = typer.Option(False, help="Enable or disable screen output"),
):
    """
    Monitor the status of the most recent batch jobs
    """
    batch_jobs = get_jobs(limit)
    table = generate_table(batch_jobs)
    with Live(
        generate_table(batch_jobs), refresh_per_second=2, screen=screen
    ) as live_table:
        while True:
            batch_jobs = get_jobs(limit)
            table = generate_table(batch_jobs)
            live_table.update(table)
            time.sleep(poll)


@app.command(
    help="Create a batch job from a file",
)
def create_from_file(
    file_path: str = typer.Option(help="File containing the batch job requests"),
):
    with console.status(f"[bold green] Uploading batch job file...", spinner="dots"):
        batch_input_file = client.files.create(
            file=open(file_path, "rb"), purpose="batch"
        )

    batch_input_file_id = batch_input_file.id

    with console.status(
        f"[bold green] Creating batch job from ID {batch_input_file_id}", spinner="dots"
    ):
        client.batches.create(
            input_file_id=batch_input_file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={"description": "testing job"},
        )

    watch(limit=5, poll=2, screen=False)


@app.command(help="Cancel a batch job")
def cancel(batch_id: str = typer.Option(help="Batch job ID to cancel")):
    try:
        client.batches.cancel(batch_id)
        watch(limit=5, poll=2, screen=False)
        console.log(f"[bold red]Job {batch_id} cancelled successfully!")
    except Exception as e:
        console.log(f"[bold red]Error cancelling job {batch_id}: {e}")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/cli.py
=======
import typer
import instructor.cli.jobs as jobs
import instructor.cli.files as files
import instructor.cli.usage as usage
import instructor.cli.hub as hub
import instructor.cli.batch as batch

app = typer.Typer()

app.add_typer(jobs.app, name="jobs", help="Monitor and create fine tuning jobs")
app.add_typer(files.app, name="files", help="Manage files on OpenAI's servers")
app.add_typer(usage.app, name="usage", help="Check OpenAI API usage data")
app.add_typer(hub.app, name="hub", help="Interact with the instructor hub")
app.add_typer(batch.app, name="batch", help="Manage OpenAI Batch jobs")


@app.command()
def docs(query: str = typer.Argument(None, help="Search the documentation")) -> None:
    """
    Open the instructor documentation website.
    """
    if query:
        typer.launch(f"https://python.useinstructor.com/?q={query}")
    else:
        typer.launch("https://python.useinstructor.com/")


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/files.py
=======
# type: ignore - stub mismatched

import time
from datetime import datetime
from typing import Literal, cast

import openai
import typer
from openai import OpenAI
from rich.console import Console
from rich.table import Table

client = OpenAI()
app = typer.Typer()
console = Console()


# Sample response data
def generate_file_table(files: list[openai.types.FileObject]) -> Table:
    table = Table(
        title="OpenAI Files",
    )
    table.add_column("File ID", style="dim")
    table.add_column("Size (bytes)", justify="right")
    table.add_column("Creation Time")
    table.add_column("Filename")
    table.add_column("Purpose")

    for file in files:
        table.add_row(
            file["id"],
            str(file["bytes"]),
            str(datetime.fromtimestamp(file["created_at"])),
            file["filename"],
            file["purpose"],
        )

    return table


def get_files() -> list[openai.types.FileObject]:
    files = client.files.list()
    files = files.data
    files = sorted(files, key=lambda x: x.created_at, reverse=True)
    return files


def get_file_status(file_id: str) -> str:
    response = client.files.retrieve(file_id)
    return response.status


@app.command(
    help="Upload a file to OpenAI's servers, will monitor the upload status until it is processed",
)
def upload(
    filepath: str = typer.Argument(help="Path to the file to upload"),
    purpose: str = typer.Option("fine-tune", help="Purpose of the file"),
    poll: int = typer.Option(5, help="Polling interval in seconds"),
) -> None:
    # Literals aren't supported by Typer yet.
    file_purpose = cast(Literal["fine-tune", "assistants"], purpose)
    with open(filepath, "rb") as file:
        response = client.files.create(file=file, purpose=file_purpose)
    file_id = response["id"]  # type: ignore - types might be out of date
    with console.status(f"Monitoring upload: {file_id}...") as status:
        status.spinner_style = "dots"
        while True:
            file_status = get_file_status(file_id)
            if file_status == "processed":
                console.log(f"[bold green]File {file_id} uploaded successfully!")
                break
            time.sleep(poll)


@app.command(
    help="Download a file from OpenAI's servers",
)
def download(
    file_id: str = typer.Argument(help="ID of the file to download"),
    output: str = typer.Argument(help="Output path for the downloaded file"),
) -> None:
    with console.status(f"[bold green]Downloading file {file_id}...", spinner="dots"):
        content = client.files.download(file_id)
        with open(output, "wb") as file:
            file.write(content)
        console.log(f"[bold green]File {file_id} downloaded successfully!")


@app.command(
    help="Delete a file from OpenAI's servers",
)
def delete(file_id: str = typer.Argument(help="ID of the file to delete")) -> None:
    with console.status(f"[bold red]Deleting file {file_id}...", spinner="dots"):
        try:
            client.files.delete(file_id)
            console.log(f"[bold red]File {file_id} deleted successfully!")
        except Exception as e:
            console.log(f"[bold red]Error deleting file {file_id}: {e}")
            return


@app.command(
    help="Monitor the status of a file on OpenAI's servers",
)
def status(
    file_id: str = typer.Argument(help="ID of the file to check the status of"),
) -> None:
    with console.status(f"Monitoring status of file {file_id}...") as status:
        while True:
            file_status = get_file_status(file_id)
            status.update(f"File status: {file_status}")
            if file_status in ["pending", "processed"]:
                break
            time.sleep(5)


@app.command(
    help="List the files on OpenAI's servers",
)
def list() -> None:
    files = get_files()
    console.log(generate_file_table(files))


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/hub.py
=======
from typing import Optional

import typer
import httpx

from pydantic import BaseModel
from rich.console import Console
from rich.table import Table
from rich.markdown import Markdown

app = typer.Typer(
    name="hub",
    help="Interact with the instructor hub, a collection of examples and cookbooks for the instructor library.",
    short_help="Interact with the instructor hub",
)
console = Console()


class HubPage(BaseModel):
    id: int
    name: str
    slug: str
    branch: str = "main"
    count: int = 0

    def get_doc_url(self) -> str:
        return f"https://jxnl.github.io/instructor/hub/{self.slug}/"

    def get_md_url(self) -> str:
        return f"https://raw.githubusercontent.com/jxnl/instructor/{self.branch}/docs/hub/{self.slug}.md?raw=true"

    def render_doc_link(self) -> str:
        return f"[link={self.get_doc_url()}](doc)[/link]"

    def render_slug(self) -> str:
        return f"{self.slug} {self.render_doc_link()}"


class HubClient:
    def __init__(
        self, base_url: str = "https://instructor-hub-proxy.jason-a3f.workers.dev"
    ):
        self.base_url = base_url

    def get_cookbooks(self, branch: str, q: Optional[str] = None, sort: bool = False):
        """Get collection index of cookbooks."""
        url = f"{self.base_url}/api/{branch}/items/"

        if q:
            url += f"?q={q}"

        response = httpx.get(url)
        if response.status_code == 200:
            pages = [HubPage(**page) for page in response.json()]
            if sort:
                return sorted(pages, key=lambda x: x.count, reverse=True)
            return pages
        else:
            raise Exception(f"Failed to fetch cookbooks: {response.status_code}")

    def get_content_markdown(self, branch: str, slug: str) -> str:
        """Get markdown content."""
        url = f"{self.base_url}/api/{branch}/items/{slug}/md/"
        response = httpx.get(url)
        if response.status_code == 200:
            return response.text
        else:
            raise Exception(f"Failed to fetch markdown content: {response.status_code}")

    def get_content_python(self, branch: str, slug: str) -> str:
        """Get Python code blocks from content."""
        url = f"{self.base_url}/api/{branch}/items/{slug}/py/"
        response = httpx.get(url)
        if response.status_code == 200:
            return response.text
        else:
            raise Exception(f"Failed to fetch Python content: {response.status_code}")

    def get_cookbook_id(self, id: int, branch: str = "main") -> Optional[HubPage]:
        for cookbook in self.get_cookbooks(branch):
            if cookbook.id == id:
                return cookbook

    def get_cookbook_slug(self, slug: str, branch: str = "main") -> Optional[HubPage]:
        for cookbook in self.get_cookbooks(branch):
            if cookbook.slug == slug:
                return cookbook


@app.command(
    "list",
    help="List all available cookbooks",
    short_help="List all available cookbooks",
)
def list_cookbooks(
    q: Optional[str] = typer.Option(None, "-q", help="Search for cookbooks by name"),
    sort: bool = typer.Option(False, "--sort", help="Sort the cookbooks by popularity"),
    branch: str = typer.Option(
        "main",
        "--branch",
        "-b",
        help="Specific branch to fetch the cookbooks from. Defaults to 'main'.",
    ),
):
    table = Table(title="Available Cookbooks")
    table.add_column("hub_id", justify="right", style="cyan", no_wrap=True)
    table.add_column("slug", style="green")
    table.add_column("title", style="white")
    table.add_column("n_downloads", justify="right")

    client = HubClient()
    for cookbook in client.get_cookbooks(branch, q=q, sort=sort):
        ii = cookbook.id
        slug = cookbook.render_slug()
        title = cookbook.name
        table.add_row(str(ii), slug, title, str(cookbook.count))

    console.print(table)


@app.command(
    "pull",
    help="Pull the latest cookbooks from the instructor hub, optionally outputting to a file",
    short_help="Pull the latest cookbooks",
)
def pull(
    id: Optional[int] = typer.Option(None, "--id", "-i", help="The cookbook id"),
    slug: Optional[str] = typer.Option(None, "--slug", "-s", help="The cookbook slug"),
    py: bool = typer.Option(False, "--py", help="Output to a Python file"),
    file: Optional[str] = typer.Option(None, "--output", help="Output to a file"),
    branch: str = typer.Option(
        "main", help="Specific branch to fetch the cookbooks from."
    ),
    page: bool = typer.Option(
        False, "--page", "-p", help="Paginate the output with a less-like pager"
    ),
):
    client = HubClient()
    cookbook = (
        client.get_cookbook_id(id, branch=branch)
        if id
        else client.get_cookbook_slug(slug, branch=branch)
        if slug
        else None
    )
    if not cookbook:
        typer.echo("Please provide a valid cookbook id or slug.")
        raise typer.Exit(code=1)

    output = (
        client.get_content_python(branch, cookbook.slug)
        if py
        else Markdown(client.get_content_markdown(branch, cookbook.slug))
    )

    if file:
        with open(file, "w") as f:
            f.write(output)  # type: ignore - markdown is writable
            return

    if page:
        with console.pager(styles=True):
            console.print(output)
    elif py:
        print(output)
    else:
        console.print(output)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/jobs.py
=======
from typing import Optional, TypedDict
from openai import OpenAI

from openai.types.fine_tuning.job_create_params import Hyperparameters
import typer
import time
from rich.live import Live
from rich.table import Table
from rich.console import Console
from datetime import datetime
from openai.types.fine_tuning import FineTuningJob

client = OpenAI()
app = typer.Typer()
console = Console()


class FuneTuningParams(TypedDict, total=False):
    hyperparameters: Hyperparameters
    validation_file: Optional[str]
    suffix: Optional[str]


def generate_table(jobs: list[FineTuningJob]) -> Table:
    # Sorting the jobs by creation time
    jobs = sorted(jobs, key=lambda x: x.created_at, reverse=True)

    table = Table(
        title="OpenAI Fine Tuning Job Monitoring",
        caption="Automatically refreshes every 5 seconds, press Ctrl+C to exit",
    )

    table.add_column("Job ID", style="dim")
    table.add_column("Status")
    table.add_column("Creation Time", justify="right")
    table.add_column("Completion Time", justify="right")
    table.add_column("Model Name")
    table.add_column("File ID")
    table.add_column("Epochs")
    table.add_column("Base Model")

    for job in jobs:
        status_emoji = {
            "running": "",
            "succeeded": "",
            "failed": "",
            "cancelled": "",
        }.get(job.status, "")

        finished_at = (
            str(datetime.fromtimestamp(job.finished_at)) if job.finished_at else "N/A"
        )

        table.add_row(
            job.id,
            f"{status_emoji} [{status_color(job.status)}]{job.status}[/]",
            str(datetime.fromtimestamp(job.created_at)),
            finished_at,
            job.fine_tuned_model,
            job.training_file,
            str(job.hyperparameters.n_epochs),
            job.model,
        )

    return table


def status_color(status: str) -> str:
    return {"running": "yellow", "succeeded": "green", "failed": "red"}.get(
        status, "white"
    )


def get_jobs(limit: int = 5) -> list[FineTuningJob]:
    return client.fine_tuning.jobs.list(limit=limit).data


def get_file_status(file_id: str) -> str:
    response = client.files.retrieve(file_id)
    return response.status


@app.command(
    name="list",
    help="Monitor the status of the most recent fine-tuning jobs.",
)
def watch(
    limit: int = typer.Option(5, help="Limit the number of jobs to monitor"),
    poll: int = typer.Option(5, help="Polling interval in seconds"),
    screen: bool = typer.Option(False, help="Enable or disable screen output"),
) -> None:
    """
    Monitor the status of the most recent fine-tuning jobs.
    """
    jobs = get_jobs(limit=limit)
    with Live(generate_table(jobs), refresh_per_second=2, screen=screen) as live_table:
        while True:
            jobs = get_jobs(limit=limit)
            live_table.update(generate_table(jobs))
            time.sleep(poll)


@app.command(
    help="Create a fine-tuning job from an existing ID.",
)
def create_from_id(
    id: str = typer.Argument(help="ID of the existing fine-tuning job"),
    model: str = typer.Option("gpt-3.5-turbo", help="Model to use for fine-tuning"),
    n_epochs: Optional[int] = typer.Option(
        None, help="Number of epochs for fine-tuning", show_default=False
    ),
    batch_size: Optional[int] = typer.Option(
        None, help="Batch size for fine-tuning", show_default=False
    ),
    learning_rate_multiplier: Optional[float] = typer.Option(
        None, help="Learning rate multiplier for fine-tuning", show_default=False
    ),
    validation_file_id: Optional[str] = typer.Option(
        None, help="ID of the uploaded validation file"
    ),
) -> None:
    hyperparameters_dict: Hyperparameters = {}
    if n_epochs is not None:
        hyperparameters_dict["n_epochs"] = n_epochs
    if batch_size is not None:
        hyperparameters_dict["batch_size"] = batch_size
    if learning_rate_multiplier is not None:
        hyperparameters_dict["learning_rate_multiplier"] = learning_rate_multiplier

    with console.status(
        f"[bold green]Creating fine-tuning job from ID {id}...", spinner="dots"
    ):
        job = client.fine_tuning.jobs.create(
            training_file=id,
            model=model,
            hyperparameters=hyperparameters_dict,
            validation_file=validation_file_id if validation_file_id else None,
        )
        console.log(f"[bold green]Fine-tuning job created with ID: {job.id}")
    watch(limit=5, poll=2, screen=False)


@app.command(
    help="Create a fine-tuning job from a file.",
)
def create_from_file(
    file: str = typer.Argument(help="Path to the file for fine-tuning"),
    model: str = typer.Option("gpt-3.5-turbo", help="Model to use for fine-tuning"),
    poll: int = typer.Option(2, help="Polling interval in seconds"),
    n_epochs: Optional[int] = typer.Option(
        None, help="Number of epochs for fine-tuning", show_default=False
    ),
    batch_size: Optional[int] = typer.Option(
        None, help="Batch size for fine-tuning", show_default=False
    ),
    learning_rate_multiplier: Optional[float] = typer.Option(
        None, help="Learning rate multiplier for fine-tuning", show_default=False
    ),
    validation_file: Optional[str] = typer.Option(
        None, help="Path to the validation file"
    ),
    model_suffix: Optional[str] = typer.Option(
        None, help="Suffix to identify the model"
    ),
) -> None:
    hyperparameters_dict: Hyperparameters = {}
    if n_epochs is not None:
        hyperparameters_dict["n_epochs"] = n_epochs
    if batch_size is not None:
        hyperparameters_dict["batch_size"] = batch_size
    if learning_rate_multiplier is not None:
        hyperparameters_dict["learning_rate_multiplier"] = learning_rate_multiplier

    with open(file, "rb") as file_buffer:
        response = client.files.create(file=file_buffer, purpose="fine-tune")

    file_id = response.id

    validation_file_id = None
    if validation_file:
        with open(validation_file, "rb") as val_file:
            val_response = client.files.create(file=val_file, purpose="fine-tune")
        validation_file_id = val_response.id

    with console.status(f"Monitoring upload: {file_id} before finetuning...") as status:
        status.spinner_style = "dots"
        while True:
            file_status = get_file_status(file_id)
            validation_file_status = (
                get_file_status(validation_file_id) if validation_file_id else ""
            )

            if file_status == "processed" and (
                not validation_file_id or validation_file_status == "processed"
            ):
                console.log(f"[bold green]File {file_id} uploaded successfully!")
                if validation_file_id:
                    console.log(
                        f"[bold green]Validation file {validation_file_id} uploaded successfully!"
                    )
                break

            time.sleep(poll)

    additional_params: FuneTuningParams = {}
    if hyperparameters_dict:
        additional_params["hyperparameters"] = hyperparameters_dict
    if validation_file:
        additional_params["validation_file"] = validation_file
    if model_suffix:
        additional_params["suffix"] = model_suffix

    job = client.fine_tuning.jobs.create(
        training_file=file_id,
        model=model,
        **additional_params,
    )
    if validation_file_id:
        console.log(
            f"[bold green]Fine-tuning job created with ID: {job.id} from file ID: {file_id} and validation_file ID: {validation_file_id}"
        )
    else:
        console.log(
            f"[bold green]Fine-tuning job created with ID: {job.id} from file ID: {file_id}"
        )
    watch(limit=5, poll=poll, screen=False)


@app.command(
    help="Cancel a fine-tuning job.",
)
def cancel(
    id: str = typer.Argument(help="ID of the fine-tuning job to cancel"),
) -> None:
    with console.status(f"[bold red]Cancelling job {id}...", spinner="dots"):
        try:
            client.fine_tuning.jobs.cancel(id)
            console.log(f"[bold red]Job {id} cancelled successfully!")
        except Exception as e:
            console.log(f"[bold red]Error cancelling job {id}: {e}")


if __name__ == "__main__":
    app()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/usage.py
=======
from typing import Any, Union
from collections.abc import Awaitable
from datetime import datetime, timedelta
import typer
import os
import aiohttp
import asyncio
from builtins import list as List
from collections import defaultdict
from rich.console import Console
from rich.table import Table
from rich.progress import Progress

from instructor._types._alias import ModelNames


app = typer.Typer()
console = Console()

api_key = os.environ.get("OPENAI_API_KEY")


async def fetch_usage(date: str) -> dict[str, Any]:
    headers = {"Authorization": f"Bearer {api_key}"}
    url = f"https://api.openai.com/v1/usage?date={date}"
    async with aiohttp.ClientSession() as session:
        async with session.get(url, headers=headers) as resp:
            return await resp.json()


async def get_usage_for_past_n_days(n_days: int) -> list[dict[str, Any]]:
    tasks: List[Awaitable[dict[str, Any]]] = []  # noqa: UP006 - conflicting with the fn name
    all_data: List[dict[str, Any]] = []  # noqa: UP006 - conflicting with the fn name
    with Progress() as progress:
        if n_days > 1:
            task = progress.add_task("[green]Fetching usage data...", total=n_days)
            for i in range(n_days):
                date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                tasks.append(fetch_usage(date))
                progress.update(task, advance=1)
        else:
            tasks.append(fetch_usage(datetime.now().strftime("%Y-%m-%d")))

        fetched_data = await asyncio.gather(*tasks)
        for data in fetched_data:
            all_data.extend(data.get("data", []))
    return all_data


# Define the cost per unit for each model
MODEL_COSTS = {
    "gpt-4o": {"prompt": 0.005 / 1000, "completion": 0.015 / 1000},
    "gpt-4o-2024-05-13": {"prompt": 0.005 / 1000, "completion": 0.015 / 1000},
    "gpt-4-turbo": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4-turbo-2024-04-09": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4-0125-preview": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4-turbo-preview": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4-1106-preview": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4-vision-preview": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
    "gpt-4": {"prompt": 0.03 / 1000, "completion": 0.06 / 1000},
    "gpt-4-0314": {"prompt": 0.03 / 1000, "completion": 0.06 / 1000},
    "gpt-4-0613": {"prompt": 0.03 / 1000, "completion": 0.06 / 1000},
    "gpt-4-32k": {"prompt": 0.06 / 1000, "completion": 0.12 / 1000},
    "gpt-4-32k-0314": {"prompt": 0.06 / 1000, "completion": 0.12 / 1000},
    "gpt-4-32k-0613": {"prompt": 0.06 / 1000, "completion": 0.12 / 1000},
    "gpt-3.5-turbo": {"prompt": 0.0005 / 1000, "completion": 0.0015 / 1000},
    "gpt-3.5-turbo-16k": {"prompt": 0.0030 / 1000, "completion": 0.0040 / 1000},
    "gpt-3.5-turbo-0301": {"prompt": 0.0015 / 1000, "completion": 0.0020 / 1000},
    "gpt-3.5-turbo-0613": {"prompt": 0.0015 / 1000, "completion": 0.0020 / 1000},
    "gpt-3.5-turbo-1106": {"prompt": 0.0010 / 1000, "completion": 0.0020 / 1000},
    "gpt-3.5-turbo-0125": {"prompt": 0.0005 / 1000, "completion": 0.0015 / 1000},
    "gpt-3.5-turbo-16k-0613": {"prompt": 0.0030 / 1000, "completion": 0.0040 / 1000},
    "gpt-3.5-turbo-instruct": {"prompt": 0.0015 / 1000, "completion": 0.0020 / 1000},
    "text-embedding-3-small": 0.00002 / 1000,
    "text-embedding-3-large": 0.00013 / 1000,
    "text-embedding-ada-002": 0.00010 / 1000,
}


def get_model_cost(
    model: ModelNames,
) -> Union[dict[str, float], float]:
    """Get the cost details for a given model."""
    if model in MODEL_COSTS:
        return MODEL_COSTS[model]

    if model.startswith("gpt-3.5-turbo-16k"):
        return MODEL_COSTS["gpt-3.5-turbo-16k"]
    elif model.startswith("gpt-3.5-turbo"):
        return MODEL_COSTS["gpt-3.5-turbo"]
    elif model.startswith("gpt-4-turbo"):
        return MODEL_COSTS["gpt-4-turbo-preview"]
    elif model.startswith("gpt-4-32k"):
        return MODEL_COSTS["gpt-4-32k"]
    elif model.startswith("gpt-4o"):
        return MODEL_COSTS["gpt-4o"]
    elif model.startswith("gpt-4"):
        return MODEL_COSTS["gpt-4"]
    else:
        raise ValueError(f"Cost for model {model} not found")


def calculate_cost(
    snapshot_id: ModelNames,
    n_context_tokens: int,
    n_generated_tokens: int,
) -> float:
    """Calculate the cost based on the snapshot ID and number of tokens."""
    cost = get_model_cost(snapshot_id)

    if isinstance(cost, (float, int)):
        return cost * (n_context_tokens + n_generated_tokens)

    prompt_cost = cost["prompt"] * n_context_tokens
    completion_cost = cost["completion"] * n_generated_tokens
    return prompt_cost + completion_cost


def group_and_sum_by_date_and_snapshot(usage_data: list[dict[str, Any]]) -> Table:
    """Group and sum the usage data by date and snapshot, including costs."""
    summary: defaultdict[str, defaultdict[str, dict[str, Union[int, float]]]] = (
        defaultdict(
            lambda: defaultdict(
                lambda: {"total_requests": 0, "total_tokens": 0, "total_cost": 0.0}
            )
        )
    )

    for usage in usage_data:
        snapshot_id = usage["snapshot_id"]
        date = datetime.fromtimestamp(usage["aggregation_timestamp"]).strftime(
            "%Y-%m-%d"
        )
        summary[date][snapshot_id]["total_requests"] += usage["n_requests"]
        summary[date][snapshot_id]["total_tokens"] += usage["n_generated_tokens_total"]

        # Calculate and add the cost
        cost = calculate_cost(
            snapshot_id,
            usage["n_context_tokens_total"],
            usage["n_generated_tokens_total"],
        )
        summary[date][snapshot_id]["total_cost"] += cost

    table = Table(title="Usage Summary by Date, Snapshot, and Cost")
    table.add_column("Date", style="dim")
    table.add_column("Model", style="dim")
    table.add_column("Total Requests", justify="right")
    table.add_column("Total Cost ($)", justify="right")

    # Sort dates and snapshots in descending order
    sorted_dates = sorted(summary.keys(), reverse=True)
    for date in sorted_dates:
        sorted_snapshots = sorted(summary[date].keys(), reverse=True)
        for snapshot_id in sorted_snapshots:
            data = summary[date][snapshot_id]
            table.add_row(
                date,
                snapshot_id,
                str(data["total_requests"]),
                "{:.2f}".format(data["total_cost"]),
            )

    return table


@app.command(help="Displays OpenAI API usage data for the past N days.")
def list(
    n: int = typer.Option(0, help="Number of days."),
) -> None:
    all_data = asyncio.run(get_usage_for_past_n_days(n))
    table = group_and_sum_by_date_and_snapshot(all_data)
    console.print(table)


if __name__ == "__main__":
    app()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/cli/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/citation.py
=======
from pydantic import BaseModel, Field, model_validator, ValidationInfo
from collections.abc import Generator


class CitationMixin(BaseModel):
    """
    Helpful mixing that can use `validation_context={"context": context}` in `from_response` to find the span of the substring_phrase in the context.

    ## Usage

    ```python
    from pydantic import BaseModel, Field
    from instructor import CitationMixin

    class User(BaseModel):
        name: str = Field(description="The name of the person")
        age: int = Field(description="The age of the person")
        role: str = Field(description="The role of the person")


    context = "Betty was a student. Jason was a student. Jason is 20 years old"

    user = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "Extract jason from {context}",
            },
        response_model=User,
        validation_context={"context": context},
        ]
    )

    for quote in user.substring_quotes:
        assert quote in context

    print(user.model_dump())
    ```

    ## Result
    ```
    {
        "name": "Jason Liu",
        "age": 20,
        "role": "student",
        "substring_quotes": [
            "Jason was a student",
            "Jason is 20 years old",
        ]
    }
    ```

    """

    substring_quotes: list[str] = Field(
        description="List of unique and specific substrings of the quote that was used to answer the question.",
    )

    @model_validator(mode="after")  # type: ignore[misc]
    def validate_sources(self, info: ValidationInfo) -> "CitationMixin":
        """
        For each substring_phrase, find the span of the substring_phrase in the context.
        If the span is not found, remove the substring_phrase from the list.
        """
        if info.context is None:
            return self

        # Get the context from the info
        text_chunks = info.context.get("context", None)

        # Get the spans of the substring_phrase in the context
        spans = list(self.get_spans(text_chunks))
        # Replace the substring_phrase with the actual substring
        self.substring_quotes = [text_chunks[span[0] : span[1]] for span in spans]
        return self

    def _get_span(
        self, quote: str, context: str, errs: int = 5
    ) -> Generator[tuple[int, int], None, None]:
        import regex

        minor = quote
        major = context

        errs_ = 0
        s = regex.search(f"({minor}){{e<={errs_}}}", major)
        while s is None and errs_ <= errs:
            errs_ += 1
            s = regex.search(f"({minor}){{e<={errs_}}}", major)

        if s is not None:
            yield from s.spans()

    def get_spans(self, context: str) -> Generator[tuple[int, int], None, None]:
        for quote in self.substring_quotes:
            yield from self._get_span(quote, context)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/iterable.py
=======
from typing import Any, Optional, cast, ClassVar
from collections.abc import AsyncGenerator, Generator, Iterable

from pydantic import BaseModel, Field, create_model  # type: ignore - remove once Pydantic is updated

from instructor.function_calls import OpenAISchema
from instructor.mode import Mode
from instructor.utils import extract_json_from_stream, extract_json_from_stream_async


class IterableBase:
    task_type: ClassVar[Optional[type[BaseModel]]] = None

    @classmethod
    def from_streaming_response(
        cls, completion: Iterable[Any], mode: Mode, **kwargs: Any
    ) -> Generator[BaseModel, None, None]:  # noqa: ARG003
        json_chunks = cls.extract_json(completion, mode)

        if mode == Mode.MD_JSON:
            json_chunks = extract_json_from_stream(json_chunks)

        yield from cls.tasks_from_chunks(json_chunks, **kwargs)

    @classmethod
    async def from_streaming_response_async(
        cls, completion: AsyncGenerator[Any, None], mode: Mode, **kwargs: Any
    ) -> AsyncGenerator[BaseModel, None]:
        json_chunks = cls.extract_json_async(completion, mode)

        if mode == Mode.MD_JSON:
            json_chunks = extract_json_from_stream_async(json_chunks)

        return cls.tasks_from_chunks_async(json_chunks, **kwargs)

    @classmethod
    def tasks_from_chunks(
        cls, json_chunks: Iterable[str], **kwargs: Any
    ) -> Generator[BaseModel, None, None]:
        started = False
        potential_object = ""
        for chunk in json_chunks:
            potential_object += chunk
            if not started:
                if "[" in chunk:
                    started = True
                    potential_object = chunk[chunk.find("[") + 1 :]
                continue

            task_json, potential_object = cls.get_object(potential_object, 0)
            if task_json:
                assert cls.task_type is not None
                obj = cls.task_type.model_validate_json(task_json, **kwargs)
                yield obj

    @classmethod
    async def tasks_from_chunks_async(
        cls, json_chunks: AsyncGenerator[str, None], **kwargs: Any
    ) -> AsyncGenerator[BaseModel, None]:
        started = False
        potential_object = ""
        async for chunk in json_chunks:
            potential_object += chunk
            if not started:
                if "[" in chunk:
                    started = True
                    potential_object = chunk[chunk.find("[") + 1 :]
                continue

            task_json, potential_object = cls.get_object(potential_object, 0)
            if task_json:
                assert cls.task_type is not None
                obj = cls.task_type.model_validate_json(task_json, **kwargs)
                yield obj

    @staticmethod
    def extract_json(
        completion: Iterable[Any], mode: Mode
    ) -> Generator[str, None, None]:
        for chunk in completion:
            try:
                if mode == Mode.ANTHROPIC_JSON:
                    if json_chunk := chunk.delta.text:
                        yield json_chunk
                if mode == Mode.ANTHROPIC_TOOLS:
                    yield chunk.delta.partial_json
                if mode == Mode.GEMINI_JSON:
                    yield chunk.text
                elif chunk.choices:
                    if mode == Mode.FUNCTIONS:
                        if json_chunk := chunk.choices[0].delta.function_call.arguments:
                            yield json_chunk
                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:
                        if json_chunk := chunk.choices[0].delta.content:
                            yield json_chunk
                    elif mode == Mode.TOOLS:
                        if json_chunk := chunk.choices[0].delta.tool_calls:
                            yield json_chunk[0].function.arguments
                    else:
                        raise NotImplementedError(
                            f"Mode {mode} is not supported for MultiTask streaming"
                        )
            except AttributeError:
                pass

    @staticmethod
    async def extract_json_async(
        completion: AsyncGenerator[Any, None], mode: Mode
    ) -> AsyncGenerator[str, None]:
        async for chunk in completion:
            try:
                if mode == Mode.ANTHROPIC_JSON:
                    if json_chunk := chunk.delta.text:
                        yield json_chunk
                if mode == Mode.ANTHROPIC_TOOLS:
                    yield chunk.delta.partial_json
                elif chunk.choices:
                    if mode == Mode.FUNCTIONS:
                        if json_chunk := chunk.choices[0].delta.function_call.arguments:
                            yield json_chunk
                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:
                        if json_chunk := chunk.choices[0].delta.content:
                            yield json_chunk
                    elif mode == Mode.TOOLS:
                        if json_chunk := chunk.choices[0].delta.tool_calls:
                            yield json_chunk[0].function.arguments
                    else:
                        raise NotImplementedError(
                            f"Mode {mode} is not supported for MultiTask streaming"
                        )
            except AttributeError:
                pass

    @staticmethod
    def get_object(s: str, stack: int) -> tuple[Optional[str], str]:
        start_index = s.find("{")
        for i, c in enumerate(s):
            if c == "{":
                stack += 1
            if c == "}":
                stack -= 1
                if stack == 0:
                    return s[start_index : i + 1], s[i + 2 :]
        return None, s


def IterableModel(
    subtask_class: type[BaseModel],
    name: Optional[str] = None,
    description: Optional[str] = None,
) -> type[BaseModel]:
    """
    Dynamically create a IterableModel OpenAISchema that can be used to segment multiple
    tasks given a base class. This creates class that can be used to create a toolkit
    for a specific task, names and descriptions are automatically generated. However
    they can be overridden.

    ## Usage

    ```python
    from pydantic import BaseModel, Field
    from instructor import IterableModel

    class User(BaseModel):
        name: str = Field(description="The name of the person")
        age: int = Field(description="The age of the person")
        role: str = Field(description="The role of the person")

    MultiUser = IterableModel(User)
    ```

    ## Result

    ```python
    class MultiUser(OpenAISchema, MultiTaskBase):
        tasks: List[User] = Field(
            default_factory=list,
            repr=False,
            description="Correctly segmented list of `User` tasks",
        )

        @classmethod
        def from_streaming_response(cls, completion) -> Generator[User]:
            '''
            Parse the streaming response from OpenAI and yield a `User` object
            for each task in the response
            '''
            json_chunks = cls.extract_json(completion)
            yield from cls.tasks_from_chunks(json_chunks)
    ```

    Parameters:
        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask
        name (Optional[str]): The name of the MultiTask class, if None then the name
            of the subtask class is used as `Multi{subtask_class.__name__}`
        description (Optional[str]): The description of the MultiTask class, if None
            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`

    Returns:
        schema (OpenAISchema): A new class that can be used to segment multiple tasks
    """
    task_name = subtask_class.__name__ if name is None else name

    name = f"Iterable{task_name}"

    list_tasks = (
        list[subtask_class],
        Field(
            default_factory=list,
            repr=False,
            description=f"Correctly segmented list of `{task_name}` tasks",
        ),
    )

    base_models = cast(tuple[type[BaseModel], ...], (OpenAISchema, IterableBase))
    new_cls = create_model(
        name,
        tasks=list_tasks,
        __base__=base_models,
    )
    new_cls = cast(type[IterableBase], new_cls)

    # set the class constructor BaseModel
    new_cls.task_type = subtask_class

    new_cls.__doc__ = (
        f"Correct segmentation of `{task_name}` tasks"
        if description is None
        else description
    )
    assert issubclass(
        new_cls, OpenAISchema
    ), "The new class should be a subclass of OpenAISchema"
    return new_cls


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/maybe.py
=======
from pydantic import BaseModel, Field, create_model  # type: ignore - remove once Pydantic is updated
from typing import Generic, Optional, TypeVar

T = TypeVar("T", bound=BaseModel)


class MaybeBase(BaseModel, Generic[T]):
    """
    Extract a result from a model, if any, otherwise set the error and message fields.
    """

    result: Optional[T]
    error: bool = Field(default=False)
    message: Optional[str]

    def __bool__(self) -> bool:
        return self.result is not None


def Maybe(model: type[T]) -> type[MaybeBase[T]]:
    """
    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.

    ## Usage

    ```python
    from pydantic import BaseModel, Field
    from instructor import Maybe

    class User(BaseModel):
        name: str = Field(description="The name of the person")
        age: int = Field(description="The age of the person")
        role: str = Field(description="The role of the person")

    MaybeUser = Maybe(User)
    ```

    ## Result

    ```python
    class MaybeUser(BaseModel):
        result: Optional[User]
        error: bool = Field(default=False)
        message: Optional[str]

        def __bool__(self):
            return self.result is not None
    ```

    Parameters:
        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.

    Returns:
        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.
    """
    return create_model(
        f"Maybe{model.__name__}",
        __base__=MaybeBase,
        result=(
            Optional[model],
            Field(
                default=None,
                description="Correctly extracted result from the model, if any, otherwise None",
            ),
        ),
        error=(bool, Field(default=False)),
        message=(
            Optional[str],
            Field(
                default=None,
                description="Error message if no result was found, should be short and concise",
            ),
        ),
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/parallel.py
=======
import sys
from typing import (
    Any,
    Optional,
    TypeVar,
    Union,
    get_args,
    get_origin,
)
from collections.abc import Generator
from pydantic import BaseModel
from instructor.function_calls import OpenAISchema, openai_schema
from collections.abc import Iterable

from instructor.mode import Mode

T = TypeVar("T", bound=OpenAISchema)


class ParallelBase:
    def __init__(self, *models: type[OpenAISchema]):
        # Note that for everything else we've created a class, but for parallel base it is an instance
        assert len(models) > 0, "At least one model is required"
        self.models = models
        self.registry = {
            model.__name__ if hasattr(model, "__name__") else str(model): model
            for model in models
        }

    def from_response(
        self,
        response: Any,
        mode: Mode,
        validation_context: Optional[Any] = None,
        strict: Optional[bool] = None,
    ) -> Generator[BaseModel, None, None]:
        #! We expect this from the OpenAISchema class, We should address
        #! this with a protocol or an abstract class... @jxnlco
        assert mode == Mode.PARALLEL_TOOLS, "Mode must be PARALLEL_TOOLS"
        for tool_call in response.choices[0].message.tool_calls:
            name = tool_call.function.name
            arguments = tool_call.function.arguments
            yield self.registry[name].model_validate_json(
                arguments, context=validation_context, strict=strict
            )


if sys.version_info >= (3, 10):
    from types import UnionType

    def is_union_type(typehint: type[Iterable[T]]) -> bool:
        return get_origin(get_args(typehint)[0]) in (Union, UnionType)
else:

    def is_union_type(typehint: type[Iterable[T]]) -> bool:
        return get_origin(get_args(typehint)[0]) is Union


def get_types_array(typehint: type[Iterable[T]]) -> tuple[type[T], ...]:
    should_be_iterable = get_origin(typehint)
    if should_be_iterable is not Iterable:
        raise TypeError(f"Model should be with Iterable instead if {typehint}")

    if is_union_type(typehint):
        # works for Iterable[Union[int, str]], Iterable[int | str]
        the_types = get_args(get_args(typehint)[0])
        return the_types

    # works for Iterable[int]
    return get_args(typehint)


def handle_parallel_model(typehint: type[Iterable[T]]) -> list[dict[str, Any]]:
    the_types = get_types_array(typehint)
    return [
        {"type": "function", "function": openai_schema(model).openai_schema}
        for model in the_types
    ]


def ParallelModel(typehint: type[Iterable[T]]) -> ParallelBase:
    the_types = get_types_array(typehint)
    return ParallelBase(*[model for model in the_types])


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/partial.py
=======
# --------------------------------------------------------------------------------
# The following code is adapted from a comment on GitHub in the pydantic/pydantic repository by silviumarcu.
# Source: https://github.com/pydantic/pydantic/issues/6381#issuecomment-1831607091
#
# This code is used in accordance with the repository's license, and this reference
# serves as an acknowledgment of the original author's contribution to this project.
# --------------------------------------------------------------------------------

from __future__ import annotations

from jiter import from_json
from pydantic import BaseModel, create_model  # type: ignore - remove once Pydantic is updated
from pydantic.fields import FieldInfo
from typing import (
    Any,
    Generic,
    get_args,
    get_origin,
    NoReturn,
    Optional,
    TypeVar,
)
from collections.abc import AsyncGenerator, Generator, Iterable
from copy import deepcopy
from functools import cache

from instructor.mode import Mode
from instructor.utils import extract_json_from_stream, extract_json_from_stream_async

T_Model = TypeVar("T_Model", bound=BaseModel)


class MakeFieldsOptional:
    pass


def _make_field_optional(
    field: FieldInfo,
) -> tuple[Any, FieldInfo]:
    tmp_field = deepcopy(field)

    annotation = field.annotation

    # Handle generics (like List, Dict, etc.)
    if get_origin(annotation) is not None:
        # Get the generic base (like List, Dict) and its arguments (like User in List[User])
        generic_base = get_origin(annotation)
        generic_args = get_args(annotation)

        # Recursively apply Partial to each of the generic arguments
        modified_args = tuple(
            (
                Partial[arg, MakeFieldsOptional]  # type: ignore[valid-type]
                if isinstance(arg, type) and issubclass(arg, BaseModel)
                else arg
            )
            for arg in generic_args
        )

        # Reconstruct the generic type with modified arguments
        tmp_field.annotation = (
            Optional[generic_base[modified_args]] if generic_base else None
        )
        tmp_field.default = None
    # If the field is a BaseModel, then recursively convert it's
    # attributes to optionals.
    elif isinstance(annotation, type) and issubclass(annotation, BaseModel):
        tmp_field.annotation = Optional[Partial[annotation, MakeFieldsOptional]]  # type: ignore[assignment, valid-type]
        tmp_field.default = {}
    else:
        tmp_field.annotation = Optional[field.annotation]  # type: ignore[assignment]
        tmp_field.default = None

    return tmp_field.annotation, tmp_field  # type: ignore


class PartialBase(Generic[T_Model]):
    @classmethod
    @cache
    def get_partial_model(cls) -> type[T_Model]:
        """Return a partial model we can use to validate partial results."""
        assert issubclass(
            cls, BaseModel
        ), f"{cls.__name__} must be a subclass of BaseModel"

        return create_model(
            __model_name=(
                cls.__name__
                if cls.__name__.startswith("Partial")
                else f"Partial{cls.__name__}"
            ),
            __base__=cls,
            __module__=cls.__module__,
            **{
                field_name: _make_field_optional(field_info)
                for field_name, field_info in cls.model_fields.items()
            },
        )  # type: ignore[all]

    @classmethod
    def from_streaming_response(
        cls, completion: Iterable[Any], mode: Mode, **kwargs: Any
    ) -> Generator[T_Model, None, None]:
        json_chunks = cls.extract_json(completion, mode)

        if mode == Mode.MD_JSON:
            json_chunks = extract_json_from_stream(json_chunks)

        yield from cls.model_from_chunks(json_chunks, **kwargs)

    @classmethod
    async def from_streaming_response_async(
        cls, completion: AsyncGenerator[Any, None], mode: Mode, **kwargs: Any
    ) -> AsyncGenerator[T_Model, None]:
        json_chunks = cls.extract_json_async(completion, mode)

        if mode == Mode.MD_JSON:
            json_chunks = extract_json_from_stream_async(json_chunks)

        return cls.model_from_chunks_async(json_chunks, **kwargs)

    @classmethod
    def model_from_chunks(
        cls, json_chunks: Iterable[Any], **kwargs: Any
    ) -> Generator[T_Model, None, None]:
        potential_object = ""
        partial_model = cls.get_partial_model()
        for chunk in json_chunks:
            potential_object += chunk
            obj = from_json(
                (potential_object or "{}").encode(), partial_mode="trailing-strings"
            )
            obj = partial_model.model_validate(obj, strict=None, **kwargs)
            yield obj

    @classmethod
    async def model_from_chunks_async(
        cls, json_chunks: AsyncGenerator[str, None], **kwargs: Any
    ) -> AsyncGenerator[T_Model, None]:
        potential_object = ""
        partial_model = cls.get_partial_model()
        async for chunk in json_chunks:
            potential_object += chunk
            obj = from_json(
                (potential_object or "{}").encode(), partial_mode="trailing-strings"
            )
            obj = partial_model.model_validate(obj, strict=None, **kwargs)
            yield obj

    @staticmethod
    def extract_json(
        completion: Iterable[Any], mode: Mode
    ) -> Generator[str, None, None]:
        for chunk in completion:
            try:
                if mode == Mode.ANTHROPIC_JSON:
                    if json_chunk := chunk.delta.text:
                        yield json_chunk
                if mode == Mode.ANTHROPIC_TOOLS:
                    yield chunk.delta.partial_json
                if mode == Mode.GEMINI_JSON:
                    yield chunk.text
                elif chunk.choices:
                    if mode == Mode.FUNCTIONS:
                        if json_chunk := chunk.choices[0].delta.function_call.arguments:
                            yield json_chunk
                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:
                        if json_chunk := chunk.choices[0].delta.content:
                            yield json_chunk
                    elif mode == Mode.TOOLS:
                        if json_chunk := chunk.choices[0].delta.tool_calls:
                            yield json_chunk[0].function.arguments
                    else:
                        raise NotImplementedError(
                            f"Mode {mode} is not supported for MultiTask streaming"
                        )
            except AttributeError:
                pass

    @staticmethod
    async def extract_json_async(
        completion: AsyncGenerator[Any, None], mode: Mode
    ) -> AsyncGenerator[str, None]:
        async for chunk in completion:
            try:
                if mode == Mode.ANTHROPIC_JSON:
                    if json_chunk := chunk.delta.text:
                        yield json_chunk
                if mode == Mode.ANTHROPIC_TOOLS:
                    yield chunk.delta.partial_json
                elif chunk.choices:
                    if mode == Mode.FUNCTIONS:
                        if json_chunk := chunk.choices[0].delta.function_call.arguments:
                            yield json_chunk
                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:
                        if json_chunk := chunk.choices[0].delta.content:
                            yield json_chunk
                    elif mode == Mode.TOOLS:
                        if json_chunk := chunk.choices[0].delta.tool_calls:
                            yield json_chunk[0].function.arguments
                    else:
                        raise NotImplementedError(
                            f"Mode {mode} is not supported for MultiTask streaming"
                        )
            except AttributeError:
                pass


class Partial(Generic[T_Model]):
    """Generate a new class which has PartialBase as a base class.

    Notes:
        This will enable partial validation of the model while streaming.

    Example:
        Partial[SomeModel]
    """

    def __new__(
        cls,
        *args: object,  # noqa :ARG003
        **kwargs: object,  # noqa :ARG003
    ) -> Partial[T_Model]:
        """Cannot instantiate.

        Raises:
            TypeError: Direct instantiation not allowed.
        """
        raise TypeError("Cannot instantiate abstract Partial class.")

    def __init_subclass__(
        cls,
        *args: object,
        **kwargs: object,
    ) -> NoReturn:
        """Cannot subclass.

        Raises:
           TypeError: Subclassing not allowed.
        """
        raise TypeError(f"Cannot subclass {cls.__module__}.Partial")

    def __class_getitem__(
        cls,
        wrapped_class: type[T_Model] | tuple[type[T_Model], type[MakeFieldsOptional]],
    ) -> type[T_Model]:
        """Convert model to one that inherits from PartialBase.

        We don't make the fields optional at this point, we just wrap them with `Partial` so the names of the nested models will be
        `Partial{ModelName}`. We want the output of `model_json_schema()` to
        reflect the name change, but everything else should be the same as the
        original model. During validation, we'll generate a true partial model
        to support partially defined fields.

        """

        make_fields_optional = None
        if isinstance(wrapped_class, tuple):
            wrapped_class, make_fields_optional = wrapped_class

        def _wrap_models(field: FieldInfo) -> tuple[object, FieldInfo]:
            tmp_field = deepcopy(field)

            annotation = field.annotation

            # Handle generics (like List, Dict, etc.)
            if get_origin(annotation) is not None:
                # Get the generic base (like List, Dict) and its arguments (like User in List[User])
                generic_base = get_origin(annotation)
                generic_args = get_args(annotation)

                # Recursively apply Partial to each of the generic arguments
                modified_args = tuple(
                    (
                        Partial[arg]
                        if isinstance(arg, type) and issubclass(arg, BaseModel)
                        else arg
                    )
                    for arg in generic_args
                )

                # Reconstruct the generic type with modified arguments
                tmp_field.annotation = (
                    generic_base[modified_args] if generic_base else None
                )
            # If the field is a BaseModel, then recursively convert it's
            # attributes to optionals.
            elif isinstance(annotation, type) and issubclass(annotation, BaseModel):
                tmp_field.annotation = Partial[annotation]
            return tmp_field.annotation, tmp_field

        return create_model(
            __model_name=(
                wrapped_class.__name__
                if wrapped_class.__name__.startswith("Partial")
                else f"Partial{wrapped_class.__name__}"
            ),
            __base__=(wrapped_class, PartialBase),
            __module__=wrapped_class.__module__,
            **{
                field_name: (
                    _make_field_optional(field_info)
                    if make_fields_optional is not None
                    else _wrap_models(field_info)
                )
                for field_name, field_info in wrapped_class.model_fields.items()
            },
        )  # type: ignore


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/simple_type.py
=======
from __future__ import annotations
from inspect import isclass
import typing
from pydantic import BaseModel, create_model  # type: ignore - remove once Pydantic is updated
from enum import Enum


from instructor.dsl.partial import Partial
from instructor.function_calls import OpenAISchema


T = typing.TypeVar("T")


class AdapterBase(BaseModel):
    pass


class ModelAdapter(typing.Generic[T]):
    """
    Accepts a response model and returns a BaseModel with the response model as the content.
    """

    def __class_getitem__(cls, response_model: type[BaseModel]) -> type[BaseModel]:
        assert is_simple_type(response_model), "Only simple types are supported"
        return create_model(
            "Response",
            content=(response_model, ...),
            __doc__="Correctly Formated and Extracted Response.",
            __base__=(AdapterBase, OpenAISchema),
        )


def validateIsSubClass(response_model: type):
    """
    Temporary guard against issues with generics in Python 3.9
    """
    import sys

    if sys.version_info < (3, 10):
        if len(typing.get_args(response_model)) == 0:
            return False
        return issubclass(typing.get_args(response_model)[0], BaseModel)
    return issubclass(response_model, BaseModel)


def is_simple_type(
    response_model: type[BaseModel] | str | int | float | bool | typing.Any,
) -> bool:
    # ! we're getting mixes between classes and instances due to how we handle some
    # ! response model types, we should fix this in later PRs

    try:
        if isclass(response_model) and validateIsSubClass(response_model):
            return False
    except TypeError:
        # ! In versions < 3.11, typing.Iterable is not a class, so we can't use isclass
        # ! for now if `response_model` is an Iterable isclass and issubclass will raise
        # ! TypeError, so we need to check if `response_model` is an Iterable
        # ! This is a workaround for now, we should fix this in later PRs
        return False

    if typing.get_origin(response_model) in {typing.Iterable, Partial}:
        # These are reserved for streaming types, would be nice to
        return False

    if response_model in {
        str,
        int,
        float,
        bool,
    }:
        return True

    # If the response_model is a simple type like annotated
    if typing.get_origin(response_model) in {
        typing.Annotated,
        typing.Literal,
        typing.Union,
        list,  # origin of List[T] is list
    }:
        return True

    if isclass(response_model) and issubclass(response_model, Enum):
        return True

    return False


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/validators.py
=======
from typing import Callable, Optional

from openai import OpenAI
from pydantic import Field

from instructor.function_calls import OpenAISchema
from instructor.client import Instructor


class Validator(OpenAISchema):
    """
    Validate if an attribute is correct and if not,
    return a new value with an error message
    """

    is_valid: bool = Field(
        default=True,
        description="Whether the attribute is valid based on the requirements",
    )
    reason: Optional[str] = Field(
        default=None,
        description="The error message if the attribute is not valid, otherwise None",
    )
    fixed_value: Optional[str] = Field(
        default=None,
        description="If the attribute is not valid, suggest a new value for the attribute",
    )


def llm_validator(
    statement: str,
    client: Instructor,
    allow_override: bool = False,
    model: str = "gpt-3.5-turbo",
    temperature: float = 0,
) -> Callable[[str], str]:
    """
    Create a validator that uses the LLM to validate an attribute

    ## Usage

    ```python
    from instructor import llm_validator
    from pydantic import BaseModel, Field, field_validator

    class User(BaseModel):
        name: str = Annotated[str, llm_validator("The name must be a full name all lowercase")
        age: int = Field(description="The age of the person")

    try:
        user = User(name="Jason Liu", age=20)
    except ValidationError as e:
        print(e)
    ```

    ```
    1 validation error for User
    name
        The name is valid but not all lowercase (type=value_error.llm_validator)
    ```

    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.

    Parameters:
        statement (str): The statement to validate
        model (str): The LLM to use for validation (default: "gpt-3.5-turbo-0613")
        temperature (float): The temperature to use for the LLM (default: 0)
        openai_client (OpenAI): The OpenAI client to use (default: None)
    """

    def llm(v: str) -> str:
        resp = client.chat.completions.create(
            response_model=Validator,
            messages=[
                {
                    "role": "system",
                    "content": "You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.",
                },
                {
                    "role": "user",
                    "content": f"Does `{v}` follow the rules: {statement}",
                },
            ],
            model=model,
            temperature=temperature,
        )

        # If the response is  not valid, return the reason, this could be used in
        # the future to generate a better response, via reasking mechanism.
        assert resp.is_valid, resp.reason

        if allow_override and not resp.is_valid and resp.fixed_value is not None:
            # If the value is not valid, but we allow override, return the fixed value
            return resp.fixed_value
        return v

    return llm


def openai_moderation(client: OpenAI) -> Callable[[str], str]:
    """
    Validates a message using OpenAI moderation model.

    Should only be used for monitoring inputs and outputs of OpenAI APIs
    Other use cases are disallowed as per:
    https://platform.openai.com/docs/guides/moderation/overview

    Example:
    ```python
    from instructor import OpenAIModeration

    class Response(BaseModel):
        message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]

    Response(message="I hate you")
    ```

    ```
     ValidationError: 1 validation error for Response
     message
    Value error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]
    ```

    client (OpenAI): The OpenAI client to use, must be sync (default: None)
    """

    def validate_message_with_openai_mod(v: str) -> str:
        response = client.moderations.create(input=v)
        out = response.results[0]
        cats = out.categories.model_dump()
        if out.flagged:
            raise ValueError(
                f"`{v}` was flagged for {', '.join(cat for cat in cats if cats[cat])}"
            )

        return v

    return validate_message_with_openai_mod


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/dsl/__init__.py
=======
from .iterable import IterableModel
from .maybe import Maybe
from .partial import Partial
from .validators import llm_validator, openai_moderation
from .citation import CitationMixin
from .simple_type import is_simple_type, ModelAdapter

__all__ = [  # noqa: F405
    "CitationMixin",
    "IterableModel",
    "Maybe",
    "Partial",
    "llm_validator",
    "openai_moderation",
    "is_simple_type",
    "ModelAdapter",
]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/_types/_alias.py
=======
from typing import Literal

from typing_extensions import TypeAlias

ModelNames: TypeAlias = Literal[
    "gpt-4o",
    "gpt-4-0125-preview",
    "gpt-4-turbo-preview",
    "gpt-4-1106-preview",
    "gpt-4-vision-preview",
    "gpt-4",
    "gpt-4-0314",
    "gpt-4-0613",
    "gpt-4-32k",
    "gpt-4-32k-0314",
    "gpt-4-32k-0613",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k",
    "gpt-3.5-turbo-0301",
    "gpt-3.5-turbo-0613",
    "gpt-3.5-turbo-1106",
    "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-16k-0613",
    "gpt-3.5-turbo-instruct",
    "text-embedding-ada-002",
    "text-embedding-ada-002-v2",
    "text-embedding-3-small",
    "text-embedding-3-large",
]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/instructor/_types/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_distil.py
=======
from typing import Any, Callable, cast
import pytest
import instructor

from openai import OpenAI
from pydantic import BaseModel

from instructor.distil import (
    Instructions,
    format_function,
    get_signature_from_fn,
    is_return_type_base_model_or_instance,
)

client = instructor.patch(OpenAI())

instructions = Instructions(
    name="test_distil",
)


class SimpleModel(BaseModel):  # type: ignore[misc]
    data: int


def test_must_have_hint() -> None:
    with pytest.raises(AssertionError):

        @instructions.distil
        def test_func(x: int):  # type: ignore[no-untyped-def]
            return SimpleModel(data=x)


def test_must_be_base_model() -> None:
    with pytest.raises(AssertionError):

        @instructions.distil
        def test_func(x: int) -> int:
            return SimpleModel(data=x)


def test_is_return_type_base_model_or_instance() -> None:
    def valid_function() -> SimpleModel:
        return SimpleModel(data=1)

    def invalid_function() -> int:
        return 1

    assert is_return_type_base_model_or_instance(valid_function)
    assert not is_return_type_base_model_or_instance(invalid_function)


def test_get_signature_from_fn() -> None:
    def test_function(a: int, b: str) -> float:  # type: ignore[empty-body]
        """Sample docstring"""
        pass

    result = get_signature_from_fn(test_function)
    expected = "def test_function(a: int, b: str) -> float"
    assert expected in result
    assert "Sample docstring" in result


def test_format_function() -> None:
    def sample_function(x: int) -> SimpleModel:
        """This is a docstring."""
        return SimpleModel(data=x)

    formatted = format_function(sample_function)
    assert "def sample_function(x: int) -> SimpleModel:" in formatted
    assert '"""This is a docstring."""' in formatted
    assert "return SimpleModel(data=x)" in formatted


def test_distil_decorator_without_arguments() -> None:
    @instructions.distil
    def test_func(x: int) -> SimpleModel:
        return SimpleModel(data=x)

    casted_test_func = cast(Callable[[int], SimpleModel], test_func)
    result: SimpleModel = casted_test_func(42)
    assert result.data == 42


def test_distil_decorator_with_name_argument() -> None:
    @instructions.distil(name="custom_name")
    def another_test_func(x: int) -> SimpleModel:
        return SimpleModel(data=x)

    casted_another_test_func = cast(Callable[[int], SimpleModel], another_test_func)
    result: SimpleModel = casted_another_test_func(55)
    assert result.data == 55


# Mock track function for decorator tests
def mock_track(*args: tuple[Any, ...], **kwargs: dict[str, Any]) -> None:
    pass


def fn(a: int, b: int) -> int:
    return client.chat.completions.create(
        messages=[], model="davinci", response_model=SimpleModel
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_function_calls.py
=======
from typing import TypeVar

import pytest
from anthropic.types import Message, Usage
from openai.types.chat.chat_completion import ChatCompletion
from pydantic import BaseModel, ValidationError

import instructor
from instructor import OpenAISchema, openai_schema
from instructor.exceptions import IncompleteOutputException
from instructor.utils import disable_pydantic_error_url

T = TypeVar("T")


@pytest.fixture  # type: ignore[misc]
def test_model() -> type[OpenAISchema]:
    class TestModel(OpenAISchema):  # type: ignore[misc]
        name: str = "TestModel"
        data: str

    return TestModel


@pytest.fixture  # type: ignore[misc]
def mock_completion(request: T) -> ChatCompletion:
    finish_reason = "stop"
    data_content = '{\n"data": "complete data"\n}'

    if hasattr(request, "param"):
        finish_reason = request.param.get("finish_reason", finish_reason)
        data_content = request.param.get("data_content", data_content)

    mock_choices = [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "function_call": {"name": "TestModel", "arguments": data_content},
                "content": data_content,
            },
            "finish_reason": finish_reason,
        }
    ]

    completion = ChatCompletion(
        id="test_id",
        choices=mock_choices,
        created=1234567890,
        model="gpt-3.5-turbo",
        object="chat.completion",
    )

    return completion


@pytest.fixture  # type: ignore[misc]
def mock_anthropic_message(request: T) -> Message:
    data_content = '{\n"data": "Claude says hi"\n}'
    if hasattr(request, "param"):
        data_content = request.param.get("data_content", data_content)
    return Message(
        id="test_id",
        content=[{"type": "text", "text": data_content}],
        model="claude-3-haiku-20240307",
        role="assistant",
        stop_reason="end_turn",
        stop_sequence=None,
        type="message",
        usage=Usage(
            input_tokens=100,
            output_tokens=100,
        ),
    )


def test_openai_schema() -> None:
    @openai_schema
    class Dataframe(BaseModel):  # type: ignore[misc]
        """
        Class representing a dataframe. This class is used to convert
        data into a frame that can be used by pandas.
        """

        data: str
        columns: str

        def to_pandas(self) -> None:
            pass

    assert hasattr(Dataframe, "openai_schema")
    assert hasattr(Dataframe, "from_response")
    assert hasattr(Dataframe, "to_pandas")
    assert Dataframe.openai_schema["name"] == "Dataframe"


def test_openai_schema_raises_error() -> None:
    with pytest.raises(TypeError, match="must be a subclass of pydantic.BaseModel"):

        @openai_schema
        class Dummy:
            pass


def test_no_docstring() -> None:
    class Dummy(OpenAISchema):  # type: ignore[misc]
        attr: str

    assert (
        Dummy.openai_schema["description"]
        == "Correctly extracted `Dummy` with all the required parameters with correct types"
    )


@pytest.mark.parametrize(
    "mock_completion",
    [{"finish_reason": "length", "data_content": '{\n"data": "incomplete dat"\n}'}],
    indirect=True,
)  # type: ignore[misc]
def test_incomplete_output_exception(
    test_model: type[OpenAISchema], mock_completion: ChatCompletion
) -> None:
    with pytest.raises(IncompleteOutputException):
        test_model.from_response(mock_completion, mode=instructor.Mode.FUNCTIONS)


def test_complete_output_no_exception(
    test_model: type[OpenAISchema], mock_completion: ChatCompletion
) -> None:
    test_model_instance = test_model.from_response(
        mock_completion, mode=instructor.Mode.FUNCTIONS
    )
    assert test_model_instance.data == "complete data"


@pytest.mark.asyncio  # type: ignore[misc]
@pytest.mark.parametrize(
    "mock_completion",
    [{"finish_reason": "length", "data_content": '{\n"data": "incomplete dat"\n}'}],
    indirect=True,
)  # type: ignore[misc]
def test_incomplete_output_exception_raise(
    test_model: type[OpenAISchema], mock_completion: ChatCompletion
) -> None:
    with pytest.raises(IncompleteOutputException):
        test_model.from_response(mock_completion, mode=instructor.Mode.FUNCTIONS)


def test_anthropic_no_exception(
    test_model: type[OpenAISchema], mock_anthropic_message: Message
) -> None:
    test_model_instance = test_model.from_response(
        mock_anthropic_message, mode=instructor.Mode.ANTHROPIC_JSON
    )
    assert test_model_instance.data == "Claude says hi"


@pytest.mark.parametrize(
    "mock_anthropic_message",
    [{"data_content": '{\n"data": "Claude likes\ncontrol\ncharacters"\n}'}],
    indirect=True,
)  # type: ignore[misc]
def test_control_characters_not_allowed_in_anthropic_json_strict_mode(
    test_model: type[OpenAISchema], mock_anthropic_message: Message
) -> None:
    with pytest.raises(ValidationError) as exc_info:
        test_model.from_response(
            mock_anthropic_message, mode=instructor.Mode.ANTHROPIC_JSON, strict=True
        )

    # https://docs.pydantic.dev/latest/errors/validation_errors/#json_invalid
    exc = exc_info.value
    assert len(exc.errors()) == 1
    assert exc.errors()[0]["type"] == "json_invalid"
    assert "control character" in exc.errors()[0]["msg"]


@pytest.mark.parametrize(
    "mock_anthropic_message",
    [{"data_content": '{\n"data": "Claude likes\ncontrol\ncharacters"\n}'}],
    indirect=True,
)  # type: ignore[misc]
def test_control_characters_allowed_in_anthropic_json_non_strict_mode(
    test_model: type[OpenAISchema], mock_anthropic_message: Message
) -> None:
    test_model_instance = test_model.from_response(
        mock_anthropic_message, mode=instructor.Mode.ANTHROPIC_JSON, strict=False
    )
    assert test_model_instance.data == "Claude likes\ncontrol\ncharacters"


def test_pylance_url_config() -> None:
    class Model(BaseModel):
        list_of_ints: list[int]
        a_float: float

    disable_pydantic_error_url()
    data = dict(list_of_ints=["1", 2, "bad"], a_float="Not a float")

    try:
        Model(**data)  # type: ignore
    except ValidationError as e:
        assert "https://errors.pydantic.dev" not in str(e)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_multitask.py
=======
from instructor import OpenAISchema
from instructor.dsl import IterableModel


def test_multi_task():
    class Search(OpenAISchema):
        """This is the search docstring"""

        id: int
        query: str

    IterableSearch = IterableModel(Search)
    assert IterableSearch.openai_schema["name"] == "IterableSearch"
    assert (
        IterableSearch.openai_schema["description"]
        == "Correct segmentation of `Search` tasks"
    )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_patch.py
=======
import functools

from openai import AsyncOpenAI, OpenAI

import instructor
from instructor.utils import is_async


def test_patch_completes_successfully():
    instructor.patch(OpenAI())


def test_apatch_completes_successfully():
    instructor.apatch(AsyncOpenAI())


def test_is_async_returns_true_if_function_is_async():
    async def async_function():
        pass

    assert is_async(async_function) is True


def test_is_async_returns_false_if_function_is_not_async():
    def sync_function():
        pass

    assert is_async(sync_function) is False


def test_is_async_returns_true_if_wrapped_function_is_async():
    async def async_function():
        pass

    @functools.wraps(async_function)
    def wrapped_function():
        pass

    assert is_async(wrapped_function) is True


def test_is_async_returns_true_if_double_wrapped_function_is_async():
    async def async_function():
        pass

    @functools.wraps(async_function)
    def wrapped_function():
        pass

    @functools.wraps(wrapped_function)
    def double_wrapped_function():
        pass

    assert is_async(double_wrapped_function) is True


def test_is_async_returns_true_if_triple_wrapped_function_is_async():
    async def async_function():
        pass

    @functools.wraps(async_function)
    def wrapped_function():
        pass

    @functools.wraps(wrapped_function)
    def double_wrapped_function():
        pass

    @functools.wraps(double_wrapped_function)
    def triple_wrapped_function():
        pass

    assert is_async(triple_wrapped_function) is True


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_process_response.py
=======
from typing_extensions import TypedDict
from pydantic import BaseModel
from instructor.process_response import handle_response_model


def test_typed_dict_conversion() -> None:
    class User(TypedDict):  # type: ignore
        name: str
        age: int

    _, user_tool_definition = handle_response_model(User)

    class User(BaseModel):
        name: str
        age: int

    _, pydantic_user_tool_definition = handle_response_model(User)
    assert user_tool_definition == pydantic_user_tool_definition


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_simple_types.py
=======
from instructor.dsl import is_simple_type, Partial
from pydantic import BaseModel


def test_enum_simple():
    from enum import Enum

    class Color(Enum):
        RED = 1
        GREEN = 2
        BLUE = 3

    assert is_simple_type(Color), "Failed for type: " + str(Color)


def test_standard_types():
    for t in [str, int, float, bool]:
        assert is_simple_type(t), "Failed for type: " + str(t)


def test_partial_not_simple():
    class SampleModel(BaseModel):
        data: int

    assert not is_simple_type(Partial[SampleModel]), "Failed for type: " + str(
        Partial[int]
    )


def test_annotated_simple():
    from pydantic import Field
    from typing import Annotated

    new_type = Annotated[int, Field(description="test")]

    assert is_simple_type(new_type), "Failed for type: " + str(new_type)


def test_literal_simple():
    from typing import Literal

    new_type = Literal[1, 2, 3]

    assert is_simple_type(new_type), "Failed for type: " + str(new_type)


def test_union_simple():
    from typing import Union

    new_type = Union[int, str]

    assert is_simple_type(new_type), "Failed for type: " + str(new_type)


def test_iterable_not_simple():
    from collections.abc import Iterable

    new_type = Iterable[int]

    assert not is_simple_type(new_type), "Failed for type: " + str(new_type)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/test_utils.py
=======
import json
import pytest
from instructor.utils import (
    classproperty,
    extract_json_from_codeblock,
    extract_json_from_stream,
    extract_json_from_stream_async,
    merge_consecutive_messages,
)


def test_extract_json_from_codeblock():
    example = """
    Here is a response

    ```json
    {
        "key": "value"
    }    
    ```
    """
    result = extract_json_from_codeblock(example)
    assert json.loads(result) == {"key": "value"}


def test_extract_json_from_codeblock_no_end():
    example = """
    Here is a response

    ```json
    {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}]
    }  
    """
    result = extract_json_from_codeblock(example)
    assert json.loads(result) == {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}],
    }


def test_extract_json_from_codeblock_no_start():
    example = """
    Here is a response

    {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}, {"key": "value"}]
    }
    """
    result = extract_json_from_codeblock(example)
    assert json.loads(result) == {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}, {"key": "value"}],
    }


def test_stream_json():
    text = """here is the json for you! 
    
    ```json
    , here
    {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}]
    }
    ```

    What do you think?
    """

    def batch_strings(chunks, n=2):
        batch = ""
        for chunk in chunks:
            for char in chunk:
                batch += char
                if len(batch) == n:
                    yield batch
                    batch = ""
        if batch:  # Yield any remaining characters in the last batch
            yield batch

    result = json.loads(
        "".join(list(extract_json_from_stream(batch_strings(text, n=3))))
    )
    assert result == {"key": "value", "another_key": [{"key": {"key": "value"}}]}


@pytest.mark.asyncio
async def test_stream_json_async():
    text = """here is the json for you! 
    
    ```json
    , here
    {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}, {"key": "value"}]
    }
    ```

    What do you think?
    """

    async def batch_strings_async(chunks, n=2):
        batch = ""
        for chunk in chunks:
            for char in chunk:
                batch += char
                if len(batch) == n:
                    yield batch
                    batch = ""
        if batch:  # Yield any remaining characters in the last batch
            yield batch

    result = json.loads(
        "".join(
            [
                chunk
                async for chunk in extract_json_from_stream_async(
                    batch_strings_async(text, n=3)
                )
            ]
        )
    )
    assert result == {
        "key": "value",
        "another_key": [{"key": {"key": "value"}}, {"key": "value"}],
    }


def test_merge_consecutive_messages():
    messages = [
        {"role": "user", "content": "Hello"},
        {"role": "user", "content": "How are you"},
        {"role": "assistant", "content": "Hello"},
        {"role": "assistant", "content": "I am good"},
    ]
    result = merge_consecutive_messages(messages)
    assert result == [
        {
            "role": "user",
            "content": "Hello\n\nHow are you",
        },
        {
            "role": "assistant",
            "content": "Hello\n\nI am good",
        },
    ]


def test_merge_consecutive_messages_empty():
    messages = []
    result = merge_consecutive_messages(messages)
    assert result == []


def test_merge_consecutive_messages_single():
    messages = [
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hello"},
    ]
    result = merge_consecutive_messages(messages)
    assert result == [
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hello"},
    ]


def test_classproperty():
    """Test custom `classproperty` descriptor."""

    class MyClass:
        @classproperty
        def my_property(cls):
            return cls

    assert MyClass.my_property is MyClass

    class MyClass:
        clvar = 1

        @classproperty
        def my_property(cls):
            return cls.clvar

    assert MyClass.my_property == 1


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/dsl/test_partial.py
=======
# type: ignore[all]
from pydantic import BaseModel, Field
from instructor.dsl.partial import Partial
import pytest
import instructor
from openai import OpenAI, AsyncOpenAI

models = ["gpt-4o"]
modes = [
    instructor.Mode.TOOLS,
]


class SampleNestedPartial(BaseModel):
    b: int


class SamplePartial(BaseModel):
    a: int
    b: SampleNestedPartial


def test_partial():
    partial = Partial[SamplePartial]
    assert partial.model_json_schema() == {
        "$defs": {
            "PartialSampleNestedPartial": {
                "properties": {"b": {"title": "B", "type": "integer"}},
                "required": ["b"],
                "title": "PartialSampleNestedPartial",
                "type": "object",
            }
        },
        "properties": {
            "a": {"title": "A", "type": "integer"},
            "b": {"$ref": "#/$defs/PartialSampleNestedPartial"},
        },
        "required": ["a", "b"],
        "title": "PartialSamplePartial",
        "type": "object",
    }, "Wrapped model JSON schema has changed"
    assert partial.get_partial_model().model_json_schema() == {
        "$defs": {
            "PartialSampleNestedPartial": {
                "properties": {
                    "b": {
                        "anyOf": [{"type": "integer"}, {"type": "null"}],
                        "default": None,
                        "title": "B",
                    }
                },
                "title": "PartialSampleNestedPartial",
                "type": "object",
            }
        },
        "properties": {
            "a": {
                "anyOf": [{"type": "integer"}, {"type": "null"}],
                "default": None,
                "title": "A",
            },
            "b": {
                "anyOf": [
                    {"$ref": "#/$defs/PartialSampleNestedPartial"},
                    {"type": "null"},
                ],
                "default": {},
            },
        },
        "title": "PartialSamplePartial",
        "type": "object",
    }, "Partial model JSON schema has changed"

    for model in partial.model_from_chunks(['{"b": {"b": 1}}']):
        assert model.model_dump() == {"a": None, "b": {"b": 1}}


def test_summary_extraction():
    class Summary(BaseModel):
        summary: str = Field(description="A detailed summary")

    client = OpenAI()
    client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)
    extraction_stream = client.chat.completions.create_partial(
        model="gpt-4o",
        response_model=Summary,
        messages=[
            {"role": "system", "content": "You summarize text"},
            {"role": "user", "content": "Summarize: Mary had a little lamb"},
        ],
        stream=True,
    )

    previous_summary = None
    updates = 0
    for extraction in extraction_stream:
        if previous_summary is not None:
            assert extraction.summary.startswith(previous_summary)
            updates += 1
        previous_summary = extraction.summary

    assert updates > 1


@pytest.mark.asyncio
async def test_summary_extraction_async():
    class Summary(BaseModel):
        summary: str = Field(description="A detailed summary")

    client = AsyncOpenAI()
    client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)
    extraction_stream = client.chat.completions.create_partial(
        model="gpt-4o",
        response_model=Summary,
        messages=[
            {"role": "system", "content": "You summarize text"},
            {"role": "user", "content": "Summarize: Mary had a little lamb"},
        ],
        stream=True,
    )

    previous_summary = None
    updates = 0
    async for extraction in extraction_stream:
        if previous_summary is not None:
            assert extraction.summary.startswith(previous_summary)
            updates += 1
        previous_summary = extraction.summary

    assert updates > 1


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/dsl/test_simple_type.py
=======
import unittest
from instructor.dsl.simple_type import is_simple_type
from pydantic import BaseModel
from enum import Enum
import typing


class SimpleTypeTests(unittest.TestCase):
    def test_is_simple_type_with_base_model(self):
        class MyModel(BaseModel):
            label: str

        self.assertFalse(is_simple_type(MyModel))

    def test_is_simple_type_with_str(self):
        self.assertTrue(is_simple_type(str))

    def test_is_simple_type_with_int(self):
        self.assertTrue(is_simple_type(int))

    def test_is_simple_type_with_float(self):
        self.assertTrue(is_simple_type(float))

    def test_is_simple_type_with_bool(self):
        self.assertTrue(is_simple_type(bool))

    def test_is_simple_type_with_enum(self):
        class MyEnum(Enum):
            VALUE = 1

        self.assertTrue(is_simple_type(MyEnum))

    def test_is_simple_type_with_annotated(self):
        AnnotatedType = typing.Annotated[int, "example"]
        self.assertTrue(is_simple_type(AnnotatedType))

    def test_is_simple_type_with_literal(self):
        LiteralType = typing.Literal[1, 2, 3]
        self.assertTrue(is_simple_type(LiteralType))

    def test_is_simple_type_with_union(self):
        UnionType = typing.Union[int, str]
        self.assertTrue(is_simple_type(UnionType))

    def test_is_simple_type_with_iterable(self):
        IterableType = typing.Iterable[int]
        self.assertFalse(is_simple_type(IterableType))


if __name__ == "__main__":
    unittest.main()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_new_client.py
=======
import cohere
import os
import openai
import instructor
import anthropic
import pytest
from pydantic import BaseModel, Field


class User(BaseModel):
    name: str
    age: int


def test_client_create():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    user = client.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


def test_client_messages_create():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    user = client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


def test_client_chat_completions_create_with_response():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    user, completion = client.chat.completions.create_with_completion(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10

    from openai.types.chat import ChatCompletion

    assert isinstance(completion, ChatCompletion)


def test_client_chat_completions_create():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    user = client.chat.completions.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


def test_client_chat_completions_create_partial():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    for user in client.chat.completions.create_partial(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    ):
        assert isinstance(user, User)


def test_client_chat_completions_create_iterable():
    client = instructor.from_openai(openai.OpenAI(), model="gpt-3.5-turbo")

    users = [
        user
        for user in client.chat.completions.create_iterable(
            response_model=User,
            messages=[{"role": "user", "content": "Alice is 25, Bob is 30"}],
            temperature=0,
        )
    ]
    assert len(users) == 2


@pytest.mark.asyncio
async def test_async_client_chat_completions_create():
    client = openai.AsyncOpenAI()
    instructor_client = instructor.from_openai(client, model="gpt-3.5-turbo")

    user = await instructor_client.chat.completions.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.asyncio
async def test_async_client_chat_completions_create_partial():
    client = openai.AsyncOpenAI()
    instructor_client = instructor.from_openai(client, model="gpt-3.5-turbo")

    async for user in instructor_client.chat.completions.create_partial(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    ):
        assert isinstance(user, User)


@pytest.mark.asyncio
async def test_async_client_chat_completions_create_iterable():
    client = openai.AsyncOpenAI()
    instructor_client = instructor.from_openai(client, model="gpt-3.5-turbo")

    async for user in instructor_client.chat.completions.create_iterable(
        response_model=User,
        messages=[{"role": "user", "content": "Alice is 25, Bob is 30"}],
        temperature=0,
    ):
        assert isinstance(user, User)


@pytest.mark.asyncio
async def test_async_client_chat_completions_create_with_response():
    client = openai.AsyncOpenAI()
    instructor_client = instructor.from_openai(client, model="gpt-3.5-turbo")

    user, response = await instructor_client.chat.completions.create_with_completion(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    from openai.types.chat import ChatCompletion

    assert user.name == "Jason"
    assert user.age == 10
    assert isinstance(response, ChatCompletion)


def test_client_from_anthropic_with_response():
    client = instructor.from_anthropic(
        anthropic.Anthropic(),
        max_tokens=1000,
        model="claude-3-haiku-20240307",
    )

    user, response = client.messages.create_with_completion(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10
    assert isinstance(response, anthropic.types.Message)


def test_client_anthropic_response():
    client = anthropic.Anthropic()
    instructor_client = instructor.from_anthropic(
        client,
        max_tokens=1000,
        model="claude-3-haiku-20240307",
    )

    user = instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.skip(reason="Skip for now")
def test_client_anthropic_bedrock_response():
    client = anthropic.AnthropicBedrock(
        aws_access_key=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        aws_session_token=os.getenv("AWS_SESSION_TOKEN"),
        aws_region=os.getenv("AWS_REGION_NAME"),
    )

    instructor_client = instructor.from_anthropic(
        client,
        max_tokens=1000,
        model="anthropic.claude-3-haiku-20240307-v1:0",
    )

    user = instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.asyncio
async def test_async_client_anthropic_response():
    client = anthropic.AsyncAnthropic()
    instructor_client = instructor.from_anthropic(
        client,
        max_tokens=1000,
        model="claude-3-haiku-20240307",
    )

    user = await instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.skip(reason="Skip for now")
@pytest.mark.asyncio
async def test_async_client_anthropic_bedrock_response():
    client = anthropic.AsyncAnthropicBedrock(
        aws_access_key=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        aws_session_token=os.getenv("AWS_SESSION_TOKEN"),
        aws_region=os.getenv("AWS_REGION_NAME"),
    )

    instructor_client = instructor.from_anthropic(
        client,
        max_tokens=1000,
        model="anthropic.claude-3-haiku-20240307-v1:0",
    )

    user = await instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.skip(reason="Skipping if Cohere API is not available")
def test_client_cohere_response():
    client = cohere.Client()
    instructor_client = instructor.from_cohere(
        client,
        max_tokens=1000,
        model="command-r-plus",
    )

    user = instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.skip(reason="Skipping if Cohere API is not available")
def test_client_cohere_response_with_nested_classes():
    client = cohere.Client()
    instructor_client = instructor.from_cohere(
        client,
        max_tokens=1000,
        model="command-r-plus",
    )

    class Person(BaseModel):
        name: str = Field(description="name of the person")
        country_of_origin: str = Field(description="country of origin of the person")

    class Group(BaseModel):
        group_name: str = Field(description="name of the group")
        members: list[Person] = Field(description="list of members in the group")

    task = """\
    Given the following text, create a Group object for 'The Beatles' band

    Text:
    The Beatles were an English rock band formed in Liverpool in 1960. With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time. The group were integral to the development of 1960s counterculture and popular music's recognition as an art form.
    """
    group = instructor_client.messages.create(
        response_model=Group,
        messages=[{"role": "user", "content": task}],
        temperature=0,
    )
    assert group.group_name == "The Beatles"
    assert len(group.members) == 4
    assert group.members[0].name == "John Lennon"
    assert group.members[1].name == "Paul McCartney"
    assert group.members[2].name == "George Harrison"
    assert group.members[3].name == "Ringo Starr"


@pytest.mark.skip(reason="Skipping if Cohere API is not available")
@pytest.mark.asyncio
async def test_client_cohere_async():
    client = cohere.AsyncClient()
    instructor_client = instructor.from_cohere(
        client,
        max_tokens=1000,
        model="command-r-plus",
    )

    class Person(BaseModel):
        name: str = Field(description="name of the person")
        country_of_origin: str = Field(description="country of origin of the person")

    class Group(BaseModel):
        group_name: str = Field(description="name of the group")
        members: list[Person] = Field(description="list of members in the group")

    task = """\
    Given the following text, create a Group object for 'The Beatles' band

    Text:
    The Beatles were an English rock band formed in Liverpool in 1960. With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time. The group were integral to the development of 1960s counterculture and popular music's recognition as an art form.
    """
    group = await instructor_client.messages.create(
        response_model=Group,
        messages=[{"role": "user", "content": task}],
        temperature=0,
    )
    assert group.group_name == "The Beatles"
    assert len(group.members) == 4
    assert group.members[0].name == "John Lennon"
    assert group.members[1].name == "Paul McCartney"
    assert group.members[2].name == "George Harrison"
    assert group.members[3].name == "Ringo Starr"


@pytest.mark.skip(reason="Skip for now")
def test_client_from_mistral_with_response():
    import mistralai.client as mistralaicli

    client = instructor.from_mistral(
        mistralaicli.MistralClient(),
        max_tokens=1000,
        model="mistral-large-latest",
    )

    user, response = client.messages.create_with_completion(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


@pytest.mark.skip(reason="Skip for now")
def test_client_mistral_response():
    import mistralai.client as mistralaicli

    client = mistralaicli.MistralClient()
    instructor_client = instructor.from_mistral(
        client, max_tokens=1000, model="mistral-large-latest"
    )

    user = instructor_client.messages.create(
        response_model=User,
        messages=[{"role": "user", "content": "Jason is 10"}],
        temperature=0,
    )
    assert user.name == "Jason"
    assert user.age == 10


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_anthropic/conftest.py
=======
# conftest.py
from anthropic import AsyncAnthropic, Anthropic
import pytest
import os

try:
    import braintrust

    wrap_anthropic = braintrust.wrap_anthropic
except ImportError:

    def wrap_anthropic(x):
        return x


@pytest.fixture(scope="session")
def client():
    if os.environ.get("BRAINTRUST_API_KEY"):
        yield wrap_anthropic(
            Anthropic(
                api_key=os.environ["BRAINTRUST_API_KEY"],
                base_url="https://braintrustproxy.com/v1",
            )
        )
    else:
        yield Anthropic()


@pytest.fixture(scope="session")
def aclient():
    if os.environ.get("BRAINTRUST_API_KEY"):
        yield wrap_anthropic(
            AsyncAnthropic(
                api_key=os.environ["BRAINTRUST_API_KEY"],
                base_url="https://braintrustproxy.com/v1",
            )
        )
    else:
        yield AsyncAnthropic()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_anthropic/test_stream.py
=======
from itertools import product
from collections.abc import Iterable
from pydantic import BaseModel
import pytest
import instructor
from instructor.dsl.partial import Partial

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


@pytest.mark.parametrize("model, mode, stream", product(models, modes, [True, False]))
def test_iterable_model(model, mode, stream, client):
    client = instructor.from_anthropic(client, mode=mode)
    model = client.messages.create(
        model=model,
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=stream,
        max_tokens=1024,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model, mode, stream", product(models, modes, [True, False]))
@pytest.mark.asyncio
async def test_iterable_model_async(model, mode, stream, aclient):
    aclient = instructor.from_anthropic(aclient, mode=mode)
    model = await aclient.messages.create(
        model=model,
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=stream,
        max_tokens=1024,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    if stream:
        async for m in model:
            assert isinstance(m, UserExtract)
    else:
        for m in model:
            assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model,mode", product(models, modes))
def test_partial_model(model, mode, client):
    client = instructor.from_anthropic(client, mode=mode)
    model = client.messages.create(
        model=model,
        response_model=Partial[UserExtract],
        max_retries=2,
        max_tokens=1024,
        stream=True,
        messages=[
            {"role": "user", "content": "Jason Liu is 12 years old"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model,mode", product(models, modes))
@pytest.mark.asyncio
async def test_partial_model_async(model, mode, aclient):
    aclient = instructor.from_anthropic(aclient, mode=mode)
    model = await aclient.messages.create(
        model=model,
        response_model=Partial[UserExtract],
        max_retries=2,
        stream=True,
        max_tokens=1024,
        messages=[
            {"role": "user", "content": "Jason Liu is 12 years old"},
        ],
    )
    async for m in model:
        assert isinstance(m, UserExtract)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_anthropic/util.py
=======
import instructor

models = ["claude-3-haiku-20240307"]
modes = [
    instructor.Mode.ANTHROPIC_TOOLS,
]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_anthropic/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_anthropic/evals/test_simple.py
=======
from enum import Enum
from typing import Literal

import anthropic
import pytest
from pydantic import BaseModel, field_validator

import instructor
from instructor.retry import InstructorRetryException

client = instructor.from_anthropic(
    anthropic.Anthropic(), mode=instructor.Mode.ANTHROPIC_TOOLS
)


def test_simple():
    class User(BaseModel):
        name: str
        age: int

        @field_validator("name")
        def name_is_uppercase(cls, v: str):
            assert v.isupper(), "Name must be uppercase, please fix"
            return v

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=2,
        messages=[
            {
                "role": "user",
                "content": "Extract John is 18 years old.",
            }
        ],
        response_model=User,
    )  # type: ignore

    assert isinstance(resp, User)
    assert resp.name == "JOHN"  # due to validation
    assert resp.age == 18


def test_nested_type():
    class Address(BaseModel):
        house_number: int
        street_name: str

    class User(BaseModel):
        name: str
        age: int
        address: Address

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=0,
        messages=[
            {
                "role": "user",
                "content": "Extract John is 18 years old and lives at 123 First Avenue.",
            }
        ],
        response_model=User,
    )  # type: ignore

    assert isinstance(resp, User)
    assert resp.name == "John"
    assert resp.age == 18

    assert isinstance(resp.address, Address)
    assert resp.address.house_number == 123
    assert resp.address.street_name == "First Avenue"


def test_list_str():
    class User(BaseModel):
        name: str
        age: int
        family: list[str]

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=0,
        messages=[
            {
                "role": "user",
                "content": "Create a user for a model with a name, age, and family members.",
            }
        ],
        response_model=User,
    )

    assert isinstance(resp, User)
    assert isinstance(resp.family, list)
    for member in resp.family:
        assert isinstance(member, str)


@pytest.mark.skip("Just use Literal!")
def test_enum():
    class Role(str, Enum):
        ADMIN = "admin"
        USER = "user"

    class User(BaseModel):
        name: str
        role: Role

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=1,
        messages=[
            {
                "role": "user",
                "content": "Create a user for a model with a name and role of admin.",
            }
        ],
        response_model=User,
    )

    assert isinstance(resp, User)
    assert resp.role == Role.ADMIN


def test_literal():
    class User(BaseModel):
        name: str
        role: Literal["admin", "user"]

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=2,
        messages=[
            {
                "role": "user",
                "content": "Create a admin user for a model with a name and role.",
            }
        ],
        response_model=User,
    )  # type: ignore

    assert isinstance(resp, User)
    assert resp.role == "admin"


def test_nested_list():
    class Properties(BaseModel):
        key: str
        value: str

    class User(BaseModel):
        name: str
        age: int
        properties: list[Properties]

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=0,
        messages=[
            {
                "role": "user",
                "content": "Create a user for a model with a name, age, and properties.",
            }
        ],
        response_model=User,
    )

    assert isinstance(resp, User)
    for property in resp.properties:
        assert isinstance(property, Properties)


def test_system_messages_allcaps():
    class User(BaseModel):
        name: str
        age: int

    resp = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1024,
        max_retries=0,
        messages=[
            {"role": "system", "content": "EVERYTHING MUST BE IN ALL CAPS"},
            {
                "role": "user",
                "content": "Create a user for a model with a name and age.",
            },
        ],
        response_model=User,
    )

    assert isinstance(resp, User)
    assert resp.name.isupper()


def test_retry_error():
    class User(BaseModel):
        name: str

        @field_validator("name")
        def validate_name(cls, _):
            raise ValueError("Never succeed")

    try:
        client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1024,
            max_retries=2,
            messages=[
                {
                    "role": "user",
                    "content": "Extract John is 18 years old",
                },
            ],
            response_model=User,
        )
    except InstructorRetryException as e:
        assert e.total_usage.input_tokens > 0 and e.total_usage.output_tokens > 0


@pytest.mark.asyncio
async def test_async_retry_error():
    client = instructor.from_anthropic(anthropic.AsyncAnthropic())

    class User(BaseModel):
        name: str

        @field_validator("name")
        def validate_name(cls, _):
            raise ValueError("Never succeed")

    try:
        await client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1024,
            max_retries=2,
            messages=[
                {
                    "role": "user",
                    "content": "Extract John is 18 years old",
                },
            ],
            response_model=User,
        )
    except InstructorRetryException as e:
        assert e.total_usage.input_tokens > 0 and e.total_usage.output_tokens > 0


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/test_modes.py
=======
from itertools import product
from pydantic import BaseModel, Field
import google.generativeai as genai

import pytest

import instructor
from .util import models, modes


class Item(BaseModel):
    name: str
    price: float


class Order(BaseModel):
    items: list[Item] = Field(..., default_factory=list)
    customer: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_nested(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    content = """
    Order Details:
    Customer: Jason
    Items:

    Name: Apple, Price: 0.50
    Name: Bread, Price: 2.00
    Name: Milk, Price: 1.50
    """

    resp = client.chat.completions.create(
        response_model=Order,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert len(resp.items) == 3
    assert {x.name.lower() for x in resp.items} == {"apple", "bread", "milk"}
    assert {x.price for x in resp.items} == {0.5, 2.0, 1.5}
    assert resp.customer.lower() == "jason"


class Book(BaseModel):
    title: str
    author: str
    genre: str
    isbn: str


class LibraryRecord(BaseModel):
    books: list[Book] = Field(..., default_factory=list)
    visitor: str
    library_id: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_complex_nested_model(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    content = """
    Library visit details:
    Visitor: Jason
    Library ID: LIB123456
    Books checked out:
    - Title: The Great Adventure, Author: Jane Doe, Genre: Fantasy, ISBN: 1234567890
    - Title: History of Tomorrow, Author: John Smith, Genre: Non-Fiction, ISBN: 0987654321
    """

    resp = client.chat.completions.create(
        response_model=LibraryRecord,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert resp.visitor.lower() == "jason"
    assert resp.library_id == "LIB123456"
    assert len(resp.books) == 2
    assert {book.title for book in resp.books} == {
        "The Great Adventure",
        "History of Tomorrow",
    }
    assert {book.author for book in resp.books} == {"Jane Doe", "John Smith"}
    assert {book.genre for book in resp.books} == {"Fantasy", "Non-Fiction"}
    assert {book.isbn for book in resp.books} == {"1234567890", "0987654321"}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/test_patch.py
=======
from itertools import product
from pydantic import BaseModel, field_validator
import pytest
import instructor
import google.generativeai as genai

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_runmodel(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    model = client.chat.completions.create(
        response_model=UserExtract,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtract), "Should be instance of UserExtract"
    assert model.name.lower() == "jason"
    assert model.age == 25
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from Gemini"


class UserExtractValidated(BaseModel):
    name: str
    age: int

    @field_validator("name")
    @classmethod
    def validate_name(cls, v):
        if v.upper() != v:
            raise ValueError(
                "Name should be uppercase, make sure to use the `uppercase` version of the name"
            )
        return v


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_runmodel_validator(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    model = client.chat.completions.create(
        response_model=UserExtractValidated,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtractValidated), "Should be instance of UserExtract"
    assert model.name == "JASON"
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from Gemini"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/test_retries.py
=======
from typing import Annotated
from pydantic import AfterValidator, BaseModel, Field
import pytest
import instructor
from itertools import product
import google.generativeai as genai

from .util import models, modes


def uppercase_validator(v):
    if v.islower():
        raise ValueError("Name must be ALL CAPS")
    return v


class UserDetail(BaseModel):
    name: Annotated[str, AfterValidator(uppercase_validator)] = Field(
        ..., description="The name of the user"
    )
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    response = client.chat.completions.create(
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=3,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case_tenacity(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    from tenacity import Retrying, stop_after_attempt, wait_fixed

    retries = Retrying(
        stop=stop_after_attempt(2),
        wait=wait_fixed(1),
    )

    response = client.chat.completions.create(
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=retries,
    )
    assert response.name == "JASON"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/test_simple_types.py
=======
import instructor
import enum

import google.generativeai as genai
from typing import Literal, Union


def test_literal():
    client = instructor.from_gemini(
        genai.GenerativeModel("models/gemini-1.5-flash-latest")
    )

    response = client.chat.completions.create(
        response_model=Literal["1231", "212", "331"],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in ["1231", "212", "331"]


def test_union():
    client = instructor.from_gemini(
        genai.GenerativeModel("models/gemini-1.5-flash-latest")
    )

    response = client.chat.completions.create(
        response_model=Union[int, str],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) in [int, str]


def test_enum():
    class Options(enum.Enum):
        A = "A"
        B = "B"
        C = "C"

    client = instructor.from_gemini(
        genai.GenerativeModel("models/gemini-1.5-flash-latest")
    )

    response = client.chat.completions.create(
        response_model=Options,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in [Options.A, Options.B, Options.C]


def test_bool():
    client = instructor.from_gemini(
        genai.GenerativeModel("models/gemini-1.5-flash-latest")
    )

    response = client.chat.completions.create(
        response_model=bool,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) == bool


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/test_stream.py
=======
from itertools import product
from collections.abc import Iterable
from pydantic import BaseModel
import pytest
import instructor
import google.generativeai as genai
from instructor.dsl.partial import Partial

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


@pytest.mark.parametrize("model, mode, stream", product(models, modes, [True, False]))
def test_iterable_model(model, mode, stream):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    model = client.chat.completions.create(
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=stream,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model,mode", product(models, modes))
def test_partial_model(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)
    model = client.chat.completions.create(
        response_model=Partial[UserExtract],
        max_retries=2,
        stream=True,
        messages=[
            {"role": "user", "content": "Jason Liu is 12 years old"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/util.py
=======
import instructor

models: list[str] = ["models/gemini-1.5-flash-latest"]
modes = [
    instructor.Mode.GEMINI_JSON,
]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/test_classification_enums.py
=======
import enum
from itertools import product

import pytest
import instructor
import google.generativeai as genai

from pydantic import BaseModel

from ..util import models, modes


class Labels(str, enum.Enum):
    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Correct class label for the given text
    """

    class_label: Labels


data = [
    (
        "I am a spammer",
        Labels.SPAM,
    ),
    (
        "I am not a spammer",
        Labels.NOT_SPAM,
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_classification(model, data, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    input, expected = data
    resp = client.chat.completions.create(
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {input}",
            },
        ],
    )
    assert resp.class_label == expected


# Define new Enum class for multiple labels
class MultiLabels(str, enum.Enum):
    BILLING = "billing"
    GENERAL_QUERY = "general_query"
    HARDWARE = "hardware"


# Adjust the prediction model to accommodate a list of labels
class MultiClassPrediction(BaseModel):
    predicted_labels: list[MultiLabels]


data = [
    (
        "I am having trouble with my billing",
        [MultiLabels.BILLING],
    ),
    (
        "I am having trouble with my hardware",
        [MultiLabels.HARDWARE],
    ),
    (
        "I have a general query and a billing issue",
        [MultiLabels.GENERAL_QUERY, MultiLabels.BILLING],
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_multi_classify(model, data, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    input, expected = data

    resp = client.chat.completions.create(
        response_model=MultiClassPrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following support ticket: {input}",
            },
        ],
    )
    assert set(resp.predicted_labels) == set(expected)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/test_classification_literals.py
=======
from itertools import product
from typing import Literal

import pytest
import instructor
import google.generativeai as genai

from pydantic import BaseModel

from ..util import models, modes


class SinglePrediction(BaseModel):
    """
    Correct class label for the given text
    """

    class_label: Literal["spam", "not_spam"]


data = [
    ("I am a spammer", "spam"),
    ("I am not a spammer", "not_spam"),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_classification(model, data, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    if mode == instructor.Mode.JSON and model in {"gpt-3.5-turbo", "gpt-4"}:
        pytest.skip(
            "JSON mode is not supported for gpt-3.5-turbo and gpt-4, skipping test"
        )

    input, expected = data
    resp = client.chat.completions.create(
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {input}",
            },
        ],
    )
    assert resp.class_label == expected


# Adjust the prediction model to accommodate a list of labels
class MultiClassPrediction(BaseModel):
    predicted_labels: list[Literal["billing", "general_query", "hardware"]]


data = [
    (
        "I am having trouble with my billing",
        ["billing"],
    ),
    (
        "I am having trouble with my hardware",
        ["hardware"],
    ),
    (
        "I have a general query and a billing issue",
        ["general_query", "billing"],
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_multi_classify(model, data, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    input, expected = data

    resp = client.chat.completions.create(
        response_model=MultiClassPrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following support ticket: {input}",
            },
        ],
    )
    assert set(resp.predicted_labels) == set(expected)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/test_entities.py
=======
from itertools import product
from pydantic import BaseModel, Field
import pytest
import google.generativeai as genai
import instructor

from ..util import models, modes


class Property(BaseModel):
    key: str
    value: str
    resolved_absolute_value: str


class Entity(BaseModel):
    id: int = Field(
        ...,
        description="Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities",
    )
    subquote_string: list[str] = Field(
        ...,
        description="Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution",
    )
    entity_title: str
    properties: list[Property] = Field(
        ..., description="List of properties of the entity"
    )
    dependencies: list[int] = Field(
        ...,
        description="List of entity ids that this entity depends  or relies on to resolve it",
    )


class DocumentExtraction(BaseModel):
    entities: list[Entity] = Field(
        ...,
        description="Body of the answer, each fact should be its seperate object with a body and a list of sources",
    )


def ask_ai(content, client) -> DocumentExtraction:
    resp: DocumentExtraction = client.chat.completions.create(
        response_model=DocumentExtraction,
        messages=[
            {
                "role": "system",
                "content": "You are a perfect entity resolution system that extracts facts from the document. Extract and resolve a list of entities from the following document:",
            },
            {
                "role": "user",
                "content": content,
            },
        ],
    )  # type: ignore
    return resp


content = """
Sample Legal Contract
Agreement Contract

This Agreement is made and entered into on 2020-01-01 by and between Company A ("the Client") and Company B ("the Service Provider").

Article 1: Scope of Work

The Service Provider will deliver the software product to the Client 30 days after the agreement date.

Article 2: Payment Terms

The total payment for the service is $50,000.
An initial payment of $10,000 will be made within 7 days of the the signed date.
The final payment will be due 45 days after [SignDate].

Article 3: Confidentiality

The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.

Article 4: Termination

The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].
"""


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_extract(model, mode):
    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    extract = ask_ai(content=content, client=client)

    assert len(extract.entities) > 0


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/test_extract_users.py
=======
import pytest
from itertools import product
from pydantic import BaseModel
import instructor
import google.generativeai as genai
from ..util import models, modes


class UserDetails(BaseModel):
    name: str
    age: int


# Lists for models, test data, and modes
test_data = [
    ("Jason is 10", "Jason", 10),
    ("Alice is 25", "Alice", 25),
    ("Bob is 35", "Bob", 35),
]


@pytest.mark.parametrize("model, data, mode", product(models, test_data, modes))
def test_extract(model, data, mode):
    sample_data, expected_name, expected_age = data

    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    # Calling the extract function with the provided model, sample data, and mode
    response = client.chat.completions.create(
        response_model=UserDetails,
        messages=[
            {"role": "user", "content": sample_data},
        ],
    )

    # Assertions
    assert (
        response.name == expected_name
    ), f"Expected name {expected_name}, got {response.name}"
    assert (
        response.age == expected_age
    ), f"Expected age {expected_age}, got {response.age}"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/test_sentiment_analysis.py
=======
import enum
from itertools import product
from pydantic import BaseModel
import pytest
import instructor
import google.generativeai as genai
from ..util import models, modes


class Sentiment(str, enum.Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"


class SentimentAnalysis(BaseModel):
    sentiment: Sentiment


test_data = [
    (
        "I absolutely love this product! It has exceeded all my expectations.",
        Sentiment.POSITIVE,
    ),
    (
        "The service was terrible. I will never use this company again.",
        Sentiment.NEGATIVE,
    ),
    (
        "The movie was okay. It had some good moments but overall it was average.",
        Sentiment.NEUTRAL,
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, test_data, modes))
def test_sentiment_analysis(model, data, mode):
    sample_data, expected_sentiment = data

    client = instructor.from_gemini(genai.GenerativeModel(model), mode=mode)

    response = client.chat.completions.create(
        response_model=SentimentAnalysis,
        messages=[
            {
                "role": "system",
                "content": "You are a sentiment analysis model. Analyze the sentiment of the given text and provide the sentiment (positive, negative, or neutral).",
            },
            {"role": "user", "content": sample_data},
        ],
    )

    assert response.sentiment == expected_sentiment


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_gemini/evals/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/conftest.py
=======
# conftest.py
from openai import AsyncOpenAI, OpenAI
import pytest
import os

try:
    import braintrust

    wrap_openai = braintrust.wrap_openai
except ImportError:

    def wrap_openai(x):
        return x


@pytest.fixture(scope="session")
def client():
    if os.environ.get("BRAINTRUST_API_KEY"):
        yield wrap_openai(
            OpenAI(
                api_key=os.environ["BRAINTRUST_API_KEY"],
                base_url="https://braintrustproxy.com/v1",
            )
        )
    else:
        yield OpenAI()


@pytest.fixture(scope="session")
def aclient():
    if os.environ.get("BRAINTRUST_API_KEY"):
        yield wrap_openai(
            AsyncOpenAI(
                api_key=os.environ["BRAINTRUST_API_KEY"],
                base_url="https://braintrustproxy.com/v1",
            )
        )
    else:
        yield AsyncOpenAI()


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_modes.py
=======
from itertools import product
from pydantic import BaseModel, Field

import pytest

import instructor
from .util import models, modes


class Item(BaseModel):
    name: str
    price: float


class Order(BaseModel):
    items: list[Item] = Field(..., default_factory=list)
    customer: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_nested(model, mode, client):
    client = instructor.patch(client, mode=mode)
    content = """
    Order Details:
    Customer: Jason
    Items:

    Name: Apple, Price: 0.50
    Name: Bread, Price: 2.00
    Name: Milk, Price: 1.50
    """

    resp = client.chat.completions.create(
        model=model,
        response_model=Order,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert len(resp.items) == 3
    assert {x.name.lower() for x in resp.items} == {"apple", "bread", "milk"}
    assert {x.price for x in resp.items} == {0.5, 2.0, 1.5}
    assert resp.customer.lower() == "jason"


class Book(BaseModel):
    title: str
    author: str
    genre: str
    isbn: str


class LibraryRecord(BaseModel):
    books: list[Book] = Field(..., default_factory=list)
    visitor: str
    library_id: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_complex_nested_model(model, mode, client):
    client = instructor.patch(client, mode=mode)

    content = """
    Library visit details:
    Visitor: Jason
    Library ID: LIB123456
    Books checked out:
    - Title: The Great Adventure, Author: Jane Doe, Genre: Fantasy, ISBN: 1234567890
    - Title: History of Tomorrow, Author: John Smith, Genre: Non-Fiction, ISBN: 0987654321
    """

    resp = client.chat.completions.create(
        model=model,
        response_model=LibraryRecord,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert resp.visitor.lower() == "jason"
    assert resp.library_id == "LIB123456"
    assert len(resp.books) == 2
    assert {book.title for book in resp.books} == {
        "The Great Adventure",
        "History of Tomorrow",
    }
    assert {book.author for book in resp.books} == {"Jane Doe", "John Smith"}
    assert {book.genre for book in resp.books} == {"Fantasy", "Non-Fiction"}
    assert {book.isbn for book in resp.books} == {"1234567890", "0987654321"}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_multitask.py
=======
from itertools import product
from collections.abc import Iterable
from pydantic import BaseModel
import pytest

import instructor
from .util import models, modes


class User(BaseModel):
    name: str
    age: int


Users = Iterable[User]


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_multi_user(model, mode, client):
    client = instructor.patch(client, mode=mode)

    def stream_extract(input: str) -> Iterable[User]:
        return client.chat.completions.create(
            model=model,
            response_model=Users,
            messages=[
                {
                    "role": "system",
                    "content": "You are a perfect entity extraction system",
                },
                {
                    "role": "user",
                    "content": (
                        f"Consider the data below:\n{input}"
                        "Correctly segment it into entitites"
                        "Make sure the JSON is correct"
                    ),
                },
            ],
            max_tokens=1000,
        )

    resp = [user for user in stream_extract(input="Jason is 20, Sarah is 30")]
    assert len(resp) == 2
    assert resp[0].name == "Jason"
    assert resp[0].age == 20
    assert resp[1].name == "Sarah"
    assert resp[1].age == 30


@pytest.mark.asyncio
@pytest.mark.parametrize("model, mode", product(models, modes))
async def test_multi_user_tools_mode_async(model, mode, aclient):
    client = instructor.patch(aclient, mode=mode)

    async def stream_extract(input: str) -> Iterable[User]:
        return await client.chat.completions.create(
            model=model,
            response_model=Users,
            messages=[
                {
                    "role": "user",
                    "content": (
                        f"Consider the data below:\n{input}"
                        "Correctly segment it into entitites"
                        "Make sure the JSON is correct"
                    ),
                },
            ],
            max_tokens=1000,
        )

    resp = []
    for user in await stream_extract(input="Jason is 20, Sarah is 30"):
        resp.append(user)
    print(resp)
    assert len(resp) == 2
    assert resp[0].name == "Jason"
    assert resp[0].age == 20
    assert resp[1].name == "Sarah"
    assert resp[1].age == 30


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_multi_user_stream(model, mode, client):
    client = instructor.patch(client, mode=mode)

    def stream_extract(input: str) -> Iterable[User]:
        return client.chat.completions.create(
            model=model,
            stream=True,
            response_model=Users,
            messages=[
                {
                    "role": "system",
                    "content": "You are a perfect entity extraction system",
                },
                {
                    "role": "user",
                    "content": (
                        f"Consider the data below:\n{input}"
                        "Correctly segment it into entitites"
                        "Make sure the JSON is correct"
                    ),
                },
            ],
            max_tokens=1000,
        )

    resp = [user for user in stream_extract(input="Jason is 20, Sarah is 30")]
    assert len(resp) == 2
    assert resp[0].name == "Jason"
    assert resp[0].age == 20
    assert resp[1].name == "Sarah"
    assert resp[1].age == 30


@pytest.mark.asyncio
@pytest.mark.parametrize("model, mode", product(models, modes))
async def test_multi_user_tools_mode_async_stream(model, mode, aclient):
    client = instructor.patch(aclient, mode=mode)

    async def stream_extract(input: str) -> Iterable[User]:
        return await client.chat.completions.create(
            model=model,
            stream=True,
            response_model=Users,
            messages=[
                {
                    "role": "user",
                    "content": (
                        f"Consider the data below:\n{input}"
                        "Correctly segment it into entitites"
                        "Make sure the JSON is correct"
                    ),
                },
            ],
            max_tokens=1000,
        )

    resp = []
    async for user in await stream_extract(input="Jason is 20, Sarah is 30"):
        resp.append(user)
    print(resp)
    assert len(resp) == 2
    assert resp[0].name == "Jason"
    assert resp[0].age == 20
    assert resp[1].name == "Sarah"
    assert resp[1].age == 30


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_parallel.py
=======
from __future__ import annotations

from typing import Literal, Union
from collections.abc import Iterable
from pydantic import BaseModel

import pytest
import instructor


class Weather(BaseModel):
    location: str
    units: Literal["imperial", "metric"]


class GoogleSearch(BaseModel):
    query: str


def test_sync_parallel_tools__error(client):
    client = instructor.patch(client, mode=instructor.Mode.PARALLEL_TOOLS)

    with pytest.raises(TypeError):
        resp = client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "You must always use tools"},
                {
                    "role": "user",
                    "content": "What is the weather in toronto and dallas and who won the super bowl?",
                },
            ],
            response_model=Weather,
        )


def test_sync_parallel_tools_or(client):
    client = instructor.from_openai(client, mode=instructor.Mode.PARALLEL_TOOLS)
    resp = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas and who won the super bowl?",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3


@pytest.mark.asyncio
async def test_async_parallel_tools_or(aclient):
    client = instructor.from_openai(aclient, mode=instructor.Mode.PARALLEL_TOOLS)
    resp = await client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas and who won the super bowl?",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3


def test_sync_parallel_tools_one(client):
    client = instructor.patch(client, mode=instructor.Mode.PARALLEL_TOOLS)
    resp = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas?",
            },
        ],
        response_model=Iterable[Weather],
    )
    assert len(list(resp)) == 2


@pytest.mark.asyncio
async def test_async_parallel_tools_one(aclient):
    client = instructor.patch(aclient, mode=instructor.Mode.PARALLEL_TOOLS)
    resp = await client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": "You must always use tools"},
            {
                "role": "user",
                "content": "What is the weather in toronto and dallas?",
            },
        ],
        response_model=Iterable[Weather],
    )
    assert len(list(resp)) == 2


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_patch.py
=======
from itertools import product
from pydantic import BaseModel, field_validator
from openai.types.chat import ChatCompletion
from typing_extensions import TypedDict
import pytest
import instructor

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


class UserExtractTypedDict(TypedDict):
    name: str
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_typed_dict(model, mode, client):
    client = instructor.patch(client, mode=mode)
    model = client.chat.completions.create(
        model=model,
        response_model=UserExtractTypedDict,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, BaseModel), "Should be instance of a pydantic model"
    assert model.name.lower() == "jason"
    assert model.age == 25
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from OpenAI"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_runmodel(model, mode, client):
    client = instructor.patch(client, mode=mode)
    model = client.chat.completions.create(
        model=model,
        response_model=UserExtract,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtract), "Should be instance of UserExtract"
    assert model.name.lower() == "jason"
    assert model.age == 25
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from OpenAI"

    ChatCompletion(**model._raw_response.model_dump())


@pytest.mark.parametrize("model, mode", product(models, modes))
@pytest.mark.asyncio
async def test_runmodel_async(model, mode, aclient):
    aclient = instructor.patch(aclient, mode=mode)
    model = await aclient.chat.completions.create(
        model=model,
        response_model=UserExtract,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtract), "Should be instance of UserExtract"
    assert model.name.lower() == "jason"
    assert model.age == 25
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from OpenAI"

    ChatCompletion(**model._raw_response.model_dump())


class UserExtractValidated(BaseModel):
    name: str
    age: int

    @field_validator("name")
    @classmethod
    def validate_name(cls, v):
        if v.upper() != v:
            raise ValueError(
                "Name should have all letters in uppercase. Make sure to use the `uppercase` form of the name"
            )
        return v


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_runmodel_validator(model, mode, client):
    client = instructor.patch(client, mode=mode)
    model = client.chat.completions.create(
        model=model,
        response_model=UserExtractValidated,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtractValidated), "Should be instance of UserExtract"
    assert model.name == "JASON"
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from OpenAI"

    ChatCompletion(**model._raw_response.model_dump())


@pytest.mark.parametrize("model, mode", product(models, modes))
@pytest.mark.asyncio
async def test_runmodel_async_validator(model, mode, aclient):
    aclient = instructor.patch(aclient, mode=mode)
    model = await aclient.chat.completions.create(
        model=model,
        response_model=UserExtractValidated,
        max_retries=2,
        messages=[
            {"role": "user", "content": "Extract jason is 25 years old"},
        ],
    )
    assert isinstance(model, UserExtractValidated), "Should be instance of UserExtract"
    assert model.name == "JASON"
    assert hasattr(
        model, "_raw_response"
    ), "The raw response should be available from OpenAI"

    ChatCompletion(**model._raw_response.model_dump())


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_retries.py
=======
from typing import Annotated
from pydantic import AfterValidator, BaseModel, Field
import pytest
import instructor
from itertools import product
from .util import models, modes


def uppercase_validator(v: str):
    if v.islower():
        raise ValueError(
            "All letters in the name should be in uppercase (Eg. TOM, JONES ) instead of tom, jones"
        )
    return v


class UserDetail(BaseModel):
    name: Annotated[str, AfterValidator(uppercase_validator)] = Field(
        ..., description="The name of the user"
    )
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
@pytest.mark.asyncio
async def test_upper_case_async(model, mode, aclient):
    client = instructor.patch(aclient, mode=mode)
    response = await client.chat.completions.create(
        model=model,
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=3,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
@pytest.mark.asyncio
async def test_upper_case_tenacity_async(model, mode, aclient):
    client = instructor.patch(aclient, mode=mode)
    from tenacity import AsyncRetrying, stop_after_attempt, wait_fixed

    retries = AsyncRetrying(
        stop=stop_after_attempt(2),
        wait=wait_fixed(1),
    )

    response = await client.chat.completions.create(
        model=model,
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=retries,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case(model, mode, client):
    client = instructor.patch(client, mode=mode)
    response = client.chat.completions.create(
        model=model,
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=3,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case_tenacity(model, mode, client):
    client = instructor.patch(client, mode=mode)
    from tenacity import Retrying, stop_after_attempt, wait_fixed

    retries = Retrying(
        stop=stop_after_attempt(2),
        wait=wait_fixed(1),
    )

    response = client.chat.completions.create(
        model=model,
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=retries,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_custom_retry_response_error(model, mode, client):
    original_key = client.api_key
    client = instructor.patch(client, mode=mode)
    client.api_key = "incorrect_key"

    from openai import AuthenticationError
    from instructor.exceptions import InstructorRetryException
    from tenacity import Retrying, retry_if_not_exception_type, stop_after_attempt

    retries = Retrying(
        retry=retry_if_not_exception_type(ZeroDivisionError), stop=stop_after_attempt(1)
    )
    try:
        client.chat.completions.create(
            model=model,
            max_retries=retries,
            messages=[
                {
                    "role": "user",
                    "content": "Jason is 25 years old",
                }
            ],
            response_model=UserDetail,
        )
    except InstructorRetryException as e:
        assert isinstance(e.__cause__.__cause__, AuthenticationError)
        assert e.last_completion is None
    finally:
        client.api_key = original_key


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_simple_types.py
=======
import pytest
import instructor
import enum

from typing import Annotated, Literal, Union
from pydantic import Field


@pytest.mark.asyncio
async def test_response_simple_types(aclient):
    client = instructor.patch(aclient, mode=instructor.Mode.TOOLS)

    for response_model in [int, bool, str]:
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            response_model=response_model,
            messages=[
                {
                    "role": "user",
                    "content": "Produce a Random but correct response given the desired output",
                },
            ],
        )
        assert type(response) == response_model


@pytest.mark.asyncio
async def test_annotate(aclient):
    client = instructor.patch(aclient, mode=instructor.Mode.TOOLS)

    response = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Annotated[int, Field(description="test")],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) == int


def test_literal(client):
    client = instructor.patch(client, mode=instructor.Mode.TOOLS)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Literal["1231", "212", "331"],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in ["1231", "212", "331"]


def test_union(client):
    client = instructor.patch(client, mode=instructor.Mode.TOOLS)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Union[int, str],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) in [int, str]


def test_enum(client):
    class Options(enum.Enum):
        A = "A"
        B = "B"
        C = "C"

    client = instructor.patch(client, mode=instructor.Mode.TOOLS)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=Options,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in [Options.A, Options.B, Options.C]


def test_bool(client):
    client = instructor.patch(client, mode=instructor.Mode.TOOLS)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=bool,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) == bool


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_stream.py
=======
from itertools import product
from collections.abc import Iterable
from pydantic import BaseModel
import pytest
import instructor
from instructor.dsl.partial import Partial

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


@pytest.mark.parametrize("model, mode, stream", product(models, modes, [True, False]))
def test_iterable_model(model, mode, stream, client):
    client = instructor.patch(client, mode=mode)
    model = client.chat.completions.create(
        model=model,
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=stream,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model, mode, stream", product(models, modes, [True, False]))
@pytest.mark.asyncio
async def test_iterable_model_async(model, mode, stream, aclient):
    aclient = instructor.patch(aclient, mode=mode)
    model = await aclient.chat.completions.create(
        model=model,
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=stream,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    if stream:
        async for m in model:
            assert isinstance(m, UserExtract)
    else:
        for m in model:
            assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model,mode", product(models, modes))
def test_partial_model(model, mode, client):
    client = instructor.patch(client, mode=mode)
    model = client.chat.completions.create(
        model=model,
        response_model=Partial[UserExtract],
        max_retries=2,
        stream=True,
        messages=[
            {"role": "user", "content": "Jason Liu is 12 years old"},
        ],
    )
    for m in model:
        assert isinstance(m, UserExtract)


@pytest.mark.parametrize("model,mode", product(models, modes))
@pytest.mark.asyncio
async def test_partial_model_async(model, mode, aclient):
    aclient = instructor.patch(aclient, mode=mode)
    model = await aclient.chat.completions.create(
        model=model,
        response_model=Partial[UserExtract],
        max_retries=2,
        stream=True,
        messages=[
            {"role": "user", "content": "Jason Liu is 12 years old"},
        ],
    )
    async for m in model:
        assert isinstance(m, UserExtract)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/test_validators.py
=======
from itertools import product
import pytest

import instructor

from typing import Annotated
from pydantic import BaseModel, AfterValidator, BeforeValidator, ValidationError

from instructor.dsl.validators import llm_validator
from .util import models, modes


def test_patch_completes_successfully(client):
    class Response(BaseModel):
        message: Annotated[
            str, AfterValidator(instructor.openai_moderation(client=client))
        ]

    with pytest.raises(ValidationError):
        Response(message="I want to make them suffer the consequences")


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_runmodel_validator_error(model, mode, client):
    client = instructor.from_openai(client, mode=mode)

    class QuestionAnswerNoEvil(BaseModel):
        question: str
        answer: Annotated[
            str,
            BeforeValidator(
                llm_validator(
                    "don't say objectionable things", model=model, client=client
                )
            ),
        ]

    with pytest.raises(ValidationError):
        QuestionAnswerNoEvil(
            question="What is the meaning of life?",
            answer="The meaning of life is to be evil and steal",
        )


@pytest.mark.parametrize("model", models)
def test_runmodel_validator_default_openai_client(model, client):
    client = instructor.from_openai(client)

    class QuestionAnswerNoEvil(BaseModel):
        question: str
        answer: Annotated[
            str,
            BeforeValidator(
                llm_validator(
                    "don't say objectionable things", model=model, client=client
                )
            ),
        ]

    with pytest.raises(ValidationError):
        QuestionAnswerNoEvil(
            question="What is the meaning of life?",
            answer="The meaning of life is to be evil and steal",
        )


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/util.py
=======
import instructor

models = ["gpt-4o"]
modes = [
    instructor.Mode.TOOLS,
]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/docs/test_docs.py
=======
import pytest
from pytest_examples import find_examples, CodeExample, EvalExample


@pytest.mark.parametrize("example", find_examples("README.md"), ids=str)
def test_readme(example: CodeExample, eval_example: EvalExample):
    if eval_example.update_examples:
        eval_example.format(example)
    else:
        eval_example.lint(example)


@pytest.mark.parametrize("example", find_examples("docs/concepts"), ids=str)
def test_format_concepts(example: CodeExample, eval_example: EvalExample):
    if eval_example.update_examples:
        eval_example.format(example)
        # eval_example.run_print_update(example)
    else:
        eval_example.lint(example)
        eval_example.run(example)


@pytest.mark.parametrize("example", find_examples("docs/index.md"), ids=str)
def test_index(example: CodeExample, eval_example: EvalExample):
    if eval_example.update_examples:
        eval_example.format(example)
    else:
        eval_example.lint(example)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/docs/test_hub.py
=======
import pytest
from pytest_examples import find_examples, CodeExample, EvalExample


@pytest.mark.parametrize("example", find_examples("docs/hub"), ids=str)
def test_format_blog(example: CodeExample, eval_example: EvalExample):
    excluded_sources = [
        "mistral",
        "ollama",
        "llama_cpp",
        "groq",
        "youtube",
        "contact",
        "langsmith",
    ]  # sources that are not supported in testing
    if any(source in example.source for source in excluded_sources):
        return

    if eval_example.update_examples:
        eval_example.format(example)
        eval_example.run_print_update(example)
    else:
        eval_example.lint(example)
        eval_example.run(example)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/docs/test_mkdocs.py
=======
import pathlib
import pytest


# Note the use of `str`, makes for pretty output
@pytest.mark.parametrize(
    "fpath", pathlib.Path("docs/examples").glob("**/*.md"), ids=str
)
@pytest.mark.skip(reason="This test is not yet implemented")
def test_files_good(fpath):
    from mktestdocs import check_md_file

    check_md_file(fpath=fpath, memory=True)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/test_classification_enums.py
=======
import enum
from itertools import product

import pytest
import instructor

from pydantic import BaseModel

from instructor.function_calls import Mode
from ..util import models, modes


class Labels(str, enum.Enum):
    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Correct class label for the given text
    """

    class_label: Labels


data = [
    (
        "I am a spammer",
        Labels.SPAM,
    ),
    (
        "I am not a spammer",
        Labels.NOT_SPAM,
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_classification(model, data, mode, client):
    client = instructor.from_openai(client, mode=mode)

    if mode == instructor.Mode.JSON and model in {"gpt-3.5-turbo", "gpt-4"}:
        pytest.skip(
            "JSON mode is not supported for gpt-3.5-turbo and gpt-4, skipping test"
        )

    input, expected = data
    resp = client.chat.completions.create(
        model=model,
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {input}",
            },
        ],
    )
    assert resp.class_label == expected


# Define new Enum class for multiple labels
class MultiLabels(str, enum.Enum):
    BILLING = "billing"
    GENERAL_QUERY = "general_query"
    HARDWARE = "hardware"


# Adjust the prediction model to accommodate a list of labels
class MultiClassPrediction(BaseModel):
    predicted_labels: list[MultiLabels]


data = [
    (
        "I am having trouble with my billing",
        [MultiLabels.BILLING],
    ),
    (
        "I am having trouble with my hardware",
        [MultiLabels.HARDWARE],
    ),
    (
        "I have a general query and a billing issue",
        [MultiLabels.GENERAL_QUERY, MultiLabels.BILLING],
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
def test_multi_classify(model, data, mode, client):
    client = instructor.from_openai(client, mode=mode)

    if (mode, model) in {
        (Mode.JSON, "gpt-3.5-turbo"),
        (Mode.JSON, "gpt-4"),
    }:
        pytest.skip(f"{mode} mode is not supported for {model}, skipping test")

    input, expected = data

    resp = client.chat.completions.create(
        model=model,
        response_model=MultiClassPrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following support ticket: {input}",
            },
        ],
    )
    assert set(resp.predicted_labels) == set(expected)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/test_classification_literals.py
=======
from itertools import product
from typing import Literal

import pytest
import instructor

from pydantic import BaseModel

from instructor.function_calls import Mode
from ..util import models, modes


class SinglePrediction(BaseModel):
    """
    Correct class label for the given text
    """

    class_label: Literal["spam", "not_spam"]


data = [
    ("I am a spammer", "spam"),
    ("I am not a spammer", "not_spam"),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
@pytest.mark.asyncio
async def test_classification(model, data, mode, aclient):
    client = instructor.from_openai(aclient, mode=mode)

    if mode == instructor.Mode.JSON and model in {"gpt-3.5-turbo", "gpt-4"}:
        pytest.skip(
            "JSON mode is not supported for gpt-3.5-turbo and gpt-4, skipping test"
        )

    input, expected = data
    resp = await client.chat.completions.create(
        model=model,
        response_model=SinglePrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following text: {input}",
            },
        ],
    )
    assert resp.class_label == expected


# Adjust the prediction model to accommodate a list of labels
class MultiClassPrediction(BaseModel):
    predicted_labels: list[Literal["billing", "general_query", "hardware"]]


data = [
    (
        "I am having trouble with my billing",
        ["billing"],
    ),
    (
        "I am having trouble with my hardware",
        ["hardware"],
    ),
    (
        "I have a general query and a billing issue",
        ["general_query", "billing"],
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, data, modes))
@pytest.mark.asyncio
async def test_multi_classify(model, data, mode, aclient):
    client = instructor.from_openai(aclient, mode=mode)

    if (mode, model) in {
        (Mode.JSON, "gpt-3.5-turbo"),
        (Mode.JSON, "gpt-4"),
    }:
        pytest.skip(f"{mode} mode is not supported for {model}, skipping test")

    input, expected = data

    resp = await client.chat.completions.create(
        model=model,
        response_model=MultiClassPrediction,
        messages=[
            {
                "role": "user",
                "content": f"Classify the following support ticket: {input}",
            },
        ],
    )
    assert set(resp.predicted_labels) == set(expected)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/test_entities.py
=======
from itertools import product
from pydantic import BaseModel, Field
import pytest

import instructor

from instructor.function_calls import Mode
from ..util import models, modes


class Property(BaseModel):
    key: str
    value: str
    resolved_absolute_value: str


class Entity(BaseModel):
    id: int = Field(
        ...,
        description="Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities",
    )
    subquote_string: list[str] = Field(
        ...,
        description="Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution",
    )
    entity_title: str
    properties: list[Property] = Field(
        ..., description="List of properties of the entity"
    )
    dependencies: list[int] = Field(
        ...,
        description="List of entity ids that this entity depends  or relies on to resolve it",
    )


class DocumentExtraction(BaseModel):
    entities: list[Entity] = Field(
        ...,
        description="Body of the answer, each fact should be its seperate object with a body and a list of sources",
    )


def ask_ai(content, model, client) -> DocumentExtraction:
    resp: DocumentExtraction = client.chat.completions.create(
        model=model,
        response_model=DocumentExtraction,
        messages=[
            {
                "role": "system",
                "content": "You are a perfect entity resolution system that extracts facts from the document. Extract and resolve a list of entities from the following document:",
            },
            {
                "role": "user",
                "content": content,
            },
        ],
        max_retries=4,
    )  # type: ignore
    return resp


content = """
Sample Legal Contract
Agreement Contract

This Agreement is made and entered into on 2020-01-01 by and between Company A ("the Client") and Company B ("the Service Provider").

Article 1: Scope of Work

The Service Provider will deliver the software product to the Client 30 days after the agreement date.

Article 2: Payment Terms

The total payment for the service is $50,000.
An initial payment of $10,000 will be made within 7 days of the the signed date.
The final payment will be due 45 days after [SignDate].

Article 3: Confidentiality

The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.

Article 4: Termination

The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].
"""


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_extract(model, mode, client):
    client = instructor.patch(client, mode=mode)
    if (mode, model) in {
        (Mode.JSON, "gpt-3.5-turbo"),
        (Mode.JSON, "gpt-4"),
    }:
        pytest.skip(f"{mode} mode is not supported for {model}, skipping test")

    # Honestly, if there are no errors, then it's a pass
    extract = ask_ai(content=content, model=model, client=client)
    assert len(extract.entities) > 0


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/test_extract_users.py
=======
import pytest
from itertools import product
from pydantic import BaseModel
import instructor
from instructor.function_calls import Mode
from ..util import models, modes


class UserDetails(BaseModel):
    name: str
    age: int


# Lists for models, test data, and modes
test_data = [
    ("Jason is 10", "Jason", 10),
    ("Alice is 25", "Alice", 25),
    ("Bob is 35", "Bob", 35),
]


@pytest.mark.parametrize("model, data, mode", product(models, test_data, modes))
def test_extract(model, data, mode, client):
    sample_data, expected_name, expected_age = data

    if (mode, model) in {
        (Mode.JSON, "gpt-3.5-turbo"),
        (Mode.JSON, "gpt-4"),
    }:
        pytest.skip(f"{mode} mode is not supported for {model}, skipping test")

    # Setting up the client with the instructor patch
    client = instructor.patch(client, mode=mode)

    # Calling the extract function with the provided model, sample data, and mode
    response = client.chat.completions.create(
        model=model,
        response_model=UserDetails,
        messages=[
            {"role": "user", "content": sample_data},
        ],
    )

    # Assertions
    assert (
        response.name == expected_name
    ), f"Expected name {expected_name}, got {response.name}"
    assert (
        response.age == expected_age
    ), f"Expected age {expected_age}, got {response.age}"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/test_sentiment_analysis.py
=======
import enum
from itertools import product
from pydantic import BaseModel
import pytest
import instructor
from instructor.function_calls import Mode
from ..util import models, modes


class Sentiment(str, enum.Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"


class SentimentAnalysis(BaseModel):
    sentiment: Sentiment


test_data = [
    (
        "I absolutely love this product! It has exceeded all my expectations.",
        Sentiment.POSITIVE,
    ),
    (
        "The service was terrible. I will never use this company again.",
        Sentiment.NEGATIVE,
    ),
    (
        "The movie was okay. It had some good moments but overall it was average.",
        Sentiment.NEUTRAL,
    ),
]


@pytest.mark.parametrize("model, data, mode", product(models, test_data, modes))
def test_sentiment_analysis(model, data, mode, client):
    sample_data, expected_sentiment = data

    if (mode, model) in {
        (Mode.JSON, "gpt-3.5-turbo"),
        (Mode.JSON, "gpt-4"),
    }:
        pytest.skip(f"{mode} mode is not supported for {model}, skipping test")

    client = instructor.patch(client, mode=mode)

    response = client.chat.completions.create(
        model=model,
        response_model=SentimentAnalysis,
        messages=[
            {
                "role": "system",
                "content": "You are a sentiment analysis model. Analyze the sentiment of the given text and provide the sentiment (positive, negative, or neutral).",
            },
            {"role": "user", "content": sample_data},
        ],
    )

    assert response.sentiment == expected_sentiment


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_openai/evals/__init__.py
=======


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/test_modes.py
=======
from itertools import product
from pydantic import BaseModel, Field
import vertexai.generative_models as gm  # type: ignore
import pytest
import instructor

from .util import models, modes


class Item(BaseModel):
    name: str
    price: float


class Order(BaseModel):
    items: list[Item] = Field(..., default_factory=list)
    customer: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_nested(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)
    content = """
    Order Details:
    Customer: Jason
    Items:

    Name: Apple, Price: 0.50
    Name: Bread, Price: 2.00
    Name: Milk, Price: 1.50
    """

    resp = client.create(
        response_model=Order,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert len(resp.items) == 3
    assert {x.name.lower() for x in resp.items} == {"apple", "bread", "milk"}
    assert {x.price for x in resp.items} == {0.5, 2.0, 1.5}
    assert resp.customer.lower() == "jason"


class Book(BaseModel):
    title: str
    author: str
    genre: str
    isbn: str


class LibraryRecord(BaseModel):
    books: list[Book] = Field(..., default_factory=list)
    visitor: str
    library_id: str


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_complex_nested_model(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)

    content = """
    Library visit details:
    Visitor: Jason
    Library ID: LIB123456
    Books checked out:
    - Title: The Great Adventure, Author: Jane Doe, Genre: Fantasy, ISBN: 1234567890
    - Title: History of Tomorrow, Author: John Smith, Genre: Non-Fiction, ISBN: 0987654321
    """

    resp = client.create(
        response_model=LibraryRecord,
        messages=[
            {
                "role": "user",
                "content": content,
            },
        ],
    )

    assert resp.visitor.lower() == "jason"
    assert resp.library_id == "LIB123456"
    assert len(resp.books) == 2
    assert {book.title for book in resp.books} == {
        "The Great Adventure",
        "History of Tomorrow",
    }
    assert {book.author for book in resp.books} == {"Jane Doe", "John Smith"}
    assert {book.genre for book in resp.books} == {"Fantasy", "Non-Fiction"}
    assert {book.isbn for book in resp.books} == {"1234567890", "0987654321"}


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/test_retries.py
=======
from itertools import product
from typing import Annotated, cast
from pydantic import AfterValidator, BaseModel, Field
import pytest
import instructor
import vertexai.generative_models as gm  # type: ignore

from .util import models, modes


def uppercase_validator(v: str):
    if v.islower():
        raise ValueError("Name must be ALL CAPS")
    return v


class UserDetail(BaseModel):
    name: Annotated[str, AfterValidator(uppercase_validator)] = Field(
        ..., description="The name of the user"
    )
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)
    response = client.create(
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=3,
    )
    assert response.name == "JASON"


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_upper_case_tenacity(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)
    from tenacity import Retrying, stop_after_attempt, wait_fixed

    retries = Retrying(
        stop=stop_after_attempt(2),
        wait=wait_fixed(1),
    )

    retries = cast(int, retries)

    response = client.create(
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": "Extract `jason is 12`"},
        ],
        max_retries=retries,
    )
    assert response.name == "JASON"


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/test_simple_types.py
=======
import instructor
import pytest
import enum
import vertexai.generative_models as gm  # type: ignore
from itertools import product
from typing import Literal, Union

from .util import models, modes


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_literal(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)

    response = client.create(
        response_model=Literal["1231", "212", "331"],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in ["1231", "212", "331"]


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_union(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)

    response = client.create(
        response_model=Union[int, str],
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) in [int, str]


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_enum(model, mode):
    class Options(enum.Enum):
        A = "A"
        B = "B"
        C = "C"

    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)

    response = client.create(
        response_model=Options,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert response in [Options.A, Options.B, Options.C]


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_bool(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)

    response = client.create(
        response_model=bool,
        messages=[
            {
                "role": "user",
                "content": "Produce a Random but correct response given the desired output",
            },
        ],
    )
    assert type(response) == bool


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/test_stream.py
=======
from collections.abc import Iterable
from itertools import product
from pydantic import BaseModel
import pytest
import instructor
import vertexai.generative_models as gm  # type: ignore
from instructor.dsl.partial import Partial

from .util import models, modes


class UserExtract(BaseModel):
    name: str
    age: int


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_iterable_model(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)
    response_stream = client.chat.completions.create(
        response_model=Iterable[UserExtract],
        max_retries=2,
        stream=True,
        messages=[
            {"role": "user", "content": "Make two up people"},
        ],
    )
    for chunk in response_stream:
        assert isinstance(chunk, UserExtract)


@pytest.mark.parametrize("model, mode", product(models, modes))
def test_partial_model(model, mode):
    client = instructor.from_vertexai(gm.GenerativeModel(model), mode)
    response_stream = client.chat.completions.create(
        response_model=Partial[UserExtract],
        max_retries=2,
        stream=True,
        messages=[
            {"role": "user", "content": "Anibal is 23 years old"},
        ],
    )
    for chunk in response_stream:
        assert isinstance(chunk, UserExtract)


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/util.py
=======
import instructor

models = ["gemini-1.5-pro-preview-0409"]
modes = [instructor.Mode.VERTEXAI_TOOLS, instructor.Mode.VERTEXAI_JSON]


======
/mnt/c/Users/brian/Downloads/instructor-main/instructor-main/tests/llm/test_vertexai/__init__.py
=======


